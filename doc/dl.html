<!doctype html>
<html lang="en">
<head>
<title>Build Your Own Deep Learning</title>
<!-- 2020-08-09 Sun 08:07 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="generator" content="Org-mode">
<meta name="author" content="lascauje">

<link  href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.5/css/bootstrap.min.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.5/js/bootstrap.min.js"></script>
<style type="text/css">
/* org mode styles on top of twbs */

html {
    position: relative;
    min-height: 100%;
}

body {
    font-size: 18px;
    margin-bottom: 105px;
}

footer {
    position: absolute;
    bottom: 0;
    width: 100%;
    height: 101px;
    background-color: #f5f5f5;
}

footer > div {
    padding: 10px;
}

footer p {
    margin: 0 0 5px;
    text-align: center;
    font-size: 16px;
}

#table-of-contents {
    margin-top: 20px;
    margin-bottom: 20px;
}

blockquote p {
    font-size: 18px;
}

pre {
    font-size: 16px;
}

.footpara {
    display: inline-block;
}

figcaption {
  font-size: 16px;
  color: #666;
  font-style: italic;
  padding-bottom: 15px;
}

/* from twbs docs */

.bs-docs-sidebar.affix {
    position: static;
}
@media (min-width: 768px) {
    .bs-docs-sidebar {
        padding-left: 20px;
    }
}

/* All levels of nav */
.bs-docs-sidebar .nav > li > a {
    display: block;
    padding: 4px 20px;
    font-size: 14px;
    font-weight: 500;
    color: #999;
}
.bs-docs-sidebar .nav > li > a:hover,
.bs-docs-sidebar .nav > li > a:focus {
    padding-left: 19px;
    color: #A1283B;
    text-decoration: none;
    background-color: transparent;
    border-left: 1px solid #A1283B;
}
.bs-docs-sidebar .nav > .active > a,
.bs-docs-sidebar .nav > .active:hover > a,
.bs-docs-sidebar .nav > .active:focus > a {
    padding-left: 18px;
    font-weight: bold;
    color: #A1283B;
    background-color: transparent;
    border-left: 2px solid #A1283B;
}

/* Nav: second level (shown on .active) */
.bs-docs-sidebar .nav .nav {
    display: none; /* Hide by default, but at >768px, show it */
    padding-bottom: 10px;
}
.bs-docs-sidebar .nav .nav > li > a {
    padding-top: 1px;
    padding-bottom: 1px;
    padding-left: 30px;
    font-size: 12px;
    font-weight: normal;
}
.bs-docs-sidebar .nav .nav > li > a:hover,
.bs-docs-sidebar .nav .nav > li > a:focus {
    padding-left: 29px;
}
.bs-docs-sidebar .nav .nav > .active > a,
.bs-docs-sidebar .nav .nav > .active:hover > a,
.bs-docs-sidebar .nav .nav > .active:focus > a {
    padding-left: 28px;
    font-weight: 500;
}

/* Nav: third level (shown on .active) */
.bs-docs-sidebar .nav .nav .nav {
    padding-bottom: 10px;
}
.bs-docs-sidebar .nav .nav .nav > li > a {
    padding-top: 1px;
    padding-bottom: 1px;
    padding-left: 40px;
    font-size: 12px;
    font-weight: normal;
}
.bs-docs-sidebar .nav .nav .nav > li > a:hover,
.bs-docs-sidebar .nav .nav .nav > li > a:focus {
    padding-left: 39px;
}
.bs-docs-sidebar .nav .nav .nav > .active > a,
.bs-docs-sidebar .nav .nav .nav > .active:hover > a,
.bs-docs-sidebar .nav .nav .nav > .active:focus > a {
    padding-left: 38px;
    font-weight: 500;
}

/* Show and affix the side nav when space allows it */
@media (min-width: 992px) {
    .bs-docs-sidebar .nav > .active > ul {
        display: block;
    }
    /* Widen the fixed sidebar */
    .bs-docs-sidebar.affix,
    .bs-docs-sidebar.affix-bottom {
        width: 213px;
    }
    .bs-docs-sidebar.affix {
        position: fixed; /* Undo the static from mobile first approach */
        top: 20px;
    }
    .bs-docs-sidebar.affix-bottom {
        position: absolute; /* Undo the static from mobile first approach */
    }
    .bs-docs-sidebar.affix .bs-docs-sidenav,.bs-docs-sidebar.affix-bottom .bs-docs-sidenav {
        margin-top: 0;
        margin-bottom: 0
    }
}
@media (min-width: 1200px) {
    /* Widen the fixed sidebar again */
    .bs-docs-sidebar.affix-bottom,
    .bs-docs-sidebar.affix {
        width: 263px;
    }
}
</style>
<script type="text/javascript">
$(function() {
    'use strict';

    $('.bs-docs-sidebar li').first().addClass('active');

    $(document.body).scrollspy({target: '.bs-docs-sidebar'});

    $('.bs-docs-sidebar').affix();
});
</script><link rel="stylesheet" type="text/css" href="export/config/org.css"/>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  displayAlign: "center",
  displayIndent: "2em",
  messageStyle: "none",
  "HTML-CSS": {
    scale: 100,
    styles: {
      ".MathJax_Display": {
        "font-size": "100%"
      }
    }
  },
  "SVG": {
    scale: 100,
    styles: {
      ".MathJax_SVG_Display": {
        "font-size": "100%",
        "margin-left": "-2.281em"
      }
    }
  }
});
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_SVG"></script>
</head>
<body>
<div id="content" class="container">
<div class="row"><div class="col-md-9"><h1 class="title">Build Your Own Deep Learning</h1>
<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1"><span class="section-number-2">1</span> Preface</h2>
<div class="outline-text-2" id="text-1">

<figure>
<p><img src="./export/img/preface.png" class="img-responsive" alt="preface.png" width="480">
</p>
</figure>
</div>
<div id="outline-container-sec-1-1" class="outline-3">
<h3 id="sec-1-1"><span class="section-number-3">1.1</span> License</h3>
<div class="outline-text-3" id="text-1-1">
<p>
Copyright © 2020 lascauje
</p>

<p>
This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.
</p>

<p>
This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.
</p>

<p>
You should have received a copy of the GNU General Public License
along with this program.  If not, see <a href="https://www.gnu.org/licenses/">https://www.gnu.org/licenses/</a>.
</p>
</div>
</div>
<div id="outline-container-sec-1-2" class="outline-3">
<h3 id="sec-1-2"><span class="section-number-3">1.2</span> Disclaimer</h3>
<div class="outline-text-3" id="text-1-2">
<p>
This document contains my personal project. It was developed for fun, and during my spare
time. Since I'm neither a machine learning engineer nor a Lisp hacker, the code
could be not perfect, and the commentary associated could miss the advanced theory.
</p>
</div>
</div>
<div id="outline-container-sec-1-3" class="outline-3">
<h3 id="sec-1-3"><span class="section-number-3">1.3</span> Philosophy</h3>
<div class="outline-text-3" id="text-1-3">
<p>
The purpose of this project was to deep dive into machine learning
with Lisp based language, in literate programming manner,
and through reproducible computations.
</p>

<p>
Happy reading!
</p>
</div>
<div id="outline-container-sec-1-3-1" class="outline-4">
<h4 id="sec-1-3-1"><span class="section-number-4">1.3.1</span> Machine Learning, The Hard Way</h4>
<div class="outline-text-4" id="text-1-3-1">
<p>
This project is divided into 10 mini projects, and tends to cover several deep
learning topics. These projects are inspired by
<a href="https://www.deeplearning.ai/deep-learning-specialization/">deep learning specialization</a>, <a href="https://www.tensorflow.org/tutorials/">tensorflow tutorials</a>, and <a href="https://keras.io/getting_started/">keras tutorials</a>.
</p>
</div>
</div>
<div id="outline-container-sec-1-3-2" class="outline-4">
<h4 id="sec-1-3-2"><span class="section-number-4">1.3.2</span> Hylang, The Lisp Language</h4>
<div class="outline-text-4" id="text-1-3-2">
<p>
Hylang is a Lisp dialect that’s embedded in Python, and it provides direct
access to Python libraries (TensorFlow, Keras, Django, etc.).
Lisp syntax (s-expression) is more convenient in REPL driven development.
Lisp is a functional language, in machine learning it's more natural
to program in this paradigm than others,
because functional programming
is closer to mathematical thinking (personal opinion).
</p>
</div>
</div>
<div id="outline-container-sec-1-3-3" class="outline-4">
<h4 id="sec-1-3-3"><span class="section-number-4">1.3.3</span> Literate Programming, <i>Ou Programmation Lettrée</i></h4>
<div class="outline-text-4" id="text-1-3-3">
<p>
The main intention of the literate programming is to treat a program as
literature: the source code of a program can be read like a (technical) book.
In other words, all the documentation
and the source code are contained in this file.
In this project, literate programming was made with org-mode,
a GNU/Emacs library.
</p>
</div>
</div>
<div id="outline-container-sec-1-3-4" class="outline-4">
<h4 id="sec-1-3-4"><span class="section-number-4">1.3.4</span> Reproducible Computations, Real Dependencies?</h4>
<div class="outline-text-4" id="text-1-3-4">
<p>
In computer science it can be quite difficult to reproduce
exactly the same results of computations in long term,
because it requires the exact same versions of all packages,
with for each all dependencies, and using the same environment.
GNU/Guix, a functional package manager, aims to ensure
reproducible computations.
</p>
</div>
</div>
</div>
<div id="outline-container-sec-1-4" class="outline-3">
<h3 id="sec-1-4"><span class="section-number-3">1.4</span> What is Deep Learning</h3>
<div class="outline-text-3" id="text-1-4">
<blockquote>
<ul class="org-ul">
<li>Artificial Intelligence: Any technique that enables computers to mimic human behavior
</li>
<li>Machine Learning: Ability to learn without explicitly being programmed
</li>
<li>Deep Learning: Extract patterns from data using neural networks
</li>
<li>Deep Learning is a subset of Machine Learning, and Machine Learning is a subset of Artificial Intelligence
</li>
</ul>
<p>
&#x2014; MIT 6.S191 Introduction to Deep Learning
</p>
</blockquote>


<blockquote>
<p>
Deep Learning is a superpower. With it you can make a computer see,
synthesize novel art, translate languages, render a medical diagnosis,
or build pieces of a car that can drive itself. If that isn’t a superpower,
I don’t know what is.
&#x2013;— Andrew Ng, Founder of deeplearning.ai and Coursera
</p>
</blockquote>
</div>
</div>
<div id="outline-container-sec-1-5" class="outline-3">
<h3 id="sec-1-5"><span class="section-number-3">1.5</span> Further Reading</h3>
<div class="outline-text-3" id="text-1-5">
<ul class="org-ul">
<li>deeplearning.ai. 2020. <a href="https://www.deeplearning.ai/deep-learning-specialization/">Deep Learning Specialization</a>.
</li>
<li>MIT 6.S191. 2020. <a href="http://introtodeeplearning.com">Introduction to Deep Learning</a>.
</li>
<li>Burkov Andriy. 2020. <a href="http://themlbook.com/">The Hundred-Page Machine Learning</a>.
</li>
<li>Schulte Eric, Davison Dan, Dye Tom. 2020. <a href="https://orgmode.org/worg/org-contrib/babel/intro.html#literate-programming">Literate Programming with Org-mode</a>.
</li>
<li>Hylang Team. 2020. <a href="https://docs.hylang.org/">The Hy Manual</a>.
</li>
<li>Hinsen Konrad. 2020. <a href="https://guix.gnu.org/blog/2020/reproducible-computations-with-guix/">Reproducible Computations with Guix</a>.
</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2"><span class="section-number-2">2</span> Build From Sources</h2>
<div class="outline-text-2" id="text-2">
<p>
<img src="./export/img/build.png" class="img-responsive" alt="build.png" width="480">
This section describes how to compile-run the source code,
and generate the documentation.
All steps below are explained for GNU/Guix, a functional package manager
for reproducible computations.
All commands are executed in a container
with reproducible manner (thanks to option <i>time-machine</i>).
However, this project is not perfectly reproducible,
because it uses Python pip command which doesn't provide this mechanism.
For example, TensorFlow-2 is installed with pip command,
but should be with GNU/Guix.
GNU/Guix has TensorFlow but not the last version (yet).
Each subsection has a Bash command line to process an action,
and several configuration files as input.
For readability, these configuration files are not exported in
the documentation, but they can be read in the <i>dl.org</i> file.
</p>
</div>
<div id="outline-container-sec-2-1" class="outline-3">
<h3 id="sec-2-1"><span class="section-number-3">2.1</span> Extract Source Code</h3>
<div class="outline-text-3" id="text-2-1">
<p>
First, the source code must be extracted from the <i>dl.org</i> file.
Since org-mode is an Elisp module, GNU/Emacs is required.
</p>
<pre class="example">
$ guix time-machine --commit=0a93e8ce328c2dc43eaeddd1033ef67d456bc4df \
-- environment --container --ad-hoc emacs \
-- emacs dl.org --batch --eval "(progn (require 'org) (org-babel-tangle))"
</pre>
</div>
</div>
<div id="outline-container-sec-2-2" class="outline-3">
<h3 id="sec-2-2"><span class="section-number-3">2.2</span> Compile And Run</h3>
<div class="outline-text-3" id="text-2-2">
</div><div id="outline-container-sec-2-2-1" class="outline-4">
<h4 id="sec-2-2-1"><span class="section-number-4">2.2.1</span> Deep Learning</h4>
<div class="outline-text-4" id="text-2-2-1">
<p>
Both compilation and execution required some Python libraries
(machine learning framework, Hylang interface, etc.),
the following command line performs these actions.
</p>

<pre class="example">
$ guix time-machine --commit=0a93e8ce328c2dc43eaeddd1033ef67d456bc4df \
-- environment --container --network bash --ad-hoc python@3 nss-certs curl \
-- bash export/config/compile_dl.sh
</pre>
</div>
</div>
<div id="outline-container-sec-2-2-2" class="outline-4">
<h4 id="sec-2-2-2"><span class="section-number-4">2.2.2</span> Title Images</h4>
<div class="outline-text-4" id="text-2-2-2">
<p>
Each section has an illustration image
(from the book <i><a href="http://jv.gilead.org.il/rpaul/">Vingt Mille Lieues sous les mers</a></i>),
and some deep learning projects have technical diagrams (made with LaTeX).
The command line below downloads and compiles these images.
</p>
<pre class="example">
$ guix time-machine --commit=0a93e8ce328c2dc43eaeddd1033ef67d456bc4df \
-- environment --container --network bash --ad-hoc curl texlive poppler \
-- bash export/config/compile_img.sh
</pre>
</div>
</div>
</div>
<div id="outline-container-sec-2-3" class="outline-3">
<h3 id="sec-2-3"><span class="section-number-3">2.3</span> HTML Documentation</h3>
<div class="outline-text-3" id="text-2-3">
<p>
Generate a documentation in HTML format through org-mode.
</p>
<pre class="example">
$ guix time-machine --commit=0a93e8ce328c2dc43eaeddd1033ef67d456bc4df \
-- environment --container -m export/config/html_requirements.scm \
-- emacs dl.org --batch -l export/config/org_html.el
</pre>
</div>
</div>
<div id="outline-container-sec-2-4" class="outline-3">
<h3 id="sec-2-4"><span class="section-number-3">2.4</span> LaTeX Documentation</h3>
<div class="outline-text-3" id="text-2-4">
<p>
Generate a documentation in pdf format through org-mode and pdflatex.
</p>
<pre class="example">
$ guix time-machine --commit=0a93e8ce328c2dc43eaeddd1033ef67d456bc4df \
-- environment --container -m export/config/latex_requirements.scm \
-- emacs dl.org --batch -l export/config/org_latex.el
</pre>
</div>
</div>
<div id="outline-container-sec-2-5" class="outline-3">
<h3 id="sec-2-5"><span class="section-number-3">2.5</span> Export Documentation</h3>
<div class="outline-text-3" id="text-2-5">
<p>
Move both HTML and LaTeX documentations in <i>doc</i> folder.
</p>
<pre class="example">
$ rm -rf doc/ dl.tex &amp;&amp; mkdir doc/ &amp;&amp; mv export/ doc/ &amp;&amp; mv dl.html doc/ &amp;&amp; mv dl.pdf doc/
</pre>
</div>
</div>
</div>
<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3"><span class="section-number-2">3</span> Deep Learning Projects</h2>
<div class="outline-text-2" id="text-3">
</div><div id="outline-container-sec-3-1" class="outline-3">
<h3 id="sec-3-1"><span class="section-number-3">3.1</span> Documentation Structure</h3>
<div class="outline-text-3" id="text-3-1">
<p>
<img src="./export/img/doc.png" class="img-responsive" alt="doc.png" width="480">
Each project has a purpose, implementation, result and references sections.
In the implementation section, each code block has both a documentation (before)
and a test (after) parts associated.
</p>
</div>
</div>
<div id="outline-container-sec-3-2" class="outline-3">
<h3 id="sec-3-2"><span class="section-number-3">3.2</span> Logistic Regression Neural Network Classification From Scratch</h3>
<div class="outline-text-3" id="text-3-2">

<figure>
<p><img src="./export/img/lrnnc.png" class="img-responsive" alt="lrnnc.png" width="480">
</p>
</figure>
</div>
<div id="outline-container-sec-3-2-1" class="outline-4">
<h4 id="sec-3-2-1"><span class="section-number-4">3.2.1</span> Purpose</h4>
<div class="outline-text-4" id="text-3-2-1">
<p>
This module implements a basic neural network model
(Logistic Regression) for CIFAR-10 photo classification.
The objective of this project is to develop an algorithm to recognize animal
class over others (airplane, truck, etc.).
This neural network is implemented from scratch using NumPy, and CIFAR-10 dataset.
The CIFAR-10 dataset is a collection of images used in machine learning for
image recognition algorithms.
The CIFAR-10 dataset consists of 60000 32x32 color images in 10 classes,
with 6000 images per class.
The figure <a href="#fig:nn.png">3</a> represents the neural network architecture used.
</p>

<figure id="fig:nn.png">
<p><img src="./export/lrnnc/nn.png" class="img-responsive" alt="nn.png" width="700">
</p>
<figcaption><span class="figure-number">Figure 3:</span> A basic neural network for image classification</figcaption>
</figure>
</div>
</div>
<div id="outline-container-sec-3-2-2" class="outline-4">
<h4 id="sec-3-2-2"><span class="section-number-4">3.2.2</span> Implementation</h4>
<div class="outline-text-4" id="text-3-2-2">
</div><ol class="org-ol"><li><a id="sec-3-2-2-1" name="sec-3-2-2-1"></a>Initialization<br ><div class="outline-text-5" id="text-3-2-2-1">
<p>
Import necessaries modules: <code>cifar10</code>, <code>numpy</code> and <code>hy.loop</code>
(loop with recursion style).
Here the <code>keras</code> framework is only used for <code>cifar10</code> datasets.
<a id="src-import-modules" name="src-import-modules"></a>
</p>
<div class="org-src-container">

<pre class="src src-hy" id="src-import-modules">(<span class="org-keyword">require</span> [hy.contrib.loop [<span class="org-keyword">loop</span>]])
(<span class="org-keyword">require</span> [hytmf [test-case]])
(<span class="org-keyword">import</span> [keras.datasets
         [cifar10]])
(<span class="org-keyword">import</span> [matplotlib [pyplot]])
(<span class="org-keyword">import</span> [numpy <span class="org-keyword">:as</span> np])
</pre>
</div>


<p>
Load the CIFAR-10 dataset.
There are 50000 examples in the training dataset and 10000 in the test dataset.
Images are 32×32 pixels, with 3 channels, and y-axis contains image label.
The labels number are:
</p>
<ol class="org-ol">
<li>airplane
</li>
<li>automobile
</li>
<li>bird
</li>
<li>cat
</li>
<li>deer
</li>
<li>dog
</li>
<li>frog
</li>
<li>horse
</li>
<li>ship
</li>
<li>truck
</li>
</ol>
<p>
The shape of <code>train_set_x_orig</code> is 50000x32x32x3, and the shape of
<code>train_set_y_orig</code> is 50000x1.
It's similar for <code>test_set</code> with 10000 instead 50000,
because the number of examples is different.
<a id="src-import-dataset" name="src-import-dataset"></a>
</p>
<div class="org-src-container">

<pre class="src src-hy" id="src-import-dataset">(<span class="org-builtin">setv</span> [[train_set_x_orig train_set_y_orig] [test_set_x_orig test_set_y_orig]]
      (.load_data cifar10))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-dataset_orig</span> []
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> train_set_x_orig.shape '(50000, 32, 32, 3)))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> train_set_y_orig.shape '(50000, 1)))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> test_set_x_orig.shape '(10000, 32, 32, 3)))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> test_set_y_orig.shape '(10000, 1)))

  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">get</span> train_set_y_orig 1 0) 9))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">get</span> train_set_y_orig 12 0) 7))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">get</span> train_set_y_orig 123 0) 2))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">get</span> train_set_y_orig 3000 0) 3))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">get</span> train_set_y_orig 3103 0) 1))

  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">get</span> test_set_y_orig 1 0) 8))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">get</span> test_set_y_orig 12 0) 5))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">get</span> test_set_y_orig 149 0) 2))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">get</span> test_set_y_orig 3000 0) 5))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">get</span> test_set_y_orig 3210 0) 9)))
(test-case (test-dataset_orig))
</pre>
</div>


<p>
In order to recognize animal label only, labels in the dataset are changed:
</p>
<ul class="org-ul">
<li>1 for animal labels (2. bird, 3. cat, 4. deer, 5. dog, 6. frog, 7. horse)
</li>
<li>0 for others (0. airplane, 1. automobile, 8. ship, 9. truck)
</li>
</ul>
<p>
<a id="src-labels-dataset" name="src-labels-dataset"></a>
</p>
<div class="org-src-container">

<pre class="src src-hy" id="src-labels-dataset">(<span class="org-builtin">setv</span> <span class="org-variable-name">train_set_y_orig</span> (.array np (<span class="org-keyword">lfor</span> x train_set_y_orig
                                        (<span class="org-keyword">lfor</span> y x (<span class="org-builtin">int</span> (<span class="org-keyword">in</span> y (<span class="org-builtin">range</span> 2 7)))))))
(<span class="org-builtin">setv</span> <span class="org-variable-name">test_set_y_orig</span> (.array np (<span class="org-keyword">lfor</span> x test_set_y_orig
                                 (<span class="org-keyword">lfor</span> y x (<span class="org-builtin">int</span> (<span class="org-keyword">in</span> y (<span class="org-builtin">range</span> 2 7)))))))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-dataset_binary_label</span> []
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> train_set_y_orig.shape '(50000, 1)))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">get</span> train_set_y_orig 1 0) 0))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">get</span> train_set_y_orig 12 0) 0))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">get</span> train_set_y_orig 123 0) 1))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">get</span> train_set_y_orig 3000 0) 1))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">get</span> train_set_y_orig 3103 0) 0))

  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> test_set_y_orig.shape '(10000, 1)))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">get</span> test_set_y_orig 1 0) 0))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">get</span> test_set_y_orig 12 0) 1))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">get</span> test_set_y_orig 149 0) 1))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">get</span> test_set_y_orig 3000 0) 1))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">get</span> test_set_y_orig 3210 0) 0)))
(test-case (test-dataset_binary_label))
</pre>
</div>


<p>
Plot the first few images from CIFAR-10 dataset.
</p>
<div class="org-src-container">

<pre class="src src-hy">(.figure pyplot <span class="org-string">"cifar"</span>)
(<span class="org-keyword">loop</span> [[i 0]]
      (<span class="org-keyword">when</span> (<span class="org-builtin">&lt;</span> i 9)
        (.subplot pyplot (<span class="org-builtin">+</span> 330 1 i))
        (.imshow pyplot (<span class="org-builtin">get</span> train_set_x_orig i))
        (<span class="org-keyword">recur</span> (<span class="org-builtin">inc</span> i))))
(.savefig pyplot <span class="org-string">"cifar"</span>)
(.close pyplot)
</pre>
</div>

<figure>
<p><img src="./export/lrnnc/cifar.png" class="img-responsive" alt="cifar.png" width="480">
</p>
<figcaption><span class="figure-number">Figure 4:</span> CIFAR-10 dataset example</figcaption>
</figure>


<p>
For convenience (in matrix multiplication, etc.),
both <code>train_set_x</code> and <code>test_set_x</code> are reshaped, and transposed.
Images of shape <code>(n, p, 3)</code> are now of shape <code>(n x p x 3, 1)</code>, where <code>n</code> is
the number of examples, <code>p</code> is the number of pixels (32x32), and 3 for channels.
Both <code>train_set_x</code> and <code>test_set_x</code>
contain <code>[(n x p x 3, 1), the number of examples]</code>.
Both <code>train_set_y</code> and <code>train_set_y</code>
contain <code>[a label value, the number of examples]</code>.
<a id="src-flatten-dataset" name="src-flatten-dataset"></a>
</p>
<div class="org-src-container">

<pre class="src src-hy" id="src-flatten-dataset">(<span class="org-builtin">setv</span> <span class="org-variable-name">train_set_x_flatten</span> (.transpose np (.reshape
                                           train_set_x_orig
                                           (<span class="org-builtin">get</span> train_set_x_orig.shape 0)
                                           -1)))
(<span class="org-builtin">setv</span> <span class="org-variable-name">test_set_x_flatten</span> (.transpose np (.reshape
                                          test_set_x_orig
                                          (<span class="org-builtin">get</span> test_set_x_orig.shape 0)
                                          -1)))

(<span class="org-builtin">setv</span> <span class="org-variable-name">train_set_y</span> (.transpose np train_set_y_orig))
(<span class="org-builtin">setv</span> <span class="org-variable-name">test_set_y</span> (.transpose np test_set_y_orig))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-datasetx_flatten</span> []
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> train_set_x_flatten.shape '(3072, 50000)))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> test_set_x_flatten.shape '(3072, 10000))))
(test-case (test-datasetx_flatten))

(<span class="org-keyword">defn</span> <span class="org-function-name">test-datasety_transpose</span> []
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> train_set_y.shape '(1, 50000)))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">get</span> train_set_y 0 1) 0))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">get</span> train_set_y 0 12) 0))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">get</span> train_set_y 0 123) 1))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">get</span> train_set_y 0 3000) 1))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">get</span> train_set_y 0 3103) 0))

  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> test_set_y.shape '(1, 10000)))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">get</span> test_set_y 0 1) 0))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">get</span> test_set_y 0 12) 1))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">get</span> test_set_y 0 149) 1))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">get</span> test_set_y 0 3000) 1))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">get</span> test_set_y 0 3210) 0)))
(test-case (test-datasety_transpose))
</pre>
</div>


<p>
Normalization preprocessing step centers and standardizes the dataset.
255 is the maximum value of a pixel channel.
<a id="src-normalize-dataset" name="src-normalize-dataset"></a>
</p>
<div class="org-src-container">

<pre class="src src-hy" id="src-normalize-dataset">(<span class="org-builtin">setv</span> <span class="org-variable-name">train_set_x</span> (<span class="org-builtin">/</span> train_set_x_flatten 255))
(<span class="org-builtin">setv</span> <span class="org-variable-name">test_set_x</span> (<span class="org-builtin">/</span> test_set_x_flatten 255))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-datasetx_norm</span> []
  (<span class="org-keyword">assert</span> (np.isclose (<span class="org-builtin">get</span> train_set_x 0 0) 0.231372))
  (<span class="org-keyword">assert</span> (np.isclose (<span class="org-builtin">get</span> test_set_x 0 0) 0.619607)))
(test-case (test-datasetx_norm))
</pre>
</div>
</div>
</li>
<li><a id="sec-3-2-2-2" name="sec-3-2-2-2"></a>Functions<br ><div class="outline-text-5" id="text-3-2-2-2">
<p>
\(\operatorname{sigmoid}\) is a non-linear function
used for binary classification output.
It is used to predict the probability of an output.
This model has two labels: animal and no animal,
and its output will be the probability that an input image is an animal or not.
The definition of \(\operatorname{sigmoid}\) function is:
</p>
\begin{align*}
\operatorname{sigmoid}(x) & = \frac{1}{1 + e^{-x}}, \mathbb{R} \to [0,1] \\\\
\operatorname{sigmoid}'(x) & =
\operatorname{sigmoid}(x)(1 - \operatorname{sigmoid}(x))
\end{align*}
<p>
<a id="src-sigmoid" name="src-sigmoid"></a>
</p>
<div class="org-src-container">

<pre class="src src-hy" id="src-sigmoid">(<span class="org-keyword">defn</span> <span class="org-function-name">sigmoid</span> [Z]
  (<span class="org-builtin">/</span> 1 (<span class="org-builtin">+</span> 1 (.exp np (<span class="org-builtin">-</span> Z)))))

(<span class="org-keyword">defn</span> <span class="org-function-name">dsigmoid</span> [Z]
  (<span class="org-builtin">setv</span> <span class="org-variable-name">a</span> (sigmoid Z))
  (<span class="org-builtin">*</span> a (<span class="org-builtin">-</span> 1 a)))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-sigmoid</span> []
  (<span class="org-keyword">assert</span> (np.isclose (<span class="org-builtin">sum</span> (sigmoid(.array np [0 2]))) 1.380797))
  (<span class="org-keyword">assert</span> (np.isclose (.mean (sigmoid(.array np [0 2]))) 0.690398)))
(test-case (test-sigmoid))
</pre>
</div>


<p>
Different plots of \(\operatorname{sigmoid}\) function.
</p>
<div class="org-src-container">

<pre class="src src-hy">(.figure pyplot <span class="org-string">"sigmoid_1"</span>)
(<span class="org-builtin">setv</span> <span class="org-variable-name">x</span> (.linspace np -10 10 100))
(.plot pyplot x (sigmoid x))
(.xlabel pyplot <span class="org-string">"x"</span>)
(.ylabel pyplot <span class="org-string">"sigmoid(x)"</span>)
(.savefig pyplot <span class="org-string">"sigmoid_1"</span>)
(.close pyplot)

(.figure pyplot <span class="org-string">"sigmoid_2"</span>)
(<span class="org-builtin">setv</span> <span class="org-variable-name">x</span> (.linspace np -10 10 100))
(.plot pyplot x (sigmoid (<span class="org-builtin">*</span> 2 x)))
(.xlabel pyplot <span class="org-string">"x"</span>)
(.ylabel pyplot <span class="org-string">"sigmoid(x)"</span>)
(.savefig pyplot <span class="org-string">"sigmoid_2"</span>)
(.close pyplot)

(.figure pyplot <span class="org-string">"sigmoid_3"</span>)
(<span class="org-builtin">setv</span> <span class="org-variable-name">x</span> (.linspace np -10 10 100))
(.plot pyplot x (sigmoid (<span class="org-builtin">+</span> (<span class="org-builtin">*</span> 2 x) 5)))
(.xlabel pyplot <span class="org-string">"x"</span>)
(.ylabel pyplot <span class="org-string">"sigmoid(x)"</span>)
(.savefig pyplot <span class="org-string">"sigmoid_3"</span>)
(.close pyplot)

(.figure pyplot <span class="org-string">"sigmoid_4"</span>)
(<span class="org-builtin">setv</span> <span class="org-variable-name">x</span> (.linspace np -10 10 100))
(.plot pyplot x (sigmoid (<span class="org-builtin">+</span> (<span class="org-builtin">*</span> 100 x) 100)))
(.xlabel pyplot <span class="org-string">"x"</span>)
(.ylabel pyplot <span class="org-string">"sigmoid(x)"</span>)
(.savefig pyplot <span class="org-string">"sigmoid_4"</span>)
(.close pyplot)
</pre>
</div>

<figure>
<p><img src="./export/lrnnc/sigmoid_1.png" class="img-responsive" alt="sigmoid_1.png" width="480">
</p>
<figcaption><span class="figure-number">Figure 5:</span> Plot of \(\operatorname{sigmoid_1}\)</figcaption>
</figure>

<figure>
<p><img src="./export/lrnnc/sigmoid_2.png" class="img-responsive" alt="sigmoid_2.png" width="480">
</p>
<figcaption><span class="figure-number">Figure 6:</span> Plot of \(\operatorname{sigmoid_2}\)</figcaption>
</figure>

<figure>
<p><img src="./export/lrnnc/sigmoid_3.png" class="img-responsive" alt="sigmoid_3.png" width="480">
</p>
<figcaption><span class="figure-number">Figure 7:</span> Plot of \(\operatorname{sigmoid_3}\)</figcaption>
</figure>

<figure>
<p><img src="./export/lrnnc/sigmoid_4.png" class="img-responsive" alt="sigmoid_4.png" width="480">
</p>
<figcaption><span class="figure-number">Figure 8:</span> Plot of \(\operatorname{sigmoid_4}\)</figcaption>
</figure>


<p>
In order to train the model, a loss function (denoted as \(\mathcal{L}\))
is used to compute the error for a single training example.
A basic loss function would be:
</p>
\begin{align*}
\mathcal{L}(a, y) & = \frac{1}{2}(a - y)^{2}
\end{align*}
<p>
but it's a non-convex function. It contains local optimum points,
which lead to a convergence problem during gradient descent.
Instead, cross-entropy function is used because it's a convex function:
</p>
\begin{align*}
\mathcal{L}(a, y) & = -\sum_{x\in[1,m]} y(x) \log(a(x))
\end{align*}
<p>
For binary classification (\(M = 2\)):
</p>
\begin{align*}
\mathcal{L}(a, y) & = - (y(1) \log(a(1)) + y(2) \log(a(2))) \\
& = - (y(1) \log(a(1)) + (1 - y(1)) \log(1 - a(1))) \\
& = - (y \log(a) + (1 - y)\log(1 - a)) \\\\
\mathcal{L}'(a, y) & = -\frac{y}{a} + \frac{1 - y}{1 - a}
\end{align*}
<ul class="org-ul">
<li>if \(y = 1 \implies \mathcal{L}(a, 1) = -\log(a) \implies a\)
should be the largest
</li>
<li>if \(y = 0 \implies \mathcal{L}(a, 0) = -\log(1 - a) \implies 1 - a\)
should be the largest \(\implies a\) should be smaller
</li>
</ul>
<p>
Also, sometimes the parameter \(a\) is denoted \(\hat{y}\).
<a id="src-loss" name="src-loss"></a>
</p>
<div class="org-src-container">

<pre class="src src-hy" id="src-loss">(<span class="org-keyword">defn</span> <span class="org-function-name">loss</span> [A Y]
  (<span class="org-builtin">+</span> (<span class="org-builtin">*</span> Y (.log np A)) (<span class="org-builtin">*</span> (<span class="org-builtin">-</span> 1 Y) (.log np (<span class="org-builtin">-</span> 1 A)))))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-loss</span> []
  (<span class="org-keyword">assert</span> (np.isclose (.mean (loss 0.8 0.6)) -0.777661)))
(test-case (test-loss))
</pre>
</div>


<p>
The loss function is applied on only one output.
To know the model quality of the entire predictions,
a cost function (denoted as \(\mathcal{J}\)) is used,
it's the average of the loss functions.
The cost function is defined by:
</p>
\begin{align*}
\mathcal{J}(w,b) & = \frac{1}{m}\sum_{x\in[1,m]}\mathcal{L}(\hat{y}_i, y_i)
\end{align*}
<p>
<a id="src-cost" name="src-cost"></a>
</p>
<div class="org-src-container">

<pre class="src src-hy" id="src-cost">(<span class="org-keyword">defn</span> <span class="org-function-name">cost</span> [m A Y]
  (<span class="org-builtin">*</span> (<span class="org-builtin">/</span> -1 m) (.sum np (loss A Y))))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-cost</span> []
  (<span class="org-keyword">assert</span> (np.isclose
            (cost 3 (.array np [[0.8 0.9 0.4]]) (.array np [[1 1 0]]))
            0.279776)))
(test-case (test-cost))
</pre>
</div>
</div>
</li>
<li><a id="sec-3-2-2-3" name="sec-3-2-2-3"></a>Computation Graph<br ><div class="outline-text-5" id="text-3-2-2-3">
<p>
The architecture of the computation graph is the following two steps
(as shown in figure <a href="#fig:nn.png">3</a>):
</p>
<ul class="org-ul">
<li>Forward propagation:
it's a forward step from input (calculated weights-bias, features \(x\))
to output (\(\operatorname{sigmoid}\), loss function).
The objective is to calculate the loss of output
with the current weights-bias.
</li>
<li>Backward propagation: it's a backward step from output to input.
The objective is to calculate the gradient (chain rule derivative)
in order to minimize the cost function.
</li>
</ul>
</div>
<ol class="org-ol"><li><a id="sec-3-2-2-3-1" name="sec-3-2-2-3-1"></a>Forward Propagation<br ><div class="outline-text-6" id="text-3-2-2-3-1">
<p>
<a id="doc-forward-prop" name="doc-forward-prop"></a>
The final formula for forward propagation
with vectorization (matrix form) is:
</p>
\begin{align*}
Z & = W^{\operatorname{T}} \cdot X + b \\\\
A & = \operatorname{sigmoid}(Z)
\end{align*}
<p>
The dimensions associated are:
</p>
\begin{align*}
\dim Z & = 1 \times m \\\\
\dim A & = 1 \times m \\\\
\dim W & = n_x \times 1 \\\\
\dim X & = n_x \times m \\\\
\dim Y & = 1 \times m \\\\
\end{align*}
<p>
The operands can be represented in a matrix form:
</p>
\begin{align*}
X & =
\begin{bmatrix}
x_{11} & x_{12} & \dots & x_{1p} & \dots & x_{1m} \\
x_{21} & x_{22} & \dots & x_{2p} & \dots & x_{2m} \\
\dots & \dots & \dots & \dots & \dots & \dots \\
x_{p1} & x_{p2} & \dots & x_{pp} & \dots & x_{pm} \\
\dots & \dots & \dots & \dots & \dots & \dots \\
x_{n_x1} & x_{n_x2} & \dots & x_{n_xp} & \dots & x_{n_xm}
\end{bmatrix} \\\\
W & =
\begin{bmatrix}
w_{11} \\
w_{21} \\
\dots \\
w_{p1} \\
\dots \\
w_{n_x1}
\end{bmatrix} \\\\
Z & =
\begin{bmatrix} z_{11} & z_{12} & \dots & z_{1p} & \dots & z_{1m}
\end{bmatrix} \\\\
W^{\operatorname{T}} \cdot X & =
\begin{bmatrix}
w_{11} \times x_{11} + w_{21} \times x_{21} + w_{p1}
\times x_{p1} + w_{n_x1} \times x_{n_x1} \\
w_{11} \times x_{12} + w_{21} \times x_{22} + w_{p1}
\times x_{p2} + w_{n_x1} \times x_{n_x2} \\
\cdots \\
w_{11} \times x_{1p} + w_{21} \times x_{2p} + w_{p1}
\times x_{pp} + w_{n_x1} \times x_{n_xp} \\
\cdots \\
w_{11} \times x_{1m} + w_{21} \times x_{2m} + w_{p1}
\times x_{pm} + w_{n_x1} \times x_{n_xm}
\end{bmatrix}
\end{align*}

<p>
To understand the forward propagation formula, let \(m = 1\) and \(n_x = 2\):
</p>
\begin{align*}
z_{11} & = w_{11} \times x_{11} + w_{21} \times x_{21} + b \\\\
\hat{y}_{11} & = a_{11} = \operatorname{sigmoid}(z_{11}) \\\\
\mathcal{L}(a_{11}, y_{11}) & = y_{11} \log(a_{11}) +
(1 - y_{11}) \log(1 - a_{11})
\end{align*}
<p>
All features \(x\) are multiplied by weights \(w\), and the vectors dimension are:
\(\dim x_{ji} = \dim w_{j1} = \dim a_{11} = \dim b = \dim z_{11} = 1 \times 1\)
</p>

<p>
With \(m > 1\) and \(n_x = 2\) the forward propagation formula becomes: <br >
<code>begin-for</code> \(i\in [1, m]\): <br >
<code>|</code> \(z_{1i} = w_{11} \times x_{1i} + w_{21} \times x_{2i} + b\) <br >
<code>|</code> \(a_{1i} = \operatorname{sigmoid}(z_{1i})\) <br >
<code>|</code> \(\mathcal{L}(a_{1i}, y_{1i}) = y_{1i} \log(a_{1i}) + (1 - y_{1i}) \log(1 - a_{1i})\) <br >
<code>end-for</code> <br >
These equations confirm the final forward propagation formula,
and why matrix multiplications are used.
Also, the cost function is:
</p>
\begin{align*}
\mathcal{J} & = \frac{1}{m} \sum_{i\in[1,m]} \mathcal{L}(a_{1i}, y_{1i})
\end{align*}
</div>
</li>
<li><a id="sec-3-2-2-3-2" name="sec-3-2-2-3-2"></a>Backward Propagation<br ><div class="outline-text-6" id="text-3-2-2-3-2">
<p>
<a id="doc-backward-prop" name="doc-backward-prop"></a>
The final backward propagation formula is:
</p>
\begin{align*}
\partial{\tilde{Z}} & = A - Y \\\\
\partial{\tilde{W}} & = \frac{1}{m} X \cdot
\partial{\tilde{Z}}^{\operatorname{T}} \\\\
\partial{\tilde{b}} & = \frac{1}{m}\sum \partial{\tilde{Z}}
\end{align*}
<p>
The dimensions associated for each operand are:
</p>
\begin{align*}
\dim \partial{\tilde{Z}} & = 1 \times m \\\\
\dim \partial{\tilde{b}} & = 1 \times 1 \\\\
\dim \partial{\tilde{W}} & = nx \times 1 \\\\
\dim \partial{\tilde{X}} & = nx \times m
\end{align*}
<p>
The matrix representation for some operands are:
</p>
\begin{align*}
\partial{\tilde{Z}} & =
\begin{bmatrix}
\partial{\tilde{z}}_{11} & \partial{\tilde{z}}_{12} & \dots &
\partial{\tilde{z}}_{1p} & \dots & \partial{\tilde{z}}_{1m}
\end{bmatrix} \\\\
X \cdot \partial{\tilde{Z}}^{\operatorname{T}} & =
\begin{bmatrix}
x_{11} \times \partial{\tilde{z}}_{11} + x_{12} \times \partial{\tilde{z}}_{12}
+ \cdots + x_{1p} \times \partial{\tilde{z}}_{1p}
+ \cdots + x_{1m} \times \partial{\tilde{z}}_{1m} \\
x_{21} \times \partial{\tilde{z}}_{11} + x_{22} \times \partial{\tilde{z}}_{12}
+ \cdots + x_{2p} \times \partial{\tilde{z}}_{1p} + \cdots
+ x_{2m} \times \partial{\tilde{z}}_{1m} \\
\dots \\
x_{p1} \times \partial{\tilde{z}}_{11} + x_{p2} \times \partial{\tilde{z}}_{12}
+ \cdots + x_{pp} \times \partial{\tilde{z}}_{1p} + \cdots
+ x_{pm} \times \partial{\tilde{z}}_{1m} \\
\dots \\
x_{n_x1} \times \partial{\tilde{z}}_{11} + x_{n_x2}
\times \partial{\tilde{z}}_{12} + \cdots + x_{n_xp}
\times \partial{\tilde{z}}_{1p} + \cdots + x_{n_xm}
\times \partial{\tilde{z}}_{1m} \\
\end{bmatrix}
\end{align*}
<p>
In the same way,
to understand these formulas let \(m\) and \(n_x\) be constant values.
Here the objective is to optimize input parameters:
\(w_{11}\), \(w_{21}\), and \(b\).
Also, the notation used for chain rule is:
</p>
\begin{align*}
\partial{\tilde{g}} & = \frac{\partial{f}}{\partial{g}} \\
& = \frac{\partial{f}}{\partial{a}} \times \frac{\partial{a}}{\partial{g}} \\\\
\end{align*}
<p>
Below the calculation for backward propagation with \(m = 1\), and \(n_x = 2\),
</p>
\begin{align*}
\partial{\tilde{a_{11}}} & = \frac{\partial{\mathcal{L}}}{\partial{a_{11}}} \\
 & = -\frac{y_{11}}{a_{11}} + \frac{1 - y_{11}}{1 - a_{11}} \\\\
\partial{\tilde{z_{11}}} & = \frac{\partial{\mathcal{L}}}{\partial{z_{11}}} \\
& = \frac{\partial{\mathcal{L}}}{\partial{a_{11}}}
\times \frac{\partial{a_{11}}}{\partial{z_{11}}} \\
& = (-\frac{y_{11}}{a_{11}} + \frac{1 - y_{11}}{1 - a_{11}}) \times a_{11}
\times (1 - a_{11}) \\
& = a_{11} - y_{11} \\\\
\partial{\tilde{w_{11}}} & = \frac{\partial{\mathcal{L}}}{\partial{w_{11}}} \\
& = \frac{\partial{\mathcal{L}}}{\partial{z_{11}}}
\times \frac{\partial{z_{11}}}{\partial{w_{11}}} \\
& = x_{11} \times \partial{\tilde{z_{11}}} \\
& = x_{11} \times (a_{11} - y_{11}) \\\\
\partial{\tilde{w_{21}}} & = \frac{\partial{\mathcal{L}}}{\partial{w_{21}}} \\
& = \frac{\partial{\mathcal{L}}}{\partial{z_{11}}}
\times \frac{\partial{z_{11}}}{\partial{w_{21}}} \\
& = x_{21} \times \partial{\tilde{z_{11}}} \\
& = x_{21} \times (a_{11} - y_{11}) \\\\
\partial{\tilde{b}} & = \frac{\partial{\mathcal{L}}}{\partial{b}} \\
& = \frac{\partial{\mathcal{L}}}{\partial{z_{11}}}
\times \frac{\partial{z_{11}}}{\partial{b}} \\
& = \partial{\tilde{z_{11}}} \\
& = a_{11} - y_{11}
\end{align*}
<p>
Now, let \(m > 1\), \(n_x = 2\), then the backward propagation formula becomes: <br >
<code>begin-for</code> \(i\in [1, m]\): <br >
<code>|</code> \(\partial{\tilde{z_{1i}}} = a_{1i} - y_{1i}\) <br >
<code>|</code> \(\partial{\tilde{w_{11}}} \mathrel{{+}{=}} x_{1i} \times (a_{1i} - y_{1i})\) <br >
<code>|</code> \(\partial{\tilde{w_{21}}} \mathrel{{ + }{=}} x_{2i} \times (a_{1i} - y_{1i})\) <br >
<code>|</code> \(\partial{\tilde{b}} \mathrel{{+}{=}} a_{1i} - y_{1i}\) <br >
<code>end-for</code> <br >
\(\partial{\tilde{w_{11}}} \mathrel{{/}{=}} m\) <br >
\(\partial{\tilde{w_{21}}} \mathrel{{/}{=}} m\) <br >
\(\partial{\tilde{b}} \mathrel{{/}{=}} m\) <br >
Also, it's \(w_{11}\), \(w_{21}\) and not \(w_{1i}\), \(w_{2i}\),
because weights are the same between different examples contained,
and sum is used because \(\dim \partial{\tilde{b}}\) is \(1 \times 1\)
without vectorization, and must stay the same with vectorization.
</p>

<p>
Implement the \(\partial{\tilde{Z}}\) equation in backward propagation formula
described at <a href="#doc-backward-prop">doc-backward-prop</a>.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">dloss_dz</span> [A Y]
  (<span class="org-builtin">-</span> A Y))
</pre>
</div>


<p>
Implement the \(\partial{\tilde{W}}\) equation in backward propagation formula
described at <a href="#doc-backward-prop">doc-backward-prop</a>.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">dloss_dw</span> [X A Y]
  (.dot np X (.transpose np (dloss_dz A Y))))

(<span class="org-keyword">defn</span> <span class="org-function-name">dcost_dw</span> [m X A Y]
  (<span class="org-builtin">*</span> (<span class="org-builtin">/</span> 1 m) (dloss_dw X A Y)))
</pre>
</div>


<p>
Implement the \(\partial{\tilde{b}}\) equation in backward propagation formula
described at <a href="#doc-backward-prop">doc-backward-prop</a>.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">dloss_db</span> [A Y]
  (dloss_dz A Y))

(<span class="org-keyword">defn</span> <span class="org-function-name">dcost_db</span> [m A Y]
  (<span class="org-builtin">*</span> (<span class="org-builtin">/</span> 1 m) (.sum np (dloss_db A Y))))
</pre>
</div>


<p>
Process of computation graph, i.e. forward and backward propagation.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">propagate</span> [w b X Y]
  (<span class="org-builtin">setv</span> <span class="org-variable-name">A</span> (sigmoid (<span class="org-builtin">+</span> (.dot np (.transpose np w) X) b)))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">m</span> (<span class="org-builtin">get</span> X.shape 1))
  [(cost m A Y) (dcost_dw m X A Y) (dcost_db m A Y)])
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-fb_propagation</span> []
  (<span class="org-builtin">setv</span> <span class="org-variable-name">w</span> (.array np [[1.] [2.]]))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">b</span> 2.)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">X</span> (.array np [[1. 2. -1.] [3. 4. -3.2]]))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">Y</span> (.array np [[1 0 1]]))
  (propagate w b X Y)
  (<span class="org-keyword">assert</span> (np.isclose (<span class="org-builtin">get</span> (<span class="org-builtin">sum</span> (<span class="org-builtin">sum</span> (propagate w b X Y))) 0) 14.999530)))
(test-case (test-fb_propagation))
</pre>
</div>
</div>
</li>
<li><a id="sec-3-2-2-3-3" name="sec-3-2-2-3-3"></a>Gradient Descent<br ><div class="outline-text-6" id="text-3-2-2-3-3">
<p>
Implement the one step gradient descent algorithm:
run the process of computation graph
(both forward and backward propagation), and update parameters
(both weights and bias) in order to minimize the cost function.
This algorithm is repeated in a loop iteration with an arbitrary \(N\).
<a id="src-gradientd-lrnnc" name="src-gradientd-lrnnc"></a>
</p>
<div class="org-src-container">

<pre class="src src-hy" id="src-gradientd-lrnnc">(<span class="org-keyword">defn</span> <span class="org-function-name">gradient_descent</span> [w b X Y learning_rate nb_iter]
  (<span class="org-keyword">loop</span> [[i 0] [w_acc w] [b_acc b] [cost_acc []]]
        (<span class="org-builtin">setv</span> [cost dw db] (propagate w_acc b_acc X Y))
        (<span class="org-keyword">if</span> (<span class="org-builtin">&gt;=</span> i nb_iter)
            [w_acc b_acc dw db cost_acc]
            (<span class="org-keyword">recur</span>
              (<span class="org-builtin">inc</span> i)
              (<span class="org-builtin">-</span> w_acc (<span class="org-builtin">*</span> learning_rate dw))
              (<span class="org-builtin">-</span> b_acc (<span class="org-builtin">*</span> learning_rate db))
              (<span class="org-keyword">if</span> (<span class="org-builtin">%</span> i 100)
                  (<span class="org-builtin">+</span> cost_acc [cost])
                  cost_acc)))))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-gradient_descent</span> []
  (<span class="org-builtin">setv</span> <span class="org-variable-name">w</span> (np.array [[1.] [2.]]))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">b</span> 2.)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">X</span> (np.array [[1. 2. -1.] [3. 4. -3.2]]))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">Y</span> (np.array [[1. 0. 1.]]))
  (<span class="org-builtin">setv</span> [test_w_acc test_b_acc test_dw test_db test_cost_acc]
        (gradient_descent w b X Y 0.009 100))
  (<span class="org-keyword">assert</span> (np.isclose (test_w_acc.mean) 0.156463))
  (<span class="org-keyword">assert</span> (np.isclose (test_b_acc.mean) 1.925359))
  (<span class="org-keyword">assert</span> (np.isclose (test_dw.mean) 1.036073))
  (<span class="org-keyword">assert</span> (np.isclose (test_db.mean) 0.219464))
  (<span class="org-keyword">assert</span> (np.isclose (<span class="org-builtin">sum</span> test_cost_acc) 304.243381)))
(test-case (test-gradient_descent))
</pre>
</div>
</div>
</li></ol>
</li>
<li><a id="sec-3-2-2-4" name="sec-3-2-2-4"></a>Model<br ><div class="outline-text-5" id="text-3-2-2-4">
<p>
Predict whether the label is either 0 or 1.
It uses both weights and bias calculated by the gradient descent, so the model
must be trained before calling this function.
Whether the label is &gt; 0.5 then the value for this label will be 1,
and it means that the image is an animal.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">predict</span> [w b X]
  (<span class="org-builtin">setv</span> <span class="org-variable-name">A</span> (sigmoid (<span class="org-builtin">+</span> (.dot np (.transpose np w) X) b)))
  (.array np (<span class="org-keyword">lfor</span> x A (<span class="org-keyword">lfor</span> y x (<span class="org-builtin">int</span> (<span class="org-builtin">&gt;</span> y 0.5))))))
</pre>
</div>

<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-predict</span> []
  (<span class="org-builtin">setv</span> <span class="org-variable-name">w</span> (.array np [[0.1124579] [0.23106775]]))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">b</span> -0.3)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">X</span> (.array np [[1. -1.1 -3.2] [1.2 2. 0.1]]))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_predict_out</span> (predict w b X))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">get</span> (<span class="org-builtin">get</span> test_predict_out 0) 0) 1))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">get</span> (<span class="org-builtin">get</span> test_predict_out 0) 1) 1))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">get</span> (<span class="org-builtin">get</span> test_predict_out 0) 2) 0)))
(test-case (test-predict))
</pre>
</div>


<p>
The model process all necessaries steps to build
the Logistic Regression algorithm:
</p>
<ul class="org-ul">
<li>Forward propagation to calculate the cost of the model
</li>
<li>Backward propagation to calculate the new weights and bias
</li>
<li>Update the weights and bias to minimize the cost function (train the model)
</li>
<li>The final step, once the model is trained, predicts whether labels
are either 0 or 1
</li>
</ul>
<p>
<a id="src-model-lrnnc" name="src-model-lrnnc"></a>
</p>
<div class="org-src-container">

<pre class="src src-hy" id="src-model-lrnnc">(<span class="org-keyword">defn</span> <span class="org-function-name">model</span> [X_train Y_train X_test learning_rate nb_iter]
  (<span class="org-builtin">setv</span> <span class="org-variable-name">w_zero</span>  (.zeros np [(<span class="org-builtin">get</span> X_train.shape 0) 1]))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">b_zero</span> 0)
  (<span class="org-builtin">setv</span> [w_acc b_acc dw db cost_acc]
        (gradient_descent w_zero b_zero X_train Y_train learning_rate nb_iter))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">Y_pred_train</span> (predict w_acc b_acc X_train))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">Y_pred_test</span> (predict w_acc b_acc X_test))
  [w_acc b_acc dw db cost_acc Y_pred_train Y_pred_test])
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-model</span> []
  (<span class="org-builtin">setv</span> [w_acc b_acc dw db cost_acc Y_pred_train Y_pred_test]
        (model train_set_x train_set_y test_set_x 0.005 2000))

  (<span class="org-keyword">assert</span> (np.isclose (<span class="org-builtin">-</span> 100 (<span class="org-builtin">*</span> (np.mean (np.abs (<span class="org-builtin">-</span> Y_pred_train train_set_y)))
                                100)) 76.466000))
  (<span class="org-keyword">assert</span> (np.isclose (<span class="org-builtin">-</span> 100 (<span class="org-builtin">*</span> (np.mean (np.abs (<span class="org-builtin">-</span> Y_pred_test test_set_y)))
                                100)) 76.72))

  (<span class="org-keyword">assert</span> (np.isclose (w_acc.mean) -6.4368831e-05))
  (<span class="org-keyword">assert</span> (np.isclose (b_acc.mean) 0.065301))
  (<span class="org-keyword">assert</span> (np.isclose (dw.mean) 3.0001236e-06))
  (<span class="org-keyword">assert</span> (np.isclose (db.mean) -0.00583625184))
  (<span class="org-keyword">assert</span> (np.isclose (<span class="org-builtin">sum</span> cost_acc) 1042.453483))
  (<span class="org-keyword">assert</span> (np.isclose (Y_pred_train.mean) 0.5091))
  (<span class="org-keyword">assert</span> (np.isclose (Y_pred_test.mean) 0.5072)))
(test-case (test-model))
</pre>
</div>
</div>
</li></ol>
</div>
<div id="outline-container-sec-3-2-3" class="outline-4">
<h4 id="sec-3-2-3"><span class="section-number-4">3.2.3</span> Results</h4>
<div class="outline-text-4" id="text-3-2-3">
<p>
In order to measure the quality of the model, a simple accuracy method is used.
The prediction of the model for both training and test sets are compared with
both training and test from the CIFAR-10 dataset.
Both training and test accuracy are about 76 %.
It shows a high bias (underfitting problem).
Underfitting means that the model is not complex enough.
It can neither model the training data nor generalize to new data.
Nevertheless, a test accuracy of 76 % for a simple Logistic Regression
model is tolerable.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">print</span> <span class="org-string">"Model running..."</span>)
(<span class="org-builtin">setv</span> [w_acc b_acc dw db cost_acc Y_pred_train Y_pred_test]
      (model train_set_x train_set_y test_set_x 0.005 2000))

(<span class="org-keyword">print</span> (.format <span class="org-string">"train accuracy: {} %"</span>
                (<span class="org-builtin">-</span> 100
                   (<span class="org-builtin">*</span> (np.mean (np.abs (<span class="org-builtin">-</span> Y_pred_train train_set_y))) 100))))
(<span class="org-keyword">print</span> (.format <span class="org-string">"test accuracy: {} %"</span>
                (<span class="org-builtin">-</span> 100
                   (<span class="org-builtin">*</span> (np.mean (np.abs (<span class="org-builtin">-</span> Y_pred_test test_set_y))) 100))))

</pre>
</div>


<pre class="example">
train accuracy: 76.46600000000001 %
test accuracy: 76.72 %
</pre>


<p>
An example of two pictures correctly classified.
The first one represents a dog, and comes from the vector <code>test_set_x_orig[12]</code>.
The prediction output for this image (<code>Y_pred_test[0 12]</code> = 1), is compared
to the vector labels from CIFAR-10 (<code>test_set_y_orig[12 0]</code> = 1).
The second image represents an airplane, and comparisons are similar
to the previous one with 0 as label instead 1.
</p>
<div class="org-src-container">

<pre class="src src-hy">(.figure pyplot <span class="org-string">"cifar_picture_classified_animal"</span>)
(.imshow pyplot (<span class="org-builtin">get</span> test_set_x_orig 12))
(.savefig pyplot <span class="org-string">"cifar_picture_classified_animal"</span>)
(.close pyplot)
(<span class="org-builtin">get</span> test_set_y_orig 12 0)
(<span class="org-builtin">get</span> Y_pred_test 0 12)

(.figure pyplot <span class="org-string">"cifar_picture_classified_noanimal"</span>)
(.imshow pyplot (<span class="org-builtin">get</span> test_set_x_orig 10))
(.savefig pyplot <span class="org-string">"cifar_picture_classified_noanimal"</span>)
(.close pyplot)
(<span class="org-builtin">get</span> test_set_y_orig 10 0)
(<span class="org-builtin">get</span> Y_pred_test 0 10)
</pre>
</div>

<figure>
<p><img src="./export/lrnnc/cifar_picture_classified_animal.png" class="img-responsive" alt="cifar_picture_classified_animal.png" width="240">
</p>
<figcaption><span class="figure-number">Figure 9:</span> A dog from CIFAR-10 picture (classified as animal)</figcaption>
</figure>

<figure>
<p><img src="./export/lrnnc/cifar_picture_classified_noanimal.png" class="img-responsive" alt="cifar_picture_classified_noanimal.png" width="240">
</p>
<figcaption><span class="figure-number">Figure 10:</span> An airplane from CIFAR-10 picture (classified as others)</figcaption>
</figure>
</div>
</div>
<div id="outline-container-sec-3-2-4" class="outline-4">
<h4 id="sec-3-2-4"><span class="section-number-4">3.2.4</span> References</h4>
<div class="outline-text-4" id="text-3-2-4">
<ul class="org-ul">
<li>deeplearning.ai. 2020. <a href="https://www.deeplearning.ai/deep-learning-specialization/">Deep Learning Specialization</a>.
</li>
<li>Brownlee Jason. 2019. <a href="https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/">How to Develop a CNN From Scratch for CIFAR-10 Photo Classification</a>.
</li>
<li>Brownlee Jason. 2019. <a href="https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/">Overfitting and Underfitting With Machine Learning Algorithms</a>.
</li>
<li>Joy Ashwin. 2020. <a href="https://pythonistaplanet.com/cifar-10-image-classification-using-keras/">Cifar-10 Image Classification Using Keras</a>.
</li>
<li>U Toronto. 2009. <a href="https://www.cs.toronto.edu/~kriz/cifar.html">The CIFAR-10 dataset</a>.
</li>
<li>Arunava. 2018. <a href="https://towardsdatascience.com/derivative-of-the-sigmoid-function-536880cf918e">Derivative of the Sigmoid function</a>.
</li>
<li>Olah Christopher. 2015. <a href="https://colah.github.io/posts/2015-08-Backprop/">Calculus on Computational Graphs: Backpropagation</a>.
</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-sec-3-3" class="outline-3">
<h3 id="sec-3-3"><span class="section-number-3">3.3</span> Deep Neural Network Classification From Scratch</h3>
<div class="outline-text-3" id="text-3-3">

<figure>
<p><img src="./export/img/dnnc.png" class="img-responsive" alt="dnnc.png" width="480">
</p>
</figure>
</div>
<div id="outline-container-sec-3-3-1" class="outline-4">
<h4 id="sec-3-3-1"><span class="section-number-4">3.3.1</span> Purpose</h4>
<div class="outline-text-4" id="text-3-3-1">
<p>
This module implements a Deep Neural Network model
for CIFAR-10 photo classification.
This project is similar
to <a href="#sec-3-2">Logistic Regression Neural Network Classification From Scratch</a> with a
deep neural network instead a basic neural network.
The figure <a href="#fig:dnn.png">12</a> represents the neural network architecture used.
</p>

<figure id="fig:dnn.png">
<p><img src="./export/dnnc/dnn.png" class="img-responsive" alt="dnn.png" width="500">
</p>
<figcaption><span class="figure-number">Figure 12:</span> A Deep Neural Network for image classification</figcaption>
</figure>
</div>
</div>
<div id="outline-container-sec-3-3-2" class="outline-4">
<h4 id="sec-3-3-2"><span class="section-number-4">3.3.2</span> Implementation</h4>
<div class="outline-text-4" id="text-3-3-2">
</div><ol class="org-ol"><li><a id="sec-3-3-2-1" name="sec-3-3-2-1"></a>Initialization<br ><div class="outline-text-5" id="text-3-3-2-1">
<p>
The code block below is described at <a href="#src-import-modules">src-import-modules</a>.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">require</span> [hy.contrib.loop [<span class="org-keyword">loop</span>]])
(<span class="org-keyword">require</span> [hytmf [test-case]])
(<span class="org-keyword">import</span> [keras.datasets
         [cifar10]])
(<span class="org-keyword">import</span> [matplotlib [pyplot]])
(<span class="org-keyword">import</span> [numpy <span class="org-keyword">:as</span> np])
</pre>
</div>


<p>
The code block below is described at <a href="#src-import-dataset">src-import-dataset</a>.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> [[train_set_x_orig train_set_y_orig] [test_set_x_orig test_set_y_orig]]
      (.load_data cifar10))
</pre>
</div>


<p>
The code block below is described at <a href="#src-labels-dataset">src-labels-dataset</a>.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> <span class="org-variable-name">train_set_y_orig</span> (.array np (<span class="org-keyword">lfor</span> x train_set_y_orig
                                        (<span class="org-keyword">lfor</span> y x (<span class="org-builtin">int</span> (<span class="org-keyword">in</span> y (<span class="org-builtin">range</span> 2 7)))))))
(<span class="org-builtin">setv</span> <span class="org-variable-name">test_set_y_orig</span> (.array np (<span class="org-keyword">lfor</span> x test_set_y_orig
                                 (<span class="org-keyword">lfor</span> y x (<span class="org-builtin">int</span> (<span class="org-keyword">in</span> y (<span class="org-builtin">range</span> 2 7)))))))
</pre>
</div>


<p>
The code block below is described at <a href="#src-flatten-dataset">src-flatten-dataset</a>.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> <span class="org-variable-name">train_set_x_flatten</span> (.transpose np (.reshape
                                           train_set_x_orig
                                           (<span class="org-builtin">get</span> train_set_x_orig.shape 0)
                                           -1)))
(<span class="org-builtin">setv</span> <span class="org-variable-name">test_set_x_flatten</span> (.transpose np (.reshape
                                          test_set_x_orig
                                          (<span class="org-builtin">get</span> test_set_x_orig.shape 0)
                                          -1)))

(<span class="org-builtin">setv</span> <span class="org-variable-name">train_set_y</span> (.transpose np train_set_y_orig))
(<span class="org-builtin">setv</span> <span class="org-variable-name">test_set_y</span> (.transpose np test_set_y_orig))
</pre>
</div>


<p>
The code block below is described at <a href="#src-normalize-dataset">src-normalize-dataset</a>.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> <span class="org-variable-name">train_set_x</span> (<span class="org-builtin">/</span> train_set_x_flatten 255))
(<span class="org-builtin">setv</span> <span class="org-variable-name">test_set_x</span> (<span class="org-builtin">/</span> test_set_x_flatten 255))
</pre>
</div>
</div>
</li>
<li><a id="sec-3-3-2-2" name="sec-3-3-2-2"></a>Functions<br ><div class="outline-text-5" id="text-3-3-2-2">
<p>
The \(\operatorname{sigmoid}\) function is described at <a href="#src-sigmoid">src-sigmoid</a>.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">sigmoid</span> [Z]
  (<span class="org-builtin">/</span> 1 (<span class="org-builtin">+</span> 1 (.exp np (<span class="org-builtin">-</span> Z)))))

(<span class="org-keyword">defn</span> <span class="org-function-name">dsigmoid</span> [Z]
  (<span class="org-builtin">setv</span> <span class="org-variable-name">a</span> (sigmoid Z))
  (<span class="org-builtin">*</span> a (<span class="org-builtin">-</span> 1 a)))
</pre>
</div>


<p>
The loss function is described at <a href="#src-loss">src-loss</a>.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">loss</span> [A Y]
  (<span class="org-builtin">+</span> (<span class="org-builtin">*</span> Y (.log np A)) (<span class="org-builtin">*</span> (<span class="org-builtin">-</span> 1 Y) (.log np (<span class="org-builtin">-</span> 1 A)))))
</pre>
</div>


<p>
The cost function is described at <a href="#src-cost">src-cost</a>.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">cost</span> [m A Y]
  (<span class="org-builtin">*</span> (<span class="org-builtin">/</span> -1 m) (.sum np (loss A Y))))
</pre>
</div>


<p>
\(\operatorname{ReLU}\) function has at least two benefits
over \(\operatorname{sigmoid}\) function:
</p>
<ul class="org-ul">
<li>\(\operatorname{ReLU}\) function avoids the vanishing gradient problem
(slow gradient descent), because \(\operatorname{sigmoid}\) derivative is always
smaller than 0, then when \(N\) hidden layers
use \(\operatorname{sigmoid}\) function-like, \(N\) small derivatives are
multiplied together. \(\operatorname{ReLU}\) gradient is either 0 (when \(x < 0\))
or 1 (when \(x > 0\)), so whatever the number of hidden layers.
In other words, \(\operatorname{ReLU}\) gradient is faster learning than
\(\operatorname{sigmoid}\) gradient.
</li>
<li>\(\operatorname{ReLU}\) is more computationally efficient to compute because
it's defined with a max function, while \(\operatorname{sigmoid}\) is defined by
an exponential function.
</li>
</ul>

<p>
The definition of \(\operatorname{ReLU}\) is:
</p>
\begin{align*}
\operatorname{ReLU}(x) & = \max(0,x) \\\\
\operatorname{ReLU}'(x) & = \begin{cases} 0 & \text{if } x < 0,
\\ 1 & \text{if } x > 0,\\ undefined & \text{if } x = 0 \end{cases}
\end{align*}
<p>
<a id="src-relu" name="src-relu"></a>
</p>
<div class="org-src-container">

<pre class="src src-hy" id="src-relu">(<span class="org-keyword">defn</span> <span class="org-function-name">relu</span> [Z]
  (.maximum np 0 Z))

(<span class="org-keyword">defn</span> <span class="org-function-name">drelu</span> [Z]
  (.array np (<span class="org-keyword">lfor</span> x Z (<span class="org-keyword">lfor</span> y x (<span class="org-keyword">if</span> (<span class="org-builtin">&lt;=</span> y 0)
                                     0
                                     1)))))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-relu</span> []
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (relu 20) 20)))
(test-case (test-relu))
</pre>
</div>


<p>
Plot of \(\operatorname{ReLU}\) function.
</p>
<div class="org-src-container">

<pre class="src src-hy">(.figure pyplot <span class="org-string">"relu"</span>)
(<span class="org-builtin">setv</span> <span class="org-variable-name">x</span> (.linspace np -10 10 100))
(.plot pyplot x (relu x))
(.xlabel pyplot <span class="org-string">"x"</span>)
(.ylabel pyplot <span class="org-string">"relu(x)"</span>)
(.savefig pyplot <span class="org-string">"relu"</span>)
(.close pyplot)
</pre>
</div>

<figure>
<p><img src="./export/dnnc/relu.png" class="img-responsive" alt="relu.png" width="480">
</p>
<figcaption><span class="figure-number">Figure 13:</span> Plot of \(\operatorname{ReLU}\)</figcaption>
</figure>
</div>
</li>
<li><a id="sec-3-3-2-3" name="sec-3-3-2-3"></a>Computation Graph<br ><ol class="org-ol"><li><a id="sec-3-3-2-3-1" name="sec-3-3-2-3-1"></a>Forward Propagation<br ><div class="outline-text-6" id="text-3-3-2-3-1">
<p>
<a id="doc-forward-prop-dnnc" name="doc-forward-prop-dnnc"></a>
The formula for forward (respectively, backward) propagation in Deep Neural
Network is a generalization of the Logistic Regression one.
The formulas for Logistic Regression are described
at <a href="#doc-forward-prop">doc-forward-prop</a> and <a href="#doc-backward-prop">doc-backward-prop</a>.
The formula for forward propagation is:
</p>
\begin{align*}
Z^{[l]} & = W^{[l]} \cdot A^{[l-1]} + b^{[l]} \\\\
A^{[l]} & = \sigma(Z^{[l]}) \\\\
A^{[0]} & = X
\end{align*}
<p>
The loss function for each example is
\(\mathcal{L}(a^{[L]}_{n_{hn}^{[L]}i}, y^{[L]}_{n_{hn}^{[L]}i})\),
\(\forall i \in [1,m]\), and the cost function \(\mathcal{J}\) is:
</p>
\begin{align*}
\mathcal{J} & = \frac{1}{m} \sum_{i\in[1,m]}
\mathcal{L}(a^{[L]}_{n_{hn}^{[L]}i}
y^{[L]}_{n_{hn}^{[L]}i}), \forall i \in [1,m]
\end{align*}
<p>
The dimension associated are:
</p>
\begin{align*}
\dim Z^{[l]} & = n_{hn}^{[l]} \times m \\\\
\dim A^{[l]} & = n_{hn}^{[l]} \times m \\\\
\dim W^{[l]} & = n_{hn}^{[l]} \times n_{hn}^{[l-1]} \\\\
\dim b^{[l]} & = n_{hn}^{[l]} \times 1
\end{align*}
<p>
A neural network can have several activation functions.
In this architecture both \(\operatorname{ReLU}\) function
and \(\operatorname{sigmoid}\) function are used, thus in the equation above,
\(\sigma\) can be either \(\operatorname{ReLU}\) or \(\operatorname{sigmoid}\).
Using only a linear activation function in neural network would just output
a linear function, thus a non-linear function is needed to increase
the range of functions that the model can learn. If \(\sigma\) is the identity
function then the model output will be linear.
</p>

<p>
The examples below are explained with three layers neural network
(the input layer isn't counted):
\(A^{[0]}\) represents the input layer, \(A^{[1]}\)
and \(A^{[2]}\) the hidden neurons layer, \(A^{[3]}\) the output layer.
In other words the number of hidden layers is three
(\(A^{[1]}\), \(A^{[2]}\), \(A^{[3]}\)), and the number of neurons in each
hidden layer (noted \(n^{[i]}_{hn}\), \(hn\) means hidden neuron)
is different for each one,
for example \(A^{[2]}\) has five (\(n_{hn}^{[2]}\)) neurons
(as shown in figure <a href="#fig:dnn.png">12</a>).
The dimension values are:
</p>
\begin{align*}
m & = 1 \\\\
n_{hn}^{[0]} & = n_x = 3 \\\\
n_{hn}^{[1]} & = 4 \\\\
n_{hn}^{[2]} & = 5 \\\\
n_{hn}^{[3]} & = 1
\end{align*}
<p>
The matrix associated are:
</p>
\begin{align*}
A^{[0]} & = X \\\\
A^{[1]} & =
\begin{bmatrix}
a_{11}^{[1]} \\
a_{21}^{[1]} \\
a_{31}^{[1]} \\
a_{41}^{[1]}
\end{bmatrix} \\\\
A^{[2]} & =
\begin{bmatrix}
a_{11}^{[2]} \\
a_{21}^{[2]} \\
a_{31}^{[2]} \\
a_{41}^{[2]} \\
a_{51}^{[2]}
\end{bmatrix} \\\\
A^{[3]} & = \hat{Y} \\\\
b^{[1]} & =
\begin{bmatrix}
b_{11}^{[1]} \\
b_{21}^{[1]} \\
b_{31}^{[1]} \\
b_{41}^{[1]}
\end{bmatrix} \\\\
X & =
\begin{bmatrix}
x_{11} \\
x_{21} \\
x_{31}
\end{bmatrix} \\\\
W^{[1]} & =
\begin{bmatrix}
w_{11}^{[1]} & w_{12}^{[1]} & w_{13}^{[1]} \\
w_{21}^{[1]} & w_{22}^{[1]} & w_{23}^{[1]} \\
w_{31}^{[1]} & w_{32}^{[1]} & w_{33}^{[1]} \\
w_{41}^{[1]} & w_{42}^{[1]} & w_{43}^{[1]}
\end{bmatrix} \\\\
W^{[3]} & =
\begin{bmatrix}
w_{11}^{[3]} & w_{12}^{[3]} & w_{13}^{[3]} &w_{14}^{[3]}
\end{bmatrix}
\end{align*}
<p>
The following result for forward propagation with \(l = 1\) is similar to
the forward propagation for Logistic Regression described at <a href="#doc-forward-prop">doc-forward-prop</a>.
</p>
\begin{align*}
Z^{[1]} & =
W^{[1]} \cdot A^{[0]} + b^{[1]} = \begin{bmatrix}
w_{11}^{[1]} \times x_{11} + w_{12}^{[1]} \times x_{21} + w_{13}^{[1]}
\times x_{31} + b_{11}^{[1]} \\
w_{21}^{[1]} \times x_{11} + w_{22}^{[1]} \times x_{21} + w_{23}^{[1]}
\times x_{31} + b_{21}^{[1]} \\
w_{31}^{[1]} \times x_{11} + w_{32}^{[1]} \times x_{21} + w_{33}^{[1]}
\times x_{31} + b_{31}^{[1]} \\
w_{41}^{[1]} \times x_{11} + w_{42}^{[1]} \times x_{21} + w_{43}^{[1]}
\times x_{31} + b_{41}^{[1]} \\
\end{bmatrix}
\end{align*}
<p>
The following two equations are equivalent:
</p>
\begin{align*}
z^{[1]}_{11} & = w_{11}^{[1]} \times x_{11} + w_{12}^{[1]}
\times x_{21} + w_{13}^{[1]} \times x_{31} + b_{11}^{[1]} \\\\
z_{11} & = w_{11} \times x_{11} + w_{21} \times x_{21} + b
\end{align*}
<p>
The second one comes from Logistic Regression.
Moreover, the \(W\) matrix is transposed,
because \(\dim W^{[l]} = n_{hn}^{[l]} \times n_{x}\)
instead of \(n_{x} \times n_{hn}^{[l]}\) (\(n_x = n_{hn}^{[l-1]}\))
in Logistic Regression.
</p>

<p>
Implement the \(Z^{[l]}\) equation in forward propagation formula
described at <a href="#doc-forward-prop-dnnc">doc-forward-prop-dnnc</a>.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">linear_forward</span> [A W b]
  (<span class="org-builtin">+</span> (.dot np W A) b))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-linear_forward</span> []
  (np.random.seed 1)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">A</span> (np.random.randn 3 2))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">W</span> (np.random.randn 1 3))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">b</span> (np.random.randn 1 1))
  (<span class="org-keyword">assert</span> (np.isclose (.mean (linear_forward A W b)) 1.014326)))
(test-case (test-linear_forward))
</pre>
</div>


<p>
The cache mechanism returns both function results and arguments.
They are saved during forward propagation and used during backward propagation.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">cache_sys</span> [func <span class="org-type">&amp;rest</span> args]
  [(func <span class="org-keyword">#*</span> args) [<span class="org-keyword">#*</span> args]])
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-cache_sys</span> []
  (np.random.seed 1)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">A</span> (np.random.randn 3 2))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">W</span> (np.random.randn 1 3))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">b</span> (np.random.randn 1 1))
  (<span class="org-keyword">assert</span> (np.isclose
            (.mean (<span class="org-builtin">get</span> (cache_sys linear_forward A W b) 0)) 1.014326))
  (<span class="org-keyword">assert</span> (np.isclose
            (.mean (<span class="org-builtin">get</span> (cache_sys linear_forward A W b) 1 0)) -0.337447))
  (<span class="org-keyword">assert</span> (np.isclose
            (.mean (<span class="org-builtin">get</span> (cache_sys linear_forward A W b) 1 1)) 0.434214))
  (<span class="org-keyword">assert</span> (np.isclose
            (.mean (<span class="org-builtin">get</span> (cache_sys linear_forward A W b) 1 2)) -0.249370)))
(test-case (test-cache_sys))
</pre>
</div>


<p>
Implement the \(A^{[l]}\) equation in forward propagation formula
described at <a href="#doc-forward-prop-dnnc">doc-forward-prop-dnnc</a>.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">activation_forward</span> [A_prev W b func]
  (<span class="org-builtin">setv</span> [Z linear_cache] (cache_sys linear_forward A_prev W b))
  (<span class="org-builtin">setv</span> [A activation_cache] (cache_sys func Z))
  [A [linear_cache activation_cache]])
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-activation_forward</span> []
  (np.random.seed 1)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">A_prev</span> (np.random.randn 3 2))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">W</span> (np.random.randn 1 3))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">b</span> (np.random.randn 1 1))
  (<span class="org-keyword">assert</span> (np.isclose (<span class="org-builtin">len</span> (activation_forward A_prev W b sigmoid)) 2))
  (<span class="org-keyword">assert</span> (np.isclose
            (.mean (<span class="org-builtin">get</span> (activation_forward A_prev W b sigmoid) 0)) 0.594282))
  (<span class="org-keyword">assert</span> (np.isclose
            (.mean (<span class="org-builtin">get</span> (<span class="org-builtin">get</span> (<span class="org-builtin">get</span> (activation_forward A_prev W b sigmoid) 1) 0)
                        0)) -0.337447))
  (<span class="org-keyword">assert</span> (np.isclose
            (.mean (<span class="org-builtin">get</span> (<span class="org-builtin">get</span> (<span class="org-builtin">get</span> (activation_forward A_prev W b sigmoid) 1) 0)
                        1)) 0.434214))
  (<span class="org-keyword">assert</span> (np.isclose
            (.mean (<span class="org-builtin">get</span> (<span class="org-builtin">get</span> (<span class="org-builtin">get</span> (activation_forward A_prev W b sigmoid) 1) 0)
                        2)) -0.249370))
  (<span class="org-keyword">assert</span> (np.isclose
            (.mean (<span class="org-builtin">get</span> (<span class="org-builtin">get</span> (<span class="org-builtin">get</span> (activation_forward A_prev W b sigmoid) 1) 1)
                        0)) 1.014326))
  (<span class="org-keyword">assert</span> (np.isclose (<span class="org-builtin">len</span> (activation_forward A_prev W b relu)) 2))
  (<span class="org-keyword">assert</span> (np.isclose
            (.mean (<span class="org-builtin">get</span> (activation_forward A_prev W b relu) 0))
            1.631476687327087))
  (<span class="org-keyword">assert</span> (np.isclose
            (.mean (<span class="org-builtin">get</span> (<span class="org-builtin">get</span> (<span class="org-builtin">get</span> (activation_forward A_prev W b relu) 1)
                             0) 0)) -0.337447))
  (<span class="org-keyword">assert</span> (np.isclose
            (.mean (<span class="org-builtin">get</span> (<span class="org-builtin">get</span> (<span class="org-builtin">get</span> (activation_forward A_prev W b relu) 1) 0)
                        1)) 0.434214))
  (<span class="org-keyword">assert</span> (np.isclose
            (.mean (<span class="org-builtin">get</span> (<span class="org-builtin">get</span> (<span class="org-builtin">get</span> (activation_forward A_prev W b relu) 1) 0)
                        2)) -0.249370))
  (<span class="org-keyword">assert</span> (np.isclose
            (.mean (<span class="org-builtin">get</span> (<span class="org-builtin">get</span> (<span class="org-builtin">get</span> (activation_forward A_prev W b relu) 1) 1)
                        0)) 1.014326)))
(test-case (test-activation_forward))
</pre>
</div>


<p>
Implement the forward propagation in all hidden layers.
It consists of
propagating \(A^{[i]} = \sigma(Z^{[i]})\), \(\forall i \in [1, L-1]\),
with \(\sigma = \operatorname{ReLU}\).
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">hidden_layer_forward</span> [X param]
  (<span class="org-builtin">setv</span> <span class="org-variable-name">num_layer</span> (<span class="org-builtin">int</span> (<span class="org-builtin">/</span> (<span class="org-builtin">len</span> param) 2)))
  (<span class="org-keyword">loop</span> [[i 1] [A_prev X] [caches []]]
        (<span class="org-keyword">if</span> (<span class="org-builtin">&gt;=</span> i num_layer)
            [A_prev caches]
            (<span class="org-keyword">do</span> (<span class="org-builtin">setv</span> [A act_cache]
                      (activation_forward A_prev
                                  (<span class="org-builtin">get</span> param (<span class="org-builtin">+</span> <span class="org-string">"W"</span> (<span class="org-builtin">str</span> i)))
                                  (<span class="org-builtin">get</span> param (<span class="org-builtin">+</span> <span class="org-string">"b"</span> (<span class="org-builtin">str</span> i))) relu))
                (<span class="org-keyword">recur</span> (<span class="org-builtin">inc</span> i) A (<span class="org-builtin">+</span> caches [act_cache]))))))
</pre>
</div>


<p>
Implement the forward propagation for the output layer (last layer).
It consists of computing \(A^{[L]} = \sigma(Z^{[L]})\),
with \(\sigma = \operatorname{sigmoid}\), \(L\) the last layer.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">output_layer_forward</span> [A param]
  (<span class="org-builtin">setv</span> <span class="org-variable-name">num_layer</span> (<span class="org-builtin">int</span> (<span class="org-builtin">/</span> (<span class="org-builtin">len</span> param) 2)))
  (activation_forward A
              (<span class="org-builtin">get</span> param (<span class="org-builtin">+</span> <span class="org-string">"W"</span> (<span class="org-builtin">str</span> num_layer)))
              (<span class="org-builtin">get</span> param (<span class="org-builtin">+</span> <span class="org-string">"b"</span> (<span class="org-builtin">str</span> num_layer))) sigmoid))
</pre>
</div>


<p>
Implement the complete forward propagation described at <a href="#doc-forward-prop-dnnc">doc-forward-prop-dnnc</a>.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">model_layer_forward</span> [X params]
  (<span class="org-builtin">setv</span> [A h_cache] (hidden_layer_forward X params))
  (<span class="org-builtin">setv</span> [AL o_cache] (output_layer_forward A params))
  [AL (<span class="org-builtin">+</span> h_cache [o_cache])])
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-model_layer_forward</span> []
  (np.random.seed 1)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">X</span> (np.random.randn 5 4))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">W1</span> (np.random.randn 4 5))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">b1</span> (np.random.randn 4 1))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">W2</span> (np.random.randn 3 4))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">b2</span> (np.random.randn 3 1))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">W3</span> (np.random.randn 1 3))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">b3</span> (np.random.randn 1 1))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">param</span> {<span class="org-string">"W1"</span> W1 <span class="org-string">"W2"</span> W2 <span class="org-string">"W3"</span> W3 <span class="org-string">"b1"</span> b1 <span class="org-string">"b2"</span> b2 <span class="org-string">"b3"</span> b3})
  (<span class="org-builtin">setv</span> [AL caches] (model_layer_forward X param))
  (<span class="org-keyword">assert</span> (np.isclose (.mean AL) 0.7759899100529316))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">len</span> caches) 3))
  (<span class="org-keyword">assert</span> (np.isclose (.mean (<span class="org-builtin">get</span> (<span class="org-builtin">get</span> (<span class="org-builtin">get</span> caches 0) 0) 0)) -0.133364))
  (<span class="org-keyword">assert</span> (np.isclose (.mean (<span class="org-builtin">get</span> (<span class="org-builtin">get</span> (<span class="org-builtin">get</span> caches 1) 0) 0)) 1.027273))
  (<span class="org-keyword">assert</span> (np.isclose (.mean (<span class="org-builtin">get</span> (<span class="org-builtin">get</span> (<span class="org-builtin">get</span> caches 2) 0) 0)) 1.809404))
  (<span class="org-keyword">assert</span> (np.isclose (.mean (<span class="org-builtin">get</span> (<span class="org-builtin">get</span> (<span class="org-builtin">get</span> caches 0) 0) 1)) -0.045831))
  (<span class="org-keyword">assert</span> (np.isclose (.mean (<span class="org-builtin">get</span> (<span class="org-builtin">get</span> (<span class="org-builtin">get</span> caches 0) 0) 2)) -0.033542))
  (<span class="org-keyword">assert</span> (np.isclose (.mean (<span class="org-builtin">get</span> (<span class="org-builtin">get</span> (<span class="org-builtin">get</span> caches 2) 1) 0)) 3.481814)))
(test-case (test-model_layer_forward))
</pre>
</div>
</div>
</li>
<li><a id="sec-3-3-2-3-2" name="sec-3-3-2-3-2"></a>Backward Propagation<br ><div class="outline-text-6" id="text-3-3-2-3-2">
<p>
<a id="doc-backward-prop-dnnc" name="doc-backward-prop-dnnc"></a>
The final formula for backward propagation is:
</p>
\begin{align*}
\partial{\tilde{W}^{[l]}} & = \frac{1}{m} \partial{\tilde{Z}^{[l]}} \cdot \partial{\tilde{A}^{[l-1]\operatorname{T}}} \\\\
\partial{\tilde{b}^{[l]}} & = \frac{1}{m} \sum \partial{\tilde{Z}^{[l]}} \\\\
\partial{\tilde{A}^{[l-1]}} & = W^{[l]\operatorname{T}} \cdot \partial{\tilde{Z}^{[l]}} \\\\
\partial{\tilde{Z}^{[l]}} & = \partial{\tilde{A}^{[l]}} \odot \sigma'(Z^{[l]}) \\\\
\partial{\tilde{A}^{[L]}} & = \frac{\partial{\mathcal{L}(A^{[L]}, Y)}}{\partial{\tilde{A}^{[L]}}}
\end{align*}
<p>
The symbol \(\odot\) is the Hadamard product.
The procedure to understand the backward propagation formula is to break down it
without vectorization, and with fixed dimensions.
Let \(L = 2\), \(m = 1\), \(n_{hn}^{[0]} = n_x = 3\), \(n_{hn}^{[1]} = 4\),
\(n_{hn}^{[2]} = 2\) be the fixed values for matrix dimension, and let
</p>
\begin{align*}
Z^{[1]} & = W^{[1]} \cdot A^{[0]} + b^{[1]}, \dim = 4 \times 1 \\\\
A^{[1]} & = \sigma_{1}(Z^{[1]}), \dim = 4 \times 1 \\\\
Z^{[2]} & = W^{[2]} \cdot A^{[1]} + b^{[2]}, \dim = 2 \times 1 \\\\
A^{[2]} & = \sigma_{2}(Z^{[2]}), \dim = 2 \times 1 \\\\
\sigma_{1} & = \operatorname{ReLU} \\\\
\sigma_{2} & = \operatorname{sigmoid} \\\\
\mathcal{L} & (A^{[2]}, Y), \dim = 2 \times 1
\end{align*}

<p>
Respecting of the chain rule:
</p>
\begin{align*}
\partial{{\tilde{a}^{[2]}_{11}}} & = \frac{\partial{\mathcal{L}}}
{\partial{\tilde{a}^{[2]}_{11}}} \\
& = -\frac{y^{[2]}_{11}}{a^{[2]}_{11}} +
\frac{1 - y^{[2]}_{11}}{1 - a^{[2]}_{11}} \\\\
\partial{{\tilde{a}^{[2]}_{21}}} & = \frac{\partial{\mathcal{L}}}
{\partial{\tilde{a}^{[2]}_{21}}} \\
& = -\frac{y^{[2]}_{21}}{a^{[2]}_{21}} +
\frac{1 - y^{[2]}_{21}}{1 - a^{[2]}_{21}} \\\\
\partial{\tilde{z}^{[2]}_{11}} & = \frac{\partial{\mathcal{L}}}
{\partial{z^{[2]}_{11}}} \\
& = \frac{\partial{\mathcal{L}}}{\partial{a^{[2]}_{11}}} \times
\frac{\partial{a^{[2]}_{11}}}{\partial{z^{[2]}_{11}}} \\
& = \partial{a^{[2]}_{11}} \times \sigma_{2}'(z^{[2]}_{11}) \\
& = (-\frac{y^{[2]}_{11}}{a^{[2]}_{11}} + \frac{1 - y^{[2]}_{11}}
{1 - a^{[2]}_{11}}) \times a^{[2]}_{11} \times (1 - a^{[2]}_{11}) \\
& = a^{[2]}_{11} - y^{[2]}_{11} \\\\
\partial{\tilde{z}^{[2]}_{21}} & = \frac{\partial{\mathcal{L}}}
{\partial{z^{[2]}_{21}}} \\
& = \frac{\partial{\mathcal{L}}}{\partial{a^{[2]}_{21}}} \times
\frac{\partial{a^{[2]}_{21}}}{\partial{z^{[2]}_{21}}} \\
& = \partial{a^{[2]}_{21}} \times \sigma_{2}'(z^{[2]}_{21}) \\
& = (-\frac{y^{[2]}_{21}}{a^{[2]}_{21}} + \frac{1 - y^{[2]}_{21}}
{1 - a^{[2]}_{21}}) \times a^{[2]}_{21} \times (1 - a^{[2]}_{21}) \\
& = a^{[2]}_{21} - y^{[2]}_{21} \\\\
\partial{\tilde{w}^{[2]}_{11}} & = \frac{\partial{\mathcal{L}}}
{\partial{w^{[2]}_{11}}} \\
& = \frac{\partial{\mathcal{L}}}{\partial{z^{[2]}_{11}}} \times
\frac{\partial{z^{[2]}_{11}}}{\partial{w^{[2]}_{11}}} \\
& = a^{[1]}_{11} \times \partial{\tilde{z}^{[2]}_{11}} \\\\
\partial{\tilde{w}^{[2]}_{21}} & = \frac{\partial{\mathcal{L}}}
{\partial{w^{[2]}_{21}}} \\
& = \frac{\partial{\mathcal{L}}}{\partial{z^{[2]}_{21}}} \times
\frac{\partial{z^{[2]}_{21}}}{\partial{w^{[2]}_{21}}} \\
& = a^{[1]}_{11} \times \partial{\tilde{z}^{[2]}_{21}} \\\\
\partial{\tilde{w}^{[2]}_{12}} & = \frac{\partial{\mathcal{L}}}
{\partial{w^{[2]}_{12}}} \\
& = \frac{\partial{\mathcal{L}}}{\partial{z^{[2]}_{11}}} \times
\frac{\partial{z^{[2]}_{11}}}{\partial{w^{[2]}_{12}}} \\
& = a^{[1]}_{21} \times \partial{\tilde{z}^{[2]}_{11}} \\\\
\partial{\tilde{w}^{[2]}_{22}} & = \frac{\partial{\mathcal{L}}}
{\partial{w^{[2]}_{22}}} \\
& = \frac{\partial{\mathcal{L}}}{\partial{z^{[2]}_{21}}} \times
\frac{\partial{z^{[2]}_{21}}}{\partial{w^{[2]}_{22}}} \\
& = a^{[1]}_{21} \times \partial{\tilde{z}^{[2]}_{21}} \\\\
& \vdots \\\\
\partial{\tilde{w}^{[2]}_{24}} & = \frac{\partial{\mathcal{L}}}
{\partial{w^{[2]}_{24}}} \\
& = \frac{\partial{\mathcal{L}}}{\partial{z^{[2]}_{21}}} \times
\frac{\partial{z^{[2]}_{21}}}{\partial{w^{[2]}_{24}}} \\
& = a^{[1]}_{41} \times \partial{\tilde{z}^{[2]}_{21}} \\\\
\partial{\tilde{b}^{[2]}_{11}} & = \frac{\partial{\mathcal{L}}}
{\partial{b}^{[2]}_{11}} \\
& = \frac{\partial{\mathcal{L}}}{\partial{z^{[2]}_{11}}} \times
\frac{\partial{z^{[2]}_{11}}}{\partial{b}^{[2]}_{11}} \\
& = \partial{\tilde{z}^{[2]}_{11}} \\\\
\partial{\tilde{b}^{[2]}_{21}} & = \frac{\partial{\mathcal{L}}}
{\partial{b}^{[2]}_{21}} \\
& = \frac{\partial{\mathcal{L}}}{\partial{z^{[2]}_{21}}} \times
\frac{\partial{z^{[2]}_{21}}}{\partial{b}^{[2]}_{21}} \\
& = \partial{\tilde{z}^{[2]}_{21}} \\\\
\partial{{\tilde{a}^{[1]}_{11}}} & = \frac{\partial{\mathcal{L}}}
{\partial{a^{[1]}_{11}}} \\
& = \frac{\partial{\mathcal{L}}}{\partial{z^{[2]}_{11}}} \times
\frac{\partial{z^{[2]}_{11}}}{\partial{a}^{[1]}_{11}} +
\frac{\partial{\mathcal{L}}}{\partial{z^{[2]}_{21}}} \times
\frac{\partial{z^{[2]}_{21}}}{\partial{a}^{[1]}_{11}} \\
& = \partial{\tilde{z}^{[2]}_{11}} \times w^{[2]}_{11} +
\partial{\tilde{z}^{[2]}_{21}} \times w^{[2]}_{21} \\\\
& \vdots \\\\
\partial{{\tilde{a}^{[1]}_{41}}} & = \frac{\partial{\mathcal{L}}}
{\partial{a^{[1]}_{41}}} \\
& = \frac{\partial{\mathcal{L}}}{\partial{z^{[2]}_{11}}} \times
\frac{\partial{z^{[2]}_{11}}}{\partial{a}^{[1]}_{41}} +
\frac{\partial{\mathcal{L}}}{\partial{z^{[2]}_{21}}} \times
\frac{\partial{z^{[2]}_{21}}}{\partial{a}^{[1]}_{41}} \\
& = \partial{\tilde{z}^{[2]}_{11}} \times w^{[2]}_{14} +
\partial{\tilde{z}^{[2]}_{21}} \times w^{[2]}_{24} \\\\
\partial{\tilde{z}^{[1]}_{11}} & = \frac{\partial{\mathcal{L}}}
{\partial{z^{[1]}_{11}}} \\
& = \frac{\partial{\mathcal{L}}}{\partial{a^{[1]}_{11}}} \times
\frac{\partial{a^{[1]}_{11}}}{\partial{z^{[1]}_{11}}} \\
& = \partial{{\tilde{a}^{[1]}_{11}}} \times \sigma_{1}'(z^{[1]}_{11}) \\\\
& \vdots \\\\
\partial{\tilde{z}^{[1]}_{41}} & = \frac{\partial{\mathcal{L}}}
{\partial{z^{[1]}_{41}}} \\
& = \frac{\partial{\mathcal{L}}}{\partial{a^{[1]}_{41}}} \times
\frac{\partial{a^{[1]}_{41}}}{\partial{z^{[1]}_{41}}} \\
& = \partial{{\tilde{a}^{[1]}_{41}}} \times \sigma{1}'(z^{[1]}_{41}) \\\\
\partial{\tilde{w}^{[1]}_{11}} & = \frac{\partial{\mathcal{L}}}
{\partial{w^{[1]}_{11}}} \\
& = \frac{\partial{\mathcal{L}}}{\partial{z^{[1]}_{11}}} \times
\frac{\partial{z^{[1]}_{11}}}{\partial{w^{[1]}_{11}}} \\
& = a^{[0]}_{11} \times \partial{\tilde{z}^{[1]}_{11}} \\\\
\partial{\tilde{w}^{[1]}_{21}} & = \frac{\partial{\mathcal{L}}}
{\partial{w^{[1]}_{21}}} \\
& = \frac{\partial{\mathcal{L}}}{\partial{z^{[1]}_{21}}} \times
\frac{\partial{z^{[1]}_{21}}}{\partial{w^{[1]}_{21}}} \\
& = a^{[0]}_{11} \times \partial{\tilde{z}^{[1]}_{21}} \\\\
& \vdots \\\\
\partial{\tilde{w}^{[1]}_{43}} & = \frac{\partial{\mathcal{L}}}
{\partial{w^{[1]}_{43}}} \\
& = \frac{\partial{\mathcal{L}}}{\partial{z^{[1]}_{41}}} \times
\frac{\partial{z^{[1]}_{41}}}{\partial{w^{[1]}_{43}}} \\
& = a^{[0]}_{31} \times \partial{\tilde{z}^{[1]}_{41}} \\\\
\partial{\tilde{b}^{[1]}_{11}} & = \frac{\partial{\mathcal{L}}}
{\partial{b}^{[1]}_{11}} \\
& = \frac{\partial{\mathcal{L}}}{\partial{z^{[1]}_{11}}} \times
\frac{\partial{z^{[1]}_{11}}}{\partial{b}^{[1]}_{11}} \\
& = \partial{\tilde{z}^{[1]}_{11}} \\\\
& \vdots \\\\
\partial{\tilde{b}^{[1]}_{41}} & = \frac{\partial{\mathcal{L}}}
{\partial{b}^{[1]}_{41}} \\
& = \frac{\partial{\mathcal{L}}}{\partial{z^{[1]}_{41}}} \times
\frac{\partial{z^{[1]}_{41}}}{\partial{b}^{[1]}_{41}} \\
& = \partial{\tilde{z}^{[1]}_{41}}
\end{align*}
<p>
With \(m > 1\) and the same other dimensions
(\(n_{hn}^{[0]} = n_x = 3\), \(n_{hn}^{[1]} = 4\), \(n_{hn}^{[2]} = 2\)),
the above equations become: <br >
<code>for</code> \(i\in [1, m]\):
</p>
\begin{align*}
\partial{{\tilde{a}^{[2]}_{1i}}} & = \frac{\partial{\mathcal{L}}}
{\partial{\tilde{a}^{[2]}_{1i}}} \\\\
\partial{{\tilde{a}^{[2]}_{2i}}} & = \frac{\partial{\mathcal{L}}}
{\partial{\tilde{a}^{[2]}_{2i}}} \\\\
\partial{\tilde{z}^{[2]}_{1i}} & = \partial{a^{[2]}_{1i}} \times
\sigma_{2}'(z^{[2]}_{1i}) \\\\
\partial{\tilde{z}^{[2]}_{2i}} & = \partial{a^{[2]}_{2i}} \times
\sigma_{2}'(z^{[2]}_{2i}) \\\\
\partial{\tilde{w}^{[2]}_{11}} & \mathrel{{+}{=}} a^{[1]}_{1i} \times
\partial{\tilde{z}^{[2]}_{1i}} \\\\
& \vdots \\\\
\partial{\tilde{w}^{[2]}_{24}} & \mathrel{{+}{=}} a^{[1]}_{4i} \times
\partial{\tilde{z}^{[2]}_{2i}} \\\\
\partial{\tilde{b}^{[2]}_{11}} & \mathrel{{+}{=}}
\partial{\tilde{z}^{[2]}_{1i}} \\\\
\partial{\tilde{b}^{[2]}_{21}} & \mathrel{{+}{=}}
\partial{\tilde{z}^{[2]}_{2i}} \\\\
\partial{{\tilde{a}^{[1]}_{1i}}} & = \partial{\tilde{z}^{[2]}_{1i}} \times
w^{[2]}_{11} + \partial{\tilde{z}^{[2]}_{2i}} \times w^{[2]}_{21} \\\\
& \vdots \\\\
\partial{{\tilde{a}^{[1]}_{4i}}} & = \partial{\tilde{z}^{[2]}_{1i}} \times
w^{[2]}_{14} + \partial{\tilde{z}^{[2]}_{2i}} \times w^{[2]}_{24} \\\\
\partial{\tilde{z}^{[1]}_{1i}} & = \partial{{\tilde{a}^{[1]}_{1i}}} \times
\sigma_{1}'(z^{[1]}_{1i}) \\\\
& \vdots \\\\
\partial{\tilde{z}^{[1]}_{4i}} & = \partial{{\tilde{a}^{[1]}_{4i}}} \times
\sigma{1}'(z^{[1]}_{4i}) \\\\
\partial{\tilde{w}^{[1]}_{11}} & \mathrel{{+}{=}} a^{[0]}_{1i} \times
\partial{\tilde{z}^{[1]}_{1i}} \\\\
& \vdots \\\\
\partial{\tilde{w}^{[1]}_{43}} & \mathrel{{+}{=}}
a^{[0]}_{3i} \times \partial{\tilde{z}^{[1]}_{4i}} \\\\
\partial{\tilde{b}^{[1]}_{11}} & \mathrel{{+}{=}}
\partial{\tilde{z}^{[1]}_{1i}} \\\\
& \vdots \\\\
\partial{\tilde{b}^{[1]}_{41}} & \mathrel{{+}{=}}
\partial{\tilde{z}^{[1]}_{4i}}
\end{align*}
<p>
In case of both no fixed dimension and with matrix forms,
the above equations are the same as <a href="#doc-backward-prop-dnnc">doc-backward-prop-dnnc</a>.
</p>

<p>
Implement the three equations \(\partial{\tilde{W}^{[l]}}\),
\(\partial{\tilde{b}^{[l]}}\), \(\partial{\tilde{A}^{[l-1]}}\),
as described at <a href="#doc-backward-prop-dnnc">doc-backward-prop-dnnc</a>.
These equations are returned to produce a linear backward propagation.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">dcost_dw</span> [m A_prev dZ]
  (<span class="org-builtin">*</span> (<span class="org-builtin">/</span> 1 m) (.dot np dZ (.transpose np A_prev))))

(<span class="org-keyword">defn</span> <span class="org-function-name">dcost_db</span> [m dZ]
  (<span class="org-builtin">*</span> (<span class="org-builtin">/</span> 1 m) (.sum np dZ <span class="org-constant">:axis</span> 1 <span class="org-constant">:keepdims</span> <span class="org-constant">True</span>)))

(<span class="org-keyword">defn</span> <span class="org-function-name">dloss_daprev</span> [W dZ]
  (.dot np (.transpose np W) dZ))

(<span class="org-keyword">defn</span> <span class="org-function-name">linear_backward</span> [cache dZ]
  (<span class="org-builtin">setv</span> [A_prev W b] cache)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">m</span> (<span class="org-builtin">get</span> A_prev.shape 1))
  [(dloss_daprev W dZ) (dcost_dw m A_prev dZ) (dcost_db m dZ)])
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-linear_backward</span> []
  (np.random.seed 1)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">dZ</span> (np.random.randn 3 4))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">A</span> (np.random.randn 5 4))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">W</span> (np.random.randn 3 5))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">b</span> (np.random.randn 3 1))
  (<span class="org-builtin">setv</span> [test_daprev test_dw test_db] (linear_backward [A W b] dZ))
  (<span class="org-keyword">assert</span> (np.isclose (test_daprev.mean) 0.048344507))
  (<span class="org-keyword">assert</span> (np.isclose (test_dw.mean) 0.11302689))
  (<span class="org-keyword">assert</span> (np.isclose (test_db.mean) -0.130786)))
(test-case (test-linear_backward))
</pre>
</div>


<p>
Implement the \(\partial{\tilde{Z}^{[l]}}\) equation in backward propagation
formula described at <a href="#doc-backward-prop-dnnc">doc-backward-prop-dnnc</a>.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">activation_backward</span> [dfunc dA Z]
  (<span class="org-builtin">*</span> dA (dfunc Z)))

(<span class="org-keyword">defn</span> <span class="org-function-name">linear_activation_backward</span> [cache dfunc dA]
  (<span class="org-builtin">setv</span> [linear_cache activation_cache] cache)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">dZ</span> (activation_backward dfunc dA (<span class="org-builtin">get</span> activation_cache 0)))
  (linear_backward linear_cache dZ))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-linear_activation_backward</span> []
  (np.random.seed 1)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">dA</span> (np.random.randn 1 2))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">A</span> (np.random.randn 3 2))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">W</span> (np.random.randn 1 3))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">b</span> (np.random.randn 1 1))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">Z</span> (np.random.randn 2 1 1))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">linear_activation_cache</span> [[A W b] Z])
  (<span class="org-builtin">setv</span> [[test_daprev test_dw test_db] test_cache test_dz]
        (linear_activation_backward linear_activation_cache dsigmoid dA))
  (<span class="org-keyword">assert</span> (np.isclose (test_daprev.mean) 0.039350))
  (<span class="org-keyword">assert</span> (np.isclose (test_dw.mean) -0.0307574))
  (<span class="org-keyword">assert</span> (np.isclose (test_db.mean) 0.180336))
  (<span class="org-keyword">assert</span> (np.isclose (test_cache.mean) 0.240041))
  (<span class="org-keyword">assert</span> (np.isclose (test_dz.mean) 0.123340))
  (<span class="org-builtin">setv</span> [[test_daprev test_dw test_db] test_cache test_dz]
        (linear_activation_backward linear_activation_cache drelu dA))
  (<span class="org-keyword">assert</span> (np.isclose (test_daprev.mean) 0.0))
  (<span class="org-keyword">assert</span> (np.isclose (test_dw.mean) 0.0))
  (<span class="org-keyword">assert</span> (np.isclose (test_db.mean) 0.0))
  (<span class="org-keyword">assert</span> (np.isclose (test_cache.mean) 0.0))
  (<span class="org-keyword">assert</span> (np.isclose (test_dz.mean) 0.0)))
(test-case (test-linear_activation_backward))
</pre>
</div>


<p>
Implement the backward propagation for the input layer
(last layer in forward, but the first in backward).
It consists of computing \(\partial{\tilde{A}^{[L]}}\),
\(\partial{\tilde{A}^{[L-1]}}\), \(\partial{\tilde{Z}^{[L]}}\),
\(\partial{\tilde{W}^{[L]}}\) and \(\partial{\tilde{b}^{[L]}}\), with
\(L\) the last layer, \(\sigma = \operatorname{sigmoid}\).
The formula is described at <a href="#doc-backward-prop-dnnc">doc-backward-prop-dnnc</a>.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">input_layer_backward</span> [AL Y caches]
  (<span class="org-builtin">setv</span> <span class="org-variable-name">num_layer</span> (<span class="org-builtin">len</span> caches))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">dALL</span> (<span class="org-builtin">+</span> (.divide np (<span class="org-builtin">-</span> Y) AL) (.divide np (<span class="org-builtin">-</span> 1 Y) (<span class="org-builtin">-</span> 1 AL))))
  (<span class="org-builtin">setv</span> [dAL_prev dW db] (linear_activation_backward
                           (<span class="org-builtin">get</span> caches (<span class="org-builtin">-</span> num_layer 1)) dsigmoid dALL))
  {(<span class="org-builtin">+</span> <span class="org-string">"dA"</span> (<span class="org-builtin">str</span> (<span class="org-builtin">-</span> num_layer 1))) dAL_prev
   (<span class="org-builtin">+</span> <span class="org-string">"dW"</span> (<span class="org-builtin">str</span> num_layer)) dW
   (<span class="org-builtin">+</span> <span class="org-string">"db"</span> (<span class="org-builtin">str</span> num_layer)) db})
</pre>
</div>


<p>
Implement the backward propagation in all hidden layers.
It consists of computing \(\partial{\tilde{A}^{[i]}}\),
\(\partial{\tilde{A}^{[i-1]}}\), \(\partial{\tilde{Z}^{[i]}}\),
\(\partial{\tilde{W}^{[i]}}\) and \(\partial{\tilde{b}^{[i]}}\),
\(\forall i \in [1, L-1]\), with \(\sigma = \operatorname{ReLU}\).
The formula is described at <a href="#doc-backward-prop-dnnc">doc-backward-prop-dnnc</a>.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">hidden_layer_backward</span> [AL Y caches grads_prev]
  (<span class="org-builtin">setv</span> <span class="org-variable-name">num_layer</span> (<span class="org-builtin">len</span> caches))
  (<span class="org-keyword">loop</span> [[l (<span class="org-builtin">-</span> num_layer 2)] [grads grads_prev]]
        (<span class="org-keyword">if</span> (<span class="org-builtin">&lt;</span> l 0)
            grads
            (<span class="org-keyword">do</span> (<span class="org-builtin">setv</span> [dAL_prev dW db]
                      (linear_activation_backward
                        (<span class="org-builtin">get</span> caches l)
                        drelu
                        (<span class="org-builtin">get</span> grads (<span class="org-builtin">+</span> <span class="org-string">"dA"</span> (<span class="org-builtin">str</span> (<span class="org-builtin">+</span> l 1))))))
                (<span class="org-keyword">recur</span>
                  (<span class="org-builtin">dec</span> l)
                  {<span class="org-keyword">#**</span> grads
                   <span class="org-keyword">#**</span> {(<span class="org-builtin">+</span> <span class="org-string">"dA"</span> (<span class="org-builtin">str</span> l)) dAL_prev
                        (<span class="org-builtin">+</span> <span class="org-string">"dW"</span> (<span class="org-builtin">str</span> (<span class="org-builtin">+</span> l 1))) dW
                        (<span class="org-builtin">+</span> <span class="org-string">"db"</span> (<span class="org-builtin">str</span> (<span class="org-builtin">+</span> l 1))) db}})))))
</pre>
</div>


<p>
Implement the complete backward propagation described at <a href="#doc-backward-prop-dnnc">doc-backward-prop-dnnc</a>.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">model_layer_backward</span> [AL Y caches]
  (<span class="org-builtin">setv</span> <span class="org-variable-name">grads_prev</span> (input_layer_backward AL Y caches))
  (hidden_layer_backward AL Y caches grads_prev))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-model_layer_backward</span> []
  (np.random.seed 1)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">AL</span> (np.random.randn 1 2))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">Y</span> (np.random.randn 1 2))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">A1</span> (np.random.randn 4 2))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">W1</span> (np.random.randn 3 4))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">b1</span> (np.random.randn 3 1))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">Z1</span> (np.random.randn 1 3 2))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">A2</span> (np.random.randn 3 2))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">W2</span> (np.random.randn 1 3))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">b2</span> (np.random.randn 1 1))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">Z2</span> (np.random.randn 1 2))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">caches</span> [[[A1 W1 b1] Z1] [[A2 W2 b2] Z2]])
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_grads</span> (model_layer_backward AL Y caches))
  (<span class="org-keyword">assert</span> (np.isclose (.mean (<span class="org-builtin">get</span> test_grads <span class="org-string">"dA1"</span>)) 0.0222427))
  (<span class="org-keyword">assert</span> (np.isclose (.mean (<span class="org-builtin">get</span> test_grads <span class="org-string">"dW2"</span>)) 0.0314625))
  (<span class="org-keyword">assert</span> (np.isclose (.mean (<span class="org-builtin">get</span> test_grads <span class="org-string">"db2"</span>)) -0.1977597))
  (<span class="org-keyword">assert</span> (np.isclose (.mean (<span class="org-builtin">get</span> test_grads <span class="org-string">"dA0"</span>)) -0.002841569))
  (<span class="org-keyword">assert</span> (np.isclose (.mean (<span class="org-builtin">get</span> test_grads <span class="org-string">"dW1"</span>)) 0.009781))
  (<span class="org-keyword">assert</span> (np.isclose (.mean (<span class="org-builtin">get</span> test_grads <span class="org-string">"db1"</span>)) 0.00890932)))
(test-case (test-model_layer_backward))
</pre>
</div>
</div>
</li>
<li><a id="sec-3-3-2-3-3" name="sec-3-3-2-3-3"></a>Gradient Descent<br ><div class="outline-text-6" id="text-3-3-2-3-3">
<p>
The gradient descent algorithm for deep neural network is similar to
Logistic Regression one, and it's described at <a href="#src-gradientd-lrnnc">src-gradientd-lrnnc</a>.
The following implementation, \(\forall i \in [1,L]\), is:
</p>
\begin{align*}
W^{[i]} & \mathrel{{-}{=}} \alpha \text{ } \partial{\tilde{W}^{[i]}} \\\\
b^{[i]} & \mathrel{{-}{=}} \alpha \text{ } \partial{\tilde{b}^{[i]}}
\end{align*}
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">gradient_descent</span> [params grads learning_rate]
  (<span class="org-builtin">setv</span> <span class="org-variable-name">num_layer</span> (<span class="org-builtin">int</span> (<span class="org-builtin">/</span> (<span class="org-builtin">len</span> params) 2)))
  (<span class="org-keyword">loop</span> [[l 0] [update_params {}]]
        (<span class="org-builtin">setv</span> <span class="org-variable-name">next_l</span> (<span class="org-builtin">str</span> (<span class="org-builtin">+</span> l 1)))
        (<span class="org-keyword">if</span> (<span class="org-builtin">&gt;=</span> l num_layer)
            [update_params]
            (<span class="org-keyword">recur</span>
              (<span class="org-builtin">inc</span> l)
              {<span class="org-keyword">#**</span> update_params
               <span class="org-keyword">#**</span> {(<span class="org-builtin">+</span> <span class="org-string">"W"</span> next_l)
                    (<span class="org-builtin">-</span> (<span class="org-builtin">get</span> params (<span class="org-builtin">+</span> <span class="org-string">"W"</span> next_l))
                       (<span class="org-builtin">*</span> learning_rate (<span class="org-builtin">get</span> grads (<span class="org-builtin">+</span> <span class="org-string">"dW"</span> next_l))))
                    (<span class="org-builtin">+</span> <span class="org-string">"b"</span> next_l)
                    (<span class="org-builtin">-</span> (<span class="org-builtin">get</span> params (<span class="org-builtin">+</span> <span class="org-string">"b"</span> next_l))
                       (<span class="org-builtin">*</span> learning_rate (<span class="org-builtin">get</span> grads (<span class="org-builtin">+</span> <span class="org-string">"db"</span> next_l))))}}))))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-gradient_descent</span> []
  (np.random.seed 1)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">W1</span> (np.random.randn 3 4))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">b1</span> (np.random.randn 3 1))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">W2</span> (np.random.randn 1 3))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">b2</span> (np.random.randn 1 1))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">dW1</span> (np.random.randn 3 4))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">db1</span> (np.random.randn 3 1))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">dW2</span> (np.random.randn 1 3))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">db2</span> (np.random.randn 1 1))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">param</span> {<span class="org-string">"W1"</span> W1
               <span class="org-string">"b1"</span> b1
               <span class="org-string">"W2"</span> W2
               <span class="org-string">"b2"</span> b2})
  (<span class="org-builtin">setv</span> <span class="org-variable-name">grads</span> {<span class="org-string">"dW1"</span> dW1
               <span class="org-string">"db1"</span> db1
               <span class="org-string">"dW2"</span> dW2
               <span class="org-string">"db2"</span> db2})
  (<span class="org-builtin">setv</span> [test_update_params] (gradient_descent param grads 0.1))
  (<span class="org-keyword">assert</span> (np.isclose (.mean (<span class="org-builtin">get</span> test_update_params <span class="org-string">"W1"</span>)) -0.137122))
  (<span class="org-keyword">assert</span> (np.isclose (.mean (<span class="org-builtin">get</span> test_update_params <span class="org-string">"b1"</span>)) 0.206737023))
  (<span class="org-keyword">assert</span> (np.isclose (.mean (<span class="org-builtin">get</span> test_update_params <span class="org-string">"W2"</span>)) -0.6566852))
  (<span class="org-keyword">assert</span> (np.isclose (.mean (<span class="org-builtin">get</span> test_update_params <span class="org-string">"b2"</span>)) 0.018772)))
(test-case (test-gradient_descent))
</pre>
</div>
</div>
</li></ol>
</li>
<li><a id="sec-3-3-2-4" name="sec-3-3-2-4"></a>Model<br ><div class="outline-text-5" id="text-3-3-2-4">
<p>
Initialize both weights and bias with random values, and dimension according the <code>layer_dim</code> parameter.
<code>layer_dim</code> parameter contains the dimensions of each layer.
The calculation for the dimensions is described at <a href="#doc-forward-prop-dnnc">doc-forward-prop-dnnc</a>.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">init_param_deep</span> [layer_dim <span class="org-type">&amp;optional</span> [seed 2]]
  (.seed np.random seed)
  {<span class="org-keyword">#**</span>
   (<span class="org-keyword">dfor</span> i (<span class="org-builtin">range</span> 1 (<span class="org-builtin">len</span> layer_dim))
         [(<span class="org-builtin">+</span> <span class="org-string">"W"</span> (<span class="org-builtin">str</span> i))
          (<span class="org-builtin">*</span> (.randn np.random (<span class="org-builtin">get</span> layer_dim i) (<span class="org-builtin">get</span> layer_dim (<span class="org-builtin">-</span> i 1)))
             0.01)])
   <span class="org-keyword">#**</span>
   (<span class="org-keyword">dfor</span> i (<span class="org-builtin">range</span> 1 (<span class="org-builtin">len</span> layer_dim))
         [(<span class="org-builtin">+</span> <span class="org-string">"b"</span> (<span class="org-builtin">str</span> i))
          (.zeros np [(<span class="org-builtin">get</span> layer_dim i) 1])])})
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-init_param</span> []
  (<span class="org-keyword">assert</span> (np.isclose
            (.mean (<span class="org-builtin">get</span> (init_param_deep [1 2 3 4]) <span class="org-string">"W1"</span>)) -0.0023651))
  (<span class="org-keyword">assert</span> (np.isclose
            (.mean (<span class="org-builtin">get</span> (init_param_deep [1 2 3 4]) <span class="org-string">"W2"</span>)) -0.0064558))
  (<span class="org-keyword">assert</span> (np.isclose
            (.mean (<span class="org-builtin">get</span> (init_param_deep [1 2 3 4]) <span class="org-string">"W3"</span>)) 0.00013353))
  (<span class="org-keyword">assert</span> (np.isclose
            (.mean (<span class="org-builtin">get</span> (init_param_deep [1 2 3 4]) <span class="org-string">"b1"</span>)) 0.0))
  (<span class="org-keyword">assert</span> (np.isclose
            (.mean (<span class="org-builtin">get</span> (init_param_deep [1 2 3 4]) <span class="org-string">"b2"</span>)) 0.0)))
(test-case (test-init_param))
</pre>
</div>


<p>
The code block below implements the Deep Neural Network model.
It's similar to the Logistic Regression model described at <a href="#src-model-lrnnc">src-model-lrnnc</a>,
but using \(L\) layers instead of only 1 layer.
The algorithm is the following steps:
</p>
<ul class="org-ul">
<li>Load the MNIST dataset in input, and initialize the dimension for \(L\) layers
</li>
<li>Forward propagation as
\((\operatorname{linear} \Rightarrow \operatorname{ReLU})^{(L-1)}\)
\(\Rightarrow\)
\((\operatorname{linear} \Rightarrow \operatorname{sigmoid})^{1}\)
</li>
<li>Calculate the cost of the model through the cost function
</li>
<li>Backward propagation as
\((\operatorname{sigmoid} \Rightarrow \operatorname{linear})^{1}\)
\(\Rightarrow\)
\((\operatorname{linear} \Rightarrow \operatorname{ReLU})^{(L-1)}\)
</li>
<li>Update parameters (weights and bias) to minimize the cost function,
this step trains the model
</li>
<li>Loop these previous steps until <code>nb_iter</code>
</li>
<li>When the training is finished, the model is ready for classification,
i.e. to predict whether the labels are either 0 or 1.
In this part the model uses the test set,
unlike the other steps that use the training set.
</li>
</ul>
<p>
<code>layers_dims</code> represents the dimension for the elements \(A\) in the vector
\(\begin{bmatrix} A^{[0]} & A^{[1]} & A^{[2]} & A^{[3]} & A^{[4]} \end{bmatrix}\).
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> <span class="org-variable-name">layers_dims</span> [3072, 10, 7, 5, 1])

(<span class="org-keyword">defn</span> <span class="org-function-name">deep_model</span> [X_train Y_train layers_dims learning_rate nb_iter]
  (<span class="org-keyword">loop</span> [[i 0] [params (init_param_deep layers_dims)]]
        (<span class="org-keyword">when</span> (<span class="org-builtin">&lt;</span> i nb_iter)
          (<span class="org-builtin">setv</span> [AL caches] (model_layer_forward X_train params))
          (<span class="org-builtin">setv</span> <span class="org-variable-name">cost_l</span> (cost (<span class="org-builtin">get</span> Y_train.shape 1) AL Y_train))
          (<span class="org-keyword">print</span> <span class="org-string">"cost"</span> i cost_l)
          (<span class="org-builtin">setv</span> <span class="org-variable-name">grads</span> (model_layer_backward AL Y_train caches))
          (<span class="org-keyword">recur</span> (<span class="org-builtin">inc</span> i)
                 (<span class="org-builtin">get</span> (gradient_descent params grads learning_rate) 0)))))
</pre>
</div>
</div>
</li></ol>
</div>
<div id="outline-container-sec-3-3-3" class="outline-4">
<h4 id="sec-3-3-3"><span class="section-number-4">3.3.3</span> Results</h4>
<div class="outline-text-4" id="text-3-3-3">
<p>
A Memory error occurs during the third iteration due to my computer performance:
only 4 GB RAM, single-core CPU.
However, as shown in the output block the cost decreases after each iteration.
Of course this doesn't validate the model, but it gives a good sanity check.
</p>
<div class="org-src-container">

<pre class="src src-hy">(deep_model train_set_x train_set_y layers_dims 0.0075 5)
</pre>
</div>


<pre class="example">
cost 0 0.6931471834644863
cost 1 0.6931471811113991
cost 2 0.6931471787358986
MemoryError: Unable to allocate 1.14 GiB
for an array with shape (3072, 50000) and data type float64
</pre>
</div>
</div>
<div id="outline-container-sec-3-3-4" class="outline-4">
<h4 id="sec-3-3-4"><span class="section-number-4">3.3.4</span> References</h4>
<div class="outline-text-4" id="text-3-3-4">
<ul class="org-ul">
<li>deeplearning.ai. 2020. <a href="https://www.deeplearning.ai/deep-learning-specialization/">Deep Learning Specialization</a>.
</li>
<li>Sharma Avinash. 2017. <a href="https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0">Understanding Activation Functions in Neural Networks</a>.
</li>
<li>Wang Chi-Feng. 2019. <a href="https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484">The Vanishing Gradient Problem</a>.
</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-sec-3-4" class="outline-3">
<h3 id="sec-3-4"><span class="section-number-3">3.4</span> Convolutional Neural Network Classification Using Keras</h3>
<div class="outline-text-3" id="text-3-4">

<figure>
<p><img src="./export/img/cnnc.png" class="img-responsive" alt="cnnc.png" width="480">
</p>
</figure>
</div>
<div id="outline-container-sec-3-4-1" class="outline-4">
<h4 id="sec-3-4-1"><span class="section-number-4">3.4.1</span> Purpose</h4>
<div class="outline-text-4" id="text-3-4-1">
<p>
The objective is to build a CIFAR-10 image classification using Keras framework.
</p>
</div>
</div>
<div id="outline-container-sec-3-4-2" class="outline-4">
<h4 id="sec-3-4-2"><span class="section-number-4">3.4.2</span> Implementation</h4>
<div class="outline-text-4" id="text-3-4-2">
</div><ol class="org-ol"><li><a id="sec-3-4-2-1" name="sec-3-4-2-1"></a>Initialization<br ><div class="outline-text-5" id="text-3-4-2-1">
<p>
Import necessaries modules: different types of layers to build the model,
CIFAR-10 dataset, and Numpy.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">require</span> [hy.contrib.loop [<span class="org-keyword">loop</span>]])
(<span class="org-keyword">require</span> [hytmf [test-case]])
(<span class="org-keyword">import</span> [keras.models
         [Model load_model]])
(<span class="org-keyword">import</span> [keras.layers
         [Input Dense Dropout Flatten Conv2D MaxPooling2D]])
(<span class="org-keyword">import</span> [keras.constraints
         [maxnorm]])
(<span class="org-keyword">import</span> [keras.optimizers
         [SGD]])
(<span class="org-keyword">import</span> [keras.utils
         [np_utils plot_model]])
(<span class="org-keyword">import</span> [keras.datasets
         [cifar10]])
(<span class="org-keyword">import</span> [keras
         [backend <span class="org-keyword">:as</span> K]])
(<span class="org-keyword">import</span> [numpy <span class="org-keyword">:as</span> np])
</pre>
</div>


<p>
Load data from CIFAR-10 dataset.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> [[X_train_load y_train_load] [X_test_load y_test_load]]
      (.load_data cifar10))
</pre>
</div>


<p>
Normalize in order to have a range between 0 and 1.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> <span class="org-variable-name">X_train_norm</span> (<span class="org-builtin">/</span> X_train_load 255))
(<span class="org-builtin">setv</span> <span class="org-variable-name">X_test_norm</span> (<span class="org-builtin">/</span> X_test_load 255))
</pre>
</div>


<p>
One-hot encode (convert the vectors into binary vectors),
it's required by Keras framework.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> <span class="org-variable-name">y_train_hot</span> (.to_categorical np_utils y_train_load))
(<span class="org-builtin">setv</span> <span class="org-variable-name">y_test_hot</span> (.to_categorical np_utils y_test_load))
</pre>
</div>


<p>
Rename data for convenient usage.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> <span class="org-variable-name">X_train</span> X_train_norm)
(<span class="org-builtin">setv</span> <span class="org-variable-name">y_train</span> y_train_hot)
(<span class="org-builtin">setv</span> <span class="org-variable-name">X_test</span> X_test_norm)
(<span class="org-builtin">setv</span> <span class="org-variable-name">y_test</span> y_test_hot)
</pre>
</div>
</div>
</li>
<li><a id="sec-3-4-2-2" name="sec-3-4-2-2"></a>Model<br ><div class="outline-text-5" id="text-3-4-2-2">
<p>
Build the model with different types of layers:
</p>
<ul class="org-ul">
<li>Conv2D filters some specific features of the image
</li>
<li>Dropout is used to prevent overfitting
</li>
<li>MaxPooling reduces the dimension of each feature
</li>
<li>Flatten and Dense are used to create a fully connected network
</li>
</ul>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> <span class="org-variable-name">num_classes</span> (<span class="org-builtin">get</span> y_test_hot.shape 1))

(<span class="org-builtin">setv</span> <span class="org-variable-name">visible</span> (Input <span class="org-constant">:shape</span> [32 32 3]))

(<span class="org-builtin">setv</span> <span class="org-variable-name">output</span> (<span class="org-keyword">-&gt;</span>
               visible
               ((Conv2D
                  <span class="org-constant">:filters</span> 32
                  <span class="org-constant">:kernel_size</span> [3 3]
                  <span class="org-constant">:activation</span> <span class="org-string">"relu"</span>
                  <span class="org-constant">:padding</span> <span class="org-string">"same"</span>))
               ((Dropout 0.2))
               ((Conv2D
                  <span class="org-constant">:filters</span> 32
                  <span class="org-constant">:kernel_size</span> [3 3]
                  <span class="org-constant">:activation</span> <span class="org-string">"relu"</span>
                  <span class="org-constant">:padding</span> <span class="org-string">"same"</span>))
               ((MaxPooling2D
                  <span class="org-constant">:pool_size</span> [2 2]))
               ((Conv2D
                  <span class="org-constant">:filters</span> 64
                  <span class="org-constant">:kernel_size</span> [3 3]
                  <span class="org-constant">:activation</span> <span class="org-string">"relu"</span>
                  <span class="org-constant">:padding</span> <span class="org-string">"same"</span>))
               ((Dropout 0.2))
               ((Conv2D
                  <span class="org-constant">:filters</span> 64
                  <span class="org-constant">:kernel_size</span> [3 3]
                  <span class="org-constant">:activation</span> <span class="org-string">"relu"</span>
                  <span class="org-constant">:padding</span> <span class="org-string">"same"</span>))
               ((MaxPooling2D
                  <span class="org-constant">:pool_size</span> [2 2]))
               ((Conv2D
                  <span class="org-constant">:filters</span> 128
                  <span class="org-constant">:kernel_size</span> [3 3]
                  <span class="org-constant">:activation</span> <span class="org-string">"relu"</span>
                  <span class="org-constant">:padding</span> <span class="org-string">"same"</span>))
               ((Dropout 0.2))
               ((Conv2D
                  <span class="org-constant">:filters</span> 128
                  <span class="org-constant">:kernel_size</span> [3 3]
                  <span class="org-constant">:activation</span> <span class="org-string">"relu"</span>
                  <span class="org-constant">:padding</span> <span class="org-string">"same"</span>))
               ((MaxPooling2D
                  <span class="org-constant">:pool_size</span> [2 2]))
               ((Flatten))
               ((Dropout 0.2))
               ((Dense
                  <span class="org-constant">:units</span> 1024
                  <span class="org-constant">:activation</span> <span class="org-string">"relu"</span>
                  <span class="org-constant">:kernel_constraint</span> (maxnorm 3)))
               ((Dropout 0.2))
               ((Dense
                  <span class="org-constant">:units</span> 512
                  <span class="org-constant">:activation</span> <span class="org-string">"relu"</span>
                  <span class="org-constant">:kernel_constraint</span> (maxnorm 3)))
               ((Dropout 0.2))
               ((Dense
                  <span class="org-constant">:units</span> num_classes
                  <span class="org-constant">:activation</span> <span class="org-string">"softmax"</span>))))

(<span class="org-builtin">setv</span> <span class="org-variable-name">model</span> (Model <span class="org-constant">:inputs</span> visible <span class="org-constant">:outputs</span> output))
</pre>
</div>


<p>
Model summary.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">print</span> (.summary model))
</pre>
</div>


<pre class="example">
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         [(None, 32, 32, 3)]       0
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 32, 32, 32)        896
_________________________________________________________________
dropout_5 (Dropout)          (None, 32, 32, 32)        0
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 32, 32, 32)        9248
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 16, 16, 32)        0
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496
_________________________________________________________________
dropout_4 (Dropout)          (None, 16, 16, 64)        0
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 16, 16, 64)        36928
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 8, 8, 128)         73856
_________________________________________________________________
dropout_3 (Dropout)          (None, 8, 8, 128)         0
_________________________________________________________________
conv2d (Conv2D)              (None, 8, 8, 128)         147584
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 4, 4, 128)         0
_________________________________________________________________
flatten (Flatten)            (None, 2048)              0
_________________________________________________________________
dropout_2 (Dropout)          (None, 2048)              0
_________________________________________________________________
dense_2 (Dense)              (None, 1024)              2098176
_________________________________________________________________
dropout_1 (Dropout)          (None, 1024)              0
_________________________________________________________________
dense_1 (Dense)              (None, 512)               524800
_________________________________________________________________
dropout (Dropout)            (None, 512)               0
_________________________________________________________________
dense (Dense)                (None, 10)                5130
=================================================================
Total params: 2,915,114
Trainable params: 2,915,114
Non-trainable params: 0
_________________________________________________________________
</pre>


<p>
Compile the model with specific parameters.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> <span class="org-variable-name">lrate</span> 0.01)
(<span class="org-builtin">setv</span> <span class="org-variable-name">sgd</span> (SGD <span class="org-constant">:lr</span> lrate <span class="org-constant">:momentum</span> 0.9 <span class="org-constant">:nesterov</span> <span class="org-constant">False</span>))
(.compile model
          <span class="org-constant">:loss</span> <span class="org-string">"categorical_crossentropy"</span>
          <span class="org-constant">:optimizer</span> sgd
          <span class="org-constant">:metrics</span> [<span class="org-string">"accuracy"</span>])
</pre>
</div>


<p>
Then train the model, and save it into a file.
</p>
<div class="org-src-container">

<pre class="src src-hy">(.fit model
      <span class="org-constant">:x</span> X_train
      <span class="org-constant">:y</span> y_train
      <span class="org-constant">:validation_data</span> [X_test y_test]
      <span class="org-constant">:epochs</span> 5
      <span class="org-constant">:batch_size</span> 32)

(.save model <span class="org-string">"keras_model.h5"</span>)
(<span class="org-builtin">setv</span> <span class="org-variable-name">model</span> (load_model <span class="org-string">"keras_model.h5"</span>))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-model</span> []
  (<span class="org-keyword">assert</span> (np.isclose (<span class="org-builtin">*</span> (<span class="org-builtin">get</span> (.evaluate model <span class="org-constant">:x</span> X_test <span class="org-constant">:y</span> y_test <span class="org-constant">:verbose</span> 1)
                              1) 100) 65.05)))
(test-case (test-model))
</pre>
</div>
</div>
</li></ol>
</div>
<div id="outline-container-sec-3-4-3" class="outline-4">
<h4 id="sec-3-4-3"><span class="section-number-4">3.4.3</span> Results</h4>
<div class="outline-text-4" id="text-3-4-3">
<p>
Print the final score.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">print</span> <span class="org-string">"Accuracy:"</span>
       (<span class="org-builtin">*</span> (<span class="org-builtin">get</span> (.evaluate model <span class="org-constant">:x</span> X_test <span class="org-constant">:y</span> y_test <span class="org-constant">:verbose</span> 1) 1) 100))
</pre>
</div>


<pre class="example">
Accuracy: 65.04999995231628
</pre>
</div>
</div>
<div id="outline-container-sec-3-4-4" class="outline-4">
<h4 id="sec-3-4-4"><span class="section-number-4">3.4.4</span> References</h4>
<div class="outline-text-4" id="text-3-4-4">
<ul class="org-ul">
<li>Joy Ashwin. 2020. <a href="https://pythonistaplanet.com/cifar-10-image-classification-using-keras/">Cifar-10 Image Classification Using Keras</a>.
</li>
<li>Brownlee Jason. 2017. <a href="https://machinelearningmastery.com/keras-functional-api-deep-learning/">How to Use the Keras Functional API for Deep Learning</a>.
</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-sec-3-5" class="outline-3">
<h3 id="sec-3-5"><span class="section-number-3">3.5</span> Data Augmentation Using Keras</h3>
<div class="outline-text-3" id="text-3-5">

<figure>
<p><img src="./export/img/da.png" class="img-responsive" alt="da.png" width="480">
</p>
</figure>
</div>
<div id="outline-container-sec-3-5-1" class="outline-4">
<h4 id="sec-3-5-1"><span class="section-number-4">3.5.1</span> Purpose</h4>
<div class="outline-text-4" id="text-3-5-1">
<p>
The data augmentation is a technique used in deep learning, to increase the
diversity of data from one already available. This project implements it with
Keras framework.
</p>
</div>
</div>
<div id="outline-container-sec-3-5-2" class="outline-4">
<h4 id="sec-3-5-2"><span class="section-number-4">3.5.2</span> Implementation</h4>
<div class="outline-text-4" id="text-3-5-2">
</div><ol class="org-ol"><li><a id="sec-3-5-2-1" name="sec-3-5-2-1"></a>Initialization<br ><div class="outline-text-5" id="text-3-5-2-1">
<p>
Import necessaries modules for image processing.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">require</span> [hy.contrib.loop [<span class="org-keyword">loop</span>]])
(<span class="org-keyword">import</span> [keras.preprocessing.image
         [ImageDataGenerator <span class="org-keyword">:as</span> image_data_generator
          array_to_img img_to_array load_img]])
(<span class="org-keyword">import</span> urllib.request)
</pre>
</div>


<p>
Initialize and configure the image generator.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> <span class="org-variable-name">datagen</span> (image_data_generator
                <span class="org-constant">:rotation_range</span> 40
                <span class="org-constant">:width_shift_range</span> 0.2
                <span class="org-constant">:height_shift_range</span> 0.2
                <span class="org-constant">:shear_range</span> 0.2
                <span class="org-constant">:zoom_range</span> 0.2
                <span class="org-constant">:horizontal_flip</span> <span class="org-constant">True</span>
                <span class="org-constant">:fill_mode</span> <span class="org-string">"nearest"</span>))
</pre>
</div>


<p>
Load image as Python Imaging Library.
</p>
<div class="org-src-container">

<pre class="src src-hy">(urllib.request.urlretrieve
  (<span class="org-builtin">+</span> <span class="org-string">"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e5/"</span>
     <span class="org-string">"Green_turtle_swimming_over_coral_reefs_in_Kona.jpg/"</span>
     <span class="org-string">"320px-Green_turtle_swimming_over_coral_reefs_in_Kona.jpg"</span>)
  <span class="org-string">"turtle_base.png"</span>)

(<span class="org-builtin">setv</span> <span class="org-variable-name">img_turtle</span> (load_img <span class="org-string">"turtle_base.png"</span>))
</pre>
</div>

<figure>
<p><img src="./export/da/turtle_base.png" class="img-responsive" alt="turtle_base.png" width="240">
</p>
<figcaption><span class="figure-number">Figure 16:</span> Source image</figcaption>
</figure>


<p>
Transform image PIL into image array of shape (3, 100, 50).
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> <span class="org-variable-name">lst_turtle</span> (img_to_array img_turtle))
</pre>
</div>


<p>
Add a new dimension to the image (1, 3, 100, 50),
it's required by Keras framework.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> <span class="org-variable-name">lst_turtle_res</span> (.reshape lst_turtle (<span class="org-builtin">+</span> '(1,) lst_turtle.shape)))
</pre>
</div>
</div>
</li>
<li><a id="sec-3-5-2-2" name="sec-3-5-2-2"></a>Generator<br ><div class="outline-text-5" id="text-3-5-2-2">
<p>
Create an iterator, and loop in order to generate new images.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> <span class="org-variable-name">k_it</span> (.flow datagen lst_turtle_res
               <span class="org-constant">:batch_size</span> 1
               <span class="org-constant">:save_to_dir</span> <span class="org-string">"."</span>
               <span class="org-constant">:save_prefix</span> <span class="org-string">"turtle_gen"</span>
               <span class="org-constant">:save_format</span> <span class="org-string">"png"</span>))

(<span class="org-keyword">loop</span> [[i 0]]
      (<span class="org-keyword">when</span> (<span class="org-builtin">&lt;</span> i 10)
        (<span class="org-builtin">next</span> k_it)
        (<span class="org-keyword">recur</span> (<span class="org-builtin">inc</span> i))))
</pre>
</div>
</div>
</li></ol>
</div>
<div id="outline-container-sec-3-5-3" class="outline-4">
<h4 id="sec-3-5-3"><span class="section-number-4">3.5.3</span> Results</h4>
<div class="outline-text-4" id="text-3-5-3">

<figure>
<p><img src="./export/da/turtle_gen_0.png" class="img-responsive" alt="turtle_gen_0.png" width="240">
</p>
<figcaption><span class="figure-number">Figure 17:</span> The first generated image</figcaption>
</figure>

<figure>
<p><img src="./export/da/turtle_gen_1.png" class="img-responsive" alt="turtle_gen_1.png" width="240">
</p>
<figcaption><span class="figure-number">Figure 18:</span> The second generated image</figcaption>
</figure>
</div>
</div>
<div id="outline-container-sec-3-5-4" class="outline-4">
<h4 id="sec-3-5-4"><span class="section-number-4">3.5.4</span> References</h4>
<div class="outline-text-4" id="text-3-5-4">
<ul class="org-ul">
<li>Chollet Francois. 2016. <a href="https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html">Building powerful image classification models using very little data</a>.
</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-sec-3-6" class="outline-3">
<h3 id="sec-3-6"><span class="section-number-3">3.6</span> Optimization Algorithms From Scratch</h3>
<div class="outline-text-3" id="text-3-6">

<figure>
<p><img src="./export/img/oa.png" class="img-responsive" alt="oa.png" width="480">
</p>
</figure>
</div>
<div id="outline-container-sec-3-6-1" class="outline-4">
<h4 id="sec-3-6-1"><span class="section-number-4">3.6.1</span> Purpose</h4>
<div class="outline-text-4" id="text-3-6-1">
<p>
The objective is to compare different optimization algorithms used
during gradient descent step.
The optimization algorithms implemented in this project are:
</p>
<ul class="org-ul">
<li>Standard gradient descent
</li>
<li>Momentum, uses exponentially weighted averages method to speed up
the standard gradient descent
</li>
<li>RMSprop, similar to Momentum
</li>
<li>Adam, combines both Momentum and RMSprop
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-3-6-2" class="outline-4">
<h4 id="sec-3-6-2"><span class="section-number-4">3.6.2</span> Implementation</h4>
<div class="outline-text-4" id="text-3-6-2">
</div><ol class="org-ol"><li><a id="sec-3-6-2-1" name="sec-3-6-2-1"></a>Initialization<br ><div class="outline-text-5" id="text-3-6-2-1">
<p>
Import necessaries modules.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">require</span> [hy.contrib.loop [<span class="org-keyword">loop</span>]])
(<span class="org-keyword">require</span> [hytmf [test-case]])
(<span class="org-keyword">import</span> [numpy <span class="org-keyword">:as</span> np])
(<span class="org-keyword">import</span> [matplotlib [pyplot <span class="org-keyword">:as</span> plt]])
(<span class="org-keyword">import</span> [random <span class="org-keyword">:as</span> rdm])
(<span class="org-keyword">import</span> math)
</pre>
</div>


<p>
Temperature of Paris in the last four years.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> <span class="org-variable-name">temp_paris</span> [4 8 2 6 8 2 11 6 10 7 11 7 12 14 11 10 14 17 17 15 20 20
                  21 18 22 24 21 21 21 22 20 21 17 18 16 21 13 14 14 19 8
                  8 8 12 7 7 6 8])
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">plot_temp_paris</span> []
  (.figure plt <span class="org-string">"temp_paris"</span>)
  (.scatter plt <span class="org-constant">:x</span> (<span class="org-builtin">list</span> (<span class="org-builtin">range</span> (<span class="org-builtin">len</span> temp_paris)))
            <span class="org-constant">:y</span> temp_paris <span class="org-constant">:c</span> <span class="org-string">"b"</span>)
  (.savefig plt <span class="org-string">"temp_paris"</span>)
  (.close plt))
(plot_temp_paris)
</pre>
</div>

<figure>
<p><img src="./export/oa/temp_paris.png" class="img-responsive" alt="temp_paris.png" width="480">
</p>
<figcaption><span class="figure-number">Figure 20:</span> Temperature of Paris in the last four years</figcaption>
</figure>


<p>
Implement a basic function used in algorithms below in order to
compare them each other.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">func</span> [x]
  (<span class="org-builtin">+</span> (<span class="org-builtin">-</span> (<span class="org-builtin">**</span> x 3) (<span class="org-builtin">*</span> 3 (<span class="org-builtin">**</span> x 2))) 7))

(<span class="org-keyword">defn</span> <span class="org-function-name">deriv_func</span> [x]
  (<span class="org-builtin">-</span> (<span class="org-builtin">*</span> 3 (<span class="org-builtin">**</span> x 2)) (<span class="org-builtin">*</span> 6 x)))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">plot_func</span> []
  (<span class="org-builtin">setv</span> <span class="org-variable-name">x</span> (.linspace np -1 3 500))
  (.figure plt <span class="org-string">"gradient_func"</span>)
  (.plot plt x (func x))
  (.savefig plt <span class="org-string">"gradient_func"</span>)
  (.close plt))
(plot_func)
</pre>
</div>

<figure>
<p><img src="./export/oa/gradient_func.png" class="img-responsive" alt="gradient_func.png" width="480">
</p>
<figcaption><span class="figure-number">Figure 21:</span> The gradient function</figcaption>
</figure>
</div>
</li>
<li><a id="sec-3-6-2-2" name="sec-3-6-2-2"></a>Optimization Functions<br ><div class="outline-text-5" id="text-3-6-2-2">
<p>
Implement the standard gradient descent algorithm, a complete description
is located at <a href="#src-gradientd-lrnnc">src-gradientd-lrnnc</a>. <br >
\(\mathtt{for:}\) <br >
<code>|</code> \(\mathtt{x = x - learning\_rate \times dx}\)
<a id="src-gradientd-oa" name="src-gradientd-oa"></a>
</p>
<div class="org-src-container">

<pre class="src src-hy" id="src-gradientd-oa">(<span class="org-keyword">defn</span> <span class="org-function-name">gradient_descent</span> [x_next x_first precision l_r]
  (<span class="org-keyword">loop</span> [[x_acc_prev x_first]
         [x_acc x_next]
         [x_list [x_next]]
         [y_list [(func x_next)]]]
        (<span class="org-keyword">if</span> (<span class="org-builtin">&lt;=</span> (<span class="org-builtin">abs</span> (<span class="org-builtin">-</span> x_acc x_acc_prev)) precision)
            [x_acc x_list y_list]
            (<span class="org-keyword">recur</span>
              x_acc
              (<span class="org-builtin">-</span> x_acc (<span class="org-builtin">*</span> l_r (deriv_func x_acc)))
              (<span class="org-builtin">+</span> x_list [x_acc])
              (<span class="org-builtin">+</span> y_list [(func x_acc)])))))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-gradient_descent</span> []
  (<span class="org-builtin">setv</span> [test_x_acc test_x_list test_y_list]
        (gradient_descent 0.5 0 0.001 0.05))
  (<span class="org-builtin">setv</span> [test_local_min test_nb_step] [test_x_acc (<span class="org-builtin">len</span> test_x_list)])
  (<span class="org-keyword">assert</span> (np.isclose test_local_min 1.998026))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> test_nb_step 25)))
(test-case (test-gradient_descent))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">plot_gradient_descent</span> []
  (<span class="org-builtin">setv</span> [x_acc x_list y_list] (gradient_descent 0.5 0 0.001 0.05))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">x</span> (.linspace np -1 3 500))
  (<span class="org-keyword">print</span> <span class="org-string">"Gradient descent local minimum:"</span> (<span class="org-builtin">str</span> x_acc))
  (<span class="org-keyword">print</span> <span class="org-string">"Gradient descent Number of steps:"</span> (<span class="org-builtin">str</span> (<span class="org-builtin">len</span> x_list)))
  (.figure plt <span class="org-string">"gradient_descent"</span>)
  (.subplot plt 1 2 2)
  (.scatter plt x_list y_list <span class="org-constant">:c</span> <span class="org-string">"g"</span>)
  (.plot plt x (func x) <span class="org-constant">:c</span> <span class="org-string">"r"</span>)
  (.savefig plt <span class="org-string">"gradient_descent"</span>)
  (.close plt))
(plot_gradient_descent)
</pre>
</div>

<figure>
<p><img src="./export/oa/gradient_descent.png" class="img-responsive" alt="gradient_descent.png" width="480">
</p>
<figcaption><span class="figure-number">Figure 22:</span> The gradient descent algorithm</figcaption>
</figure>


<p>
With the given parameters (cf. function above), the standard gradient descent
finds the local minimum in 25 steps.
</p>
<pre class="example">
Gradient descent local minimum: 1.9980265135950486
Gradient descent Number of steps: 25
</pre>


<p>
The purpose of the exponentially weighted averages algorithm is to
<i>smooth</i> the data.
It is not a gradient descent algorithm, instead it's a technique used by
Momentum algorithm (<a href="#src-momentum-oa">src-momentum-oa</a>). <br >
\(\mathtt{v = 0}\) <br >
\(\mathtt{for\ t:}\) <br >
<code>|</code> \(\mathtt{get\ \theta_{t}}\) <br >
<code>|</code> \(\mathtt{v_{t} = \beta \times v_{t-1} + (1 - \beta) \times \theta_{t}}\) <br >
\(\mathtt{\beta = 0.9}\) is a common values used, and \(\mathtt{\beta < 1}\).
The formula is equals to
\(\mathtt{v_{t} = 0.9 \times v_{t-1} + (0.1 - \beta) \times \theta_{t}}\),
with \(\mathtt{\beta = 0.9}\).
<a id="src-expwavg-oa" name="src-expwavg-oa"></a>
</p>
<div class="org-src-container">

<pre class="src src-hy" id="src-expwavg-oa">(<span class="org-keyword">defn</span> <span class="org-function-name">exp_w_avg</span> [beta]
  (<span class="org-keyword">loop</span> [[i 0]
         [v_acc 0]
         [v_acc_list []]]
        (<span class="org-keyword">if</span> (<span class="org-builtin">&gt;=</span> i (<span class="org-builtin">len</span> temp_paris))
            v_acc_list
            (<span class="org-keyword">recur</span>
              (<span class="org-builtin">inc</span> i)
              (<span class="org-builtin">+</span> (<span class="org-builtin">*</span> beta v_acc) (<span class="org-builtin">*</span> (<span class="org-builtin">-</span> 1 beta) (<span class="org-builtin">get</span> temp_paris i)))
              (<span class="org-builtin">+</span> v_acc_list [v_acc])))))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-exp_w_avg</span> []
  (<span class="org-keyword">assert</span> (np.isclose (<span class="org-builtin">get</span> (exp_w_avg 0.09) -1) 6.093383))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">len</span> (exp_w_avg 0.09)) 48)))
(test-case (test-exp_w_avg))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">plot_exp_w_avg</span> []
  (<span class="org-builtin">setv</span> <span class="org-variable-name">v_acc_list</span> (exp_w_avg 0.9))
  (<span class="org-keyword">print</span> <span class="org-string">"Exponentially weighted average:"</span> (<span class="org-builtin">str</span> (<span class="org-builtin">get</span> v_acc_list -1)))
  (.figure plt <span class="org-string">"plot_exp_w_avg"</span>)
  (.scatter plt <span class="org-constant">:x</span> (<span class="org-builtin">list</span> (<span class="org-builtin">range</span> (<span class="org-builtin">len</span> temp_paris)))
            <span class="org-constant">:y</span> temp_paris <span class="org-constant">:c</span> <span class="org-string">"b"</span>)
  (.plot plt (<span class="org-builtin">list</span> (<span class="org-builtin">range</span> (<span class="org-builtin">len</span> temp_paris))) v_acc_list <span class="org-constant">:c</span> <span class="org-string">"r"</span>)
  (.savefig plt <span class="org-string">"plot_exp_w_avg"</span>)
  (.close plt))
(plot_exp_w_avg)
</pre>
</div>

<figure>
<p><img src="./export/oa/plot_exp_w_avg.png" class="img-responsive" alt="plot_exp_w_avg.png" width="480">
</p>
<figcaption><span class="figure-number">Figure 23:</span> The exponentially weighted averages algorithm</figcaption>
</figure>


<p>
The average of Paris temperature in the last four years.
</p>
<pre class="example">
Exponentially weighted average: 12.261606029039104
</pre>


<p>
Improve the exponentially weighted averages with
the bias correction technique. <br >
\(\mathtt{v = 0}\) <br >
\(\mathtt{for\ t:}\) <br >
<code>|</code> \(\mathtt{get\ \theta_{t}}\) <br >
<code>|</code> \(\mathtt{v_{t} = \beta \times v_{t-1} + (1 - \beta) \times \theta_{t}}\) <br >
<code>|</code> \(\mathtt{v\_list.append(v_{t} / (1 - \beta^{t}))}\) <br >
Since the curve starts from 0, values accumulated are lower than expected.
Only initial values need to be compensated.
Each values at \(\mathtt{t}\) time are divided by \(\mathtt{(1 - \beta^{t})}\).
Bias correction factor has effect when \(\mathtt{t}\) value is small,
but no effect when \(\mathtt{t}\) is large,
because \((1 - \beta^{t})\) becomes close to 1.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">exp_w_avg_bias</span> [beta]
  (<span class="org-keyword">loop</span> [[i 0]
         [v_acc 0]
         [v_acc_list []]]
        (<span class="org-keyword">if</span> (<span class="org-builtin">&gt;=</span> i (<span class="org-builtin">len</span> temp_paris))
            v_acc_list
            (<span class="org-keyword">recur</span>
              (<span class="org-builtin">inc</span> i)
              (<span class="org-builtin">+</span> (<span class="org-builtin">*</span> beta v_acc) (<span class="org-builtin">*</span> (<span class="org-builtin">-</span> 1 beta) (<span class="org-builtin">get</span> temp_paris i)))
              (<span class="org-builtin">+</span> v_acc_list [(<span class="org-builtin">/</span> v_acc (<span class="org-builtin">-</span> 1 (<span class="org-builtin">**</span> beta (<span class="org-builtin">+</span> i 1))))])))))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-exp_w_avg_bias</span> []
  (<span class="org-keyword">assert</span> (np.isclose (<span class="org-builtin">get</span> (exp_w_avg_bias 0.09) -1) 6.093383))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">len</span> (exp_w_avg_bias 0.09)) 48)))
(test-case (test-exp_w_avg_bias))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">plot_exp_w_avg_bias</span> []
  (<span class="org-builtin">setv</span> <span class="org-variable-name">v_acc_list_bias</span> (exp_w_avg_bias 0.9))
  (<span class="org-keyword">print</span> <span class="org-string">"Exponentially weighted average bias:"</span> (<span class="org-builtin">str</span> (<span class="org-builtin">get</span> v_acc_list_bias -1)))
  (.figure plt <span class="org-string">"plot_exp_w_avg_bias"</span>)
  (.scatter plt <span class="org-constant">:x</span> (<span class="org-builtin">list</span> (<span class="org-builtin">range</span> (<span class="org-builtin">len</span> temp_paris)))
            <span class="org-constant">:y</span> temp_paris <span class="org-constant">:c</span> <span class="org-string">"b"</span>)
  (.plot plt (<span class="org-builtin">list</span> (<span class="org-builtin">range</span> (<span class="org-builtin">len</span> temp_paris))) (exp_w_avg 0.9) <span class="org-constant">:c</span> <span class="org-string">"r"</span>)
  (.plot plt (<span class="org-builtin">list</span> (<span class="org-builtin">range</span> (<span class="org-builtin">len</span> temp_paris))) v_acc_list_bias <span class="org-constant">:c</span> <span class="org-string">"g"</span>)
  (.savefig plt <span class="org-string">"plot_exp_w_avg_bias"</span>)
  (.close plt))
(plot_exp_w_avg_bias)
</pre>
</div>

<figure>
<p><img src="./export/oa/plot_exp_w_avg_bias.png" class="img-responsive" alt="plot_exp_w_avg_bias.png" width="480">
</p>
<figcaption><span class="figure-number">Figure 24:</span> The exponentially weighted averages bias algorithm</figcaption>
</figure>


<p>
The average with bias correction is greater than the average without
bias correction (cf. <a href="#src-expwavg-oa">src-expwavg-oa</a>), because initial values are compensated.
</p>
<pre class="example">
Exponentially weighted average bias: 12.340122345830757
</pre>


<p>
Implementation of the Momentum algorithm.
It's a technique to speed up the standard gradient descent
(described at <a href="#src-gradientd-oa">src-gradientd-oa</a>). <br >
\(\mathtt{vdx = 0}\) <br >
\(\mathtt{for:}\) <br >
<code>|</code> \(\mathtt{vdx = \beta \times vdx + (1 - \beta) \times dx}\) <br >
<code>|</code> \(\mathtt{x = x - learning\_rate \times vdx}\) <br >
For some loss functions, the standard gradient descent
convergence time is long.
Momentum algorithm reduces oscillation by adding a portion
of the previous weight (exponentially weighted average of the derivatives,
<a href="#src-expwavg-oa">src-expwavg-oa</a>) to the current one.
<a id="src-momentum-oa" name="src-momentum-oa"></a>
</p>
<div class="org-src-container">

<pre class="src src-hy" id="src-momentum-oa">(<span class="org-keyword">defn</span> <span class="org-function-name">momentum</span> [x_next x_first precision l_r beta]
  (<span class="org-keyword">loop</span> [[x_acc_prev x_first]
         [vdx (<span class="org-builtin">*</span> (<span class="org-builtin">-</span> 1 beta) (deriv_func x_next))]
         [x_acc x_next]
         [x_list [x_next]]
         [y_list [(func x_next)]]]
        (<span class="org-keyword">if</span> (<span class="org-builtin">&lt;=</span> (<span class="org-builtin">abs</span> (<span class="org-builtin">-</span> x_acc x_acc_prev)) precision)
            [x_acc x_list y_list]
            (<span class="org-keyword">recur</span>
              x_acc
              (<span class="org-builtin">+</span> (<span class="org-builtin">*</span> beta vdx) (<span class="org-builtin">*</span> (<span class="org-builtin">-</span> 1 beta) (deriv_func x_acc)))
              (<span class="org-builtin">-</span> x_acc (<span class="org-builtin">*</span> l_r vdx))
              (<span class="org-builtin">+</span> x_list [x_acc])
              (<span class="org-builtin">+</span> y_list [(func x_acc)])))))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-momentum</span> []
  (<span class="org-builtin">setv</span> [test_x_acc_m test_x_list_m test_y_list_m]
        (momentum 0.5 0 0.001 0.05 0.09))
  (<span class="org-builtin">setv</span> [test_mom_localmin test_mom_nbstep]
        [test_x_acc_m (<span class="org-builtin">len</span> test_x_list_m)])
  (<span class="org-keyword">assert</span> (np.isclose test_mom_localmin 2.0044111))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> test_mom_nbstep 18)))
(test-case (test-momentum))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">plot_momentum</span> []
  (<span class="org-builtin">setv</span> [x_acc_gd x_list_gd y_list_gd] (gradient_descent 0.5 0 0.001 0.05))
  (<span class="org-builtin">setv</span> [x_acc_m x_list_m y_list_m] (momentum 0.5 0 0.001 0.05 0.09))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">x</span> (.linspace np -1 3 500))

  (<span class="org-keyword">print</span> <span class="org-string">"Gradient descent local minimum:"</span> (<span class="org-builtin">str</span> x_acc_gd))
  (<span class="org-keyword">print</span> <span class="org-string">"Gradient descent number of steps:"</span> (<span class="org-builtin">str</span> (<span class="org-builtin">len</span> x_list_gd)))
  (<span class="org-keyword">print</span> <span class="org-string">"Momentum local minimum:"</span> (<span class="org-builtin">str</span> x_acc_m))
  (<span class="org-keyword">print</span> <span class="org-string">"Momentum number of steps:"</span> (<span class="org-builtin">str</span> (<span class="org-builtin">len</span> x_list_m)))

  (.figure plt <span class="org-string">"momentum"</span>)
  (.subplot plt 1 2 2)
  (.scatter plt x_list_gd y_list_gd <span class="org-constant">:c</span> <span class="org-string">"g"</span>)
  (.scatter plt x_list_m y_list_m <span class="org-constant">:c</span> <span class="org-string">"r"</span>)
  (.plot plt x (func x) <span class="org-constant">:c</span> <span class="org-string">"b"</span>)
  (.savefig plt <span class="org-string">"momentum"</span>)
  (.close plt)

  (.figure plt <span class="org-string">"momentum_zoomed"</span>)
  (.subplot plt 1 2 1)
  (.scatter plt x_list_gd y_list_gd <span class="org-constant">:c</span> <span class="org-string">"g"</span>)
  (.scatter plt x_list_m y_list_m <span class="org-constant">:c</span> <span class="org-string">"r"</span>)
  (.plot plt x (func x) <span class="org-constant">:c</span> <span class="org-string">"b"</span>)
  (.xlim plt 1.0 2.1)
  (.savefig plt <span class="org-string">"momentum_zoomed"</span>)
  (.close plt))
(plot_momentum)
</pre>
</div>

<figure>
<p><img src="./export/oa/momentum.png" class="img-responsive" alt="momentum.png" width="480">
</p>
<figcaption><span class="figure-number">Figure 25:</span> The momentum algorithm</figcaption>
</figure>

<figure>
<p><img src="./export/oa/momentum_zoomed.png" class="img-responsive" alt="momentum_zoomed.png" width="480">
</p>
<figcaption><span class="figure-number">Figure 26:</span> The momentum algorithm (zoomed)</figcaption>
</figure>


<p>
The Momentum algorithm finds the local minimum faster
than the standard gradient descent, with well-tuned parameters.
</p>
<pre class="example">
Gradient descent local minimum: 1.9980265135950486
Gradient descent number of steps: 25
Momentum local minimum: 2.0044111590230216
Momentum number of steps: 18
</pre>


<p>
Implementation of the RMSprop (Root mean square prop) algorithm.
Similar to the Momentum algorithm (described at <a href="#src-momentum-oa">src-momentum-oa</a>),
its goal is to speed up the standard gradient descent with
a moving-average-like. <br >
\(\mathtt{sdx = 0}\) <br >
\(\mathtt{for:}\) <br >
<code>|</code> \(\mathtt{sdx = \beta \times sdx + (1 - \beta) \times dx^{2}}\) <br >
<code>|</code> \(\mathtt{x = x - learning\_rate \times dx / \sqrt(sdx)}\)
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">rmsprop</span> [x_next x_first precision l_r beta]
  (<span class="org-keyword">loop</span> [[x_acc_prev x_first]
         [sdx (<span class="org-builtin">*</span> (<span class="org-builtin">-</span> 1 beta) (<span class="org-builtin">**</span> (deriv_func x_next) 2))]
         [x_acc x_next]
         [x_list [x_next]]
         [y_list [(func x_next)]]]
        (<span class="org-keyword">if</span> (<span class="org-builtin">&lt;=</span> (<span class="org-builtin">abs</span> (<span class="org-builtin">-</span> x_acc x_acc_prev)) precision)
            [x_acc x_list y_list]
            (<span class="org-keyword">recur</span>
              x_acc
              (<span class="org-builtin">+</span> (<span class="org-builtin">*</span> beta sdx) (<span class="org-builtin">*</span> (<span class="org-builtin">-</span> 1 beta) (<span class="org-builtin">**</span> (deriv_func x_acc) 2)))
              (<span class="org-builtin">-</span> x_acc (<span class="org-builtin">/</span> (<span class="org-builtin">*</span> l_r (deriv_func x_acc)) (math.sqrt sdx)))
              (<span class="org-builtin">+</span> x_list [x_acc])
              (<span class="org-builtin">+</span> y_list [(func x_acc)])))))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-rmsprop</span> []
  (<span class="org-builtin">setv</span> [test_x_acc_r test_x_list_r test_y_list_r]
        (rmsprop 0.5 0 0.001 0.09 0.9))
  (<span class="org-builtin">setv</span> [test_local_min test_nb_step] [test_x_acc_r (<span class="org-builtin">len</span> test_x_list_r)])
  (<span class="org-keyword">assert</span> (np.isclose test_local_min 1.99948))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> test_nb_step 21)))
(test-case (test-rmsprop))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">plot_rmsprop</span> [gd m rms]
  (<span class="org-builtin">setv</span> [x_acc_gd x_list_gd y_list_gd] gd)
  (<span class="org-builtin">setv</span> [x_acc_m x_list_m y_list_m] m)
  (<span class="org-builtin">setv</span> [x_acc_r x_list_r y_list_r] rms)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">x</span> (.linspace np -1 3 500))

  (<span class="org-keyword">print</span> <span class="org-string">"Gradient descent local minimum:"</span> (<span class="org-builtin">str</span> x_acc_gd))
  (<span class="org-keyword">print</span> <span class="org-string">"Gradient descent number of steps:"</span> (<span class="org-builtin">str</span> (<span class="org-builtin">len</span> x_list_gd)))
  (<span class="org-keyword">print</span> <span class="org-string">"Momentum local minimum:"</span> (<span class="org-builtin">str</span> x_acc_m))
  (<span class="org-keyword">print</span> <span class="org-string">"Momentum number of steps:"</span> (<span class="org-builtin">str</span> (<span class="org-builtin">len</span> x_list_m)))
  (<span class="org-keyword">print</span> <span class="org-string">"RMSprop local minimum:"</span> (<span class="org-builtin">str</span> x_acc_r))
  (<span class="org-keyword">print</span> <span class="org-string">"RMSprop number of steps:"</span> (<span class="org-builtin">str</span> (<span class="org-builtin">len</span> x_list_r)))

  (.figure plt <span class="org-string">"rmsprop"</span>)
  (.subplot plt 1 2 2)
  (.scatter plt x_list_gd y_list_gd <span class="org-constant">:c</span> <span class="org-string">"g"</span>)
  (.scatter plt x_list_m y_list_m <span class="org-constant">:c</span> <span class="org-string">"r"</span>)
  (.scatter plt x_list_r y_list_r <span class="org-constant">:c</span> <span class="org-string">"magenta"</span>)
  (.plot plt x (func x) <span class="org-constant">:c</span> <span class="org-string">"b"</span>)
  (.savefig plt <span class="org-string">"rmsprop"</span>)
  (.close plt)

  (.figure plt <span class="org-string">"rmsprop_zoomed"</span>)
  (.subplot plt 1 2 1)
  (.scatter plt x_list_gd y_list_gd <span class="org-constant">:c</span> <span class="org-string">"g"</span>)
  (.scatter plt x_list_m y_list_m <span class="org-constant">:c</span> <span class="org-string">"r"</span>)
  (.scatter plt x_list_r y_list_r <span class="org-constant">:c</span> <span class="org-string">"magenta"</span>)
  (.plot plt x (func x) <span class="org-constant">:c</span> <span class="org-string">"b"</span>)
  (.xlim plt 1.0 2.1)
  (.savefig plt <span class="org-string">"rmsprop_zoomed"</span>)
  (.close plt))
(plot_rmsprop (gradient_descent 0.5 0 0.001 0.09)
              (momentum 0.5 0 0.001 0.09 0.9)
              (rmsprop 0.5 0 0.001 0.09 0.9))
(plot_rmsprop (gradient_descent 0.5 0 0.001 0.001)
              (momentum 0.5 0 0.001 0.001 0.9)
              (rmsprop 0.5 0 0.001 0.001 0.9))
</pre>
</div>

<figure>
<p><img src="./export/oa/rmsprop.png" class="img-responsive" alt="rmsprop.png" width="480">
</p>
<figcaption><span class="figure-number">Figure 27:</span> The RMSprop algorithm</figcaption>
</figure>

<figure>
<p><img src="./export/oa/rmsprop_zoomed.png" class="img-responsive" alt="rmsprop_zoomed.png" width="480">
</p>
<figcaption><span class="figure-number">Figure 28:</span> The RMSprop algorithm (zoomed)</figcaption>
</figure>


<p>
In the first case the standard gradient descent is the best choice,
and RMSprop is faster than Momentum.
In the second case RMSprop is the fastest, and Momentum doesn't work.
</p>
<pre class="example">
Gradient descent local minimum: 1.9995816511863498
Gradient descent number of steps: 15
Momentum local minimum: 2.170613047523136
Momentum number of steps: 75
RMSprop local minimum: 1.9994846996509628
RMSprop number of steps: 21

Gradient descent local minimum: 1.8177270754230204
Gradient descent number of steps: 567
Momentum local minimum: 0.500225
Momentum number of steps: 2
RMSprop local minimum: 1.0109592377573589
RMSprop number of steps: 500
</pre>


<p>
Implementation of Adam (Adaptive Moment Estimation) optimization algorithm.
Adam algorithm uses both Momentum and RMSprop algorithms with fixing bias. <br >
\(\mathtt{vdx = 0}\) <br >
\(\mathtt{sdx = 0}\) <br >
\(\mathtt{for:}\) <br >
<code>|</code> \(\mathtt{vdx = (\beta_1 \times vdx) + (1 - \beta_1) \times dx\ ;\ Momentum}\) <br >
<code>|</code> \(\mathtt{vdx = vdx / (1 - \beta_1^{t})\ ;\ Fixing\ bias\ for\ momentum}\) <br >
<code>|</code> <br >
<code>|</code> \(\mathtt{sdx = (\beta_2 \times sdx) + (1 - \beta_2) \times dx^{2}\ ;\ RMSprop}\) <br >
<code>|</code> \(\mathtt{sdx = sdx / (1 - \beta_2^{t})\ ;\ Fixing\ bias\ for\ RMSprop}\) <br >
<code>|</code> <br >
<code>|</code> \(\mathtt{x = x - learning\_rate \times vdx / (\sqrt(sdx) + \epsilon)}\) <br >
<code>|</code> \(\mathtt{\ ;\ Momentum\ because\ "\times vdx"}\) <br >
<code>|</code> \(\mathtt{\ ;\ RMSprop\ because\ "/ (\sqrt(sdx) + \epsilon)"}\)
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">momentum_bias</span> [x vdx beta t]
  (<span class="org-builtin">/</span> (<span class="org-builtin">+</span> (<span class="org-builtin">*</span> beta vdx) (<span class="org-builtin">*</span> (<span class="org-builtin">-</span> 1 beta) (deriv_func x))) (<span class="org-builtin">-</span> 1 (<span class="org-builtin">**</span> beta t))))

(<span class="org-keyword">defn</span> <span class="org-function-name">rmsprop_bias</span> [x sdx beta t]
  (<span class="org-builtin">/</span> (<span class="org-builtin">+</span> (<span class="org-builtin">*</span> beta sdx) (<span class="org-builtin">*</span> (<span class="org-builtin">-</span> 1 beta) (<span class="org-builtin">**</span> (deriv_func x) 2))) (<span class="org-builtin">-</span> 1 (<span class="org-builtin">**</span> beta t))))

(<span class="org-keyword">defn</span> <span class="org-function-name">adam</span> [x_next x_first precision l_r beta1 beta2 epsilon]
  (<span class="org-keyword">loop</span> [[t 2]
         [x_acc_prev x_first]
         [vdx (momentum_bias x_next 0 beta1 1)]
         [sdx (rmsprop_bias x_next 0 beta2 1)]
         [x_acc x_next]
         [x_list [x_next]]
         [y_list [(func x_next)]]]
        (<span class="org-keyword">if</span> (<span class="org-builtin">&lt;=</span> (<span class="org-builtin">abs</span> (<span class="org-builtin">-</span> x_acc x_acc_prev)) precision)
            [x_acc x_list y_list]
            (<span class="org-keyword">recur</span>
              (<span class="org-builtin">inc</span> t)
              x_acc
              (momentum_bias x_next vdx beta1 t)
              (rmsprop_bias x_next sdx beta2 t)
              (<span class="org-builtin">-</span> x_acc (<span class="org-builtin">/</span> (<span class="org-builtin">*</span> l_r vdx) (<span class="org-builtin">+</span> (math.sqrt sdx) epsilon)))
              (<span class="org-builtin">+</span> x_list [x_acc])
              (<span class="org-builtin">+</span> y_list [(func x_acc)])))))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-adam</span> []
  (<span class="org-builtin">setv</span> [test_x_acc_a test_x_list_a test_y_list_a]
        (adam 0.5 0 0.001 0.6 0.9 0.99 (<span class="org-builtin">**</span> 10 -8)))
  (<span class="org-builtin">setv</span> [test_local_min test_nb_step] [test_x_acc_a (<span class="org-builtin">len</span> test_x_list_a)])
  (<span class="org-keyword">assert</span> (np.isclose test_local_min 2.068365))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> test_nb_step 12)))
(test-case (test-adam))
</pre>
</div>
</div>
</li></ol>
</div>
<div id="outline-container-sec-3-6-3" class="outline-4">
<h4 id="sec-3-6-3"><span class="section-number-4">3.6.3</span> Results</h4>
<div class="outline-text-4" id="text-3-6-3">
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">plot_adam</span> [only_adam_plot]
  (<span class="org-builtin">setv</span> [x_acc_gd x_list_gd y_list_gd] (gradient_descent 0.5 0 0.001 0.09))
  (<span class="org-builtin">setv</span> [x_acc_m x_list_m y_list_m] (momentum 0.5 0 0.001 0.09 0.9))
  (<span class="org-builtin">setv</span> [x_acc_r x_list_r y_list_r] (rmsprop 0.5 0 0.001 0.09 0.9))
  (<span class="org-builtin">setv</span> [x_acc_a x_list_a y_list_a] (adam 0.5 0 0.001 0.6 0.9 0.99 (<span class="org-builtin">**</span> 10 -8)))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">x</span> (.linspace np -1 3 500))

  (<span class="org-keyword">print</span> <span class="org-string">"Gradient descent local minimum:"</span> (<span class="org-builtin">str</span> x_acc_gd))
  (<span class="org-keyword">print</span> <span class="org-string">"Gradient descent number of steps:"</span> (<span class="org-builtin">str</span> (<span class="org-builtin">len</span> x_list_gd)))
  (<span class="org-keyword">print</span> <span class="org-string">"Momentum local minimum:"</span> (<span class="org-builtin">str</span> x_acc_m))
  (<span class="org-keyword">print</span> <span class="org-string">"Momentum number of steps:"</span> (<span class="org-builtin">str</span> (<span class="org-builtin">len</span> x_list_m)))
  (<span class="org-keyword">print</span> <span class="org-string">"RMSprop local minimum:"</span> (<span class="org-builtin">str</span> x_acc_r))
  (<span class="org-keyword">print</span> <span class="org-string">"RMSprop number of steps:"</span> (<span class="org-builtin">str</span> (<span class="org-builtin">len</span> x_list_r)))
  (<span class="org-keyword">print</span> <span class="org-string">"Adam local minimum:"</span> (<span class="org-builtin">str</span> x_acc_a))
  (<span class="org-keyword">print</span> <span class="org-string">"Adam number of steps:"</span> (<span class="org-builtin">str</span> (<span class="org-builtin">len</span> x_list_a)))

  (.figure plt <span class="org-string">"adam"</span>)
  (.subplot plt 1 2 2)
  (<span class="org-keyword">when</span> (<span class="org-builtin">is</span> only_adam_plot <span class="org-constant">False</span>)
    (.scatter plt x_list_gd y_list_gd <span class="org-constant">:c</span> <span class="org-string">"g"</span>)
    (.scatter plt x_list_m y_list_m <span class="org-constant">:c</span> <span class="org-string">"r"</span>)
    (.scatter plt x_list_r y_list_r <span class="org-constant">:c</span> <span class="org-string">"magenta"</span>))
  (.scatter plt x_list_a y_list_a <span class="org-constant">:c</span> <span class="org-string">"cyan"</span>)
  (.plot plt x (func x) <span class="org-constant">:c</span> <span class="org-string">"b"</span>)
  (.savefig plt <span class="org-string">"adam"</span>)
  (.close plt)

  (.figure plt <span class="org-string">"adam_zoomed"</span>)
  (.subplot plt 1 2 1)
  (.scatter plt x_list_gd y_list_gd <span class="org-constant">:c</span> <span class="org-string">"g"</span>)
  (.scatter plt x_list_m y_list_m <span class="org-constant">:c</span> <span class="org-string">"r"</span>)
  (.scatter plt x_list_r y_list_r <span class="org-constant">:c</span> <span class="org-string">"magenta"</span>)
  (.scatter plt x_list_a y_list_a <span class="org-constant">:c</span> <span class="org-string">"cyan"</span>)
  (.plot plt x (func x) <span class="org-constant">:c</span> <span class="org-string">"b"</span>)
  (.xlim plt 1.0 2.1)
  (.savefig plt <span class="org-string">"adam_zoomed"</span>)
  (.close plt))
(plot_adam <span class="org-constant">False</span>)
(plot_adam <span class="org-constant">True</span>)
</pre>
</div>

<figure>
<p><img src="./export/oa/adam.png" class="img-responsive" alt="adam.png" width="480">
</p>
<figcaption><span class="figure-number">Figure 29:</span> The adam algorithm</figcaption>
</figure>

<figure>
<p><img src="./export/oa/adam_zoomed.png" class="img-responsive" alt="adam_zoomed.png" width="480">
</p>
<figcaption><span class="figure-number">Figure 30:</span> The adam algorithm (zoomed)</figcaption>
</figure>


<p>
In this configuration (data, beta choice, learning rate, etc.)
Adam algorithm is the fastest. It finds the local minimum in a few steps.
Again in this configuration the standard gradient descent is better than
both Momentum and RMSprop algorithms.
Adam algorithm can be tuned with two parameters (beta1, beta2),
and it's an advantage over the other algorithms.
</p>
<pre class="example">
Gradient descent local minimum: 1.9995816511863498
Gradient descent number of steps: 15
Momentum local minimum: 2.170613047523136
Momentum number of steps: 75
RMSprop local minimum: 1.9994846996509628
RMSprop number of steps: 21
Adam local minimum: 2.068365088385878
Adam number of steps: 12
</pre>
</div>
</div>
<div id="outline-container-sec-3-6-4" class="outline-4">
<h4 id="sec-3-6-4"><span class="section-number-4">3.6.4</span> References</h4>
<div class="outline-text-4" id="text-3-6-4">
<ul class="org-ul">
<li>deeplearning.ai. 2020. <a href="https://www.deeplearning.ai/deep-learning-specialization/">Deep Learning Specialization</a>.
</li>
<li>Battini Deepak. 2018. <a href="https://www.tech-quantum.com/implementation-of-gradient-descent-in-python/">Implementation of Gradient Descent in Python</a>.
</li>
<li>Battini Deepak. 2018. <a href="https://www.tech-quantum.com/adam-optimization-algorithms-in-deep-learning/">Adam Optimization algorithms in Deep Learning</a>.
</li>
<li>Brownlee Jason. 2017. <a href="https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/">Gentle Introduction to the Adam Optimization Algorithm for Deep Learning</a>.
</li>
<li>Ashyibo. 2019. <a href="https://medium.com/datadriveninvestor/exponentially-weighted-average-for-deep-neural-networks-39873b8230e9">Exponentially Weighted Average for Deep Neural Networks</a>.
</li>
<li>Chang Henry. 2018. <a href="https://medium.com/@hengluchang/visualizing-gradient-descent-with-momentum-in-python-7ef904c8a847">Visualizing Gradient Descent with Momentum in Python</a>.
</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-sec-3-7" class="outline-3">
<h3 id="sec-3-7"><span class="section-number-3">3.7</span> LeNet-5: Convolutional Neural Network Model From Scratch</h3>
<div class="outline-text-3" id="text-3-7">

<figure>
<p><img src="./export/img/lenet5.png" class="img-responsive" alt="lenet5.png" width="480">
</p>
</figure>
</div>
<div id="outline-container-sec-3-7-1" class="outline-4">
<h4 id="sec-3-7-1"><span class="section-number-4">3.7.1</span> Purpose</h4>
<div class="outline-text-4" id="text-3-7-1">
<p>
The objective is to implement all layers of a
Convolutional Neural Network (CNN)
from scratch, and use them to build a LeNet-5 model.
LeNet-5 is a CNN architecture used for handwritten digit recognition.
This project contains only the forward propagation (first step),
so the model will be not trained, and its output will be only used to check
if the forward propagation algorithm works correctly.
The dataset used here comes from MNIST (handwritten digit) database,
however it could be anything else since only forward propagation is implemented.
</p>
</div>
</div>
<div id="outline-container-sec-3-7-2" class="outline-4">
<h4 id="sec-3-7-2"><span class="section-number-4">3.7.2</span> Implementation</h4>
<div class="outline-text-4" id="text-3-7-2">
</div><ol class="org-ol"><li><a id="sec-3-7-2-1" name="sec-3-7-2-1"></a>Definition<br ><div class="outline-text-5" id="text-3-7-2-1">
<p>
Convolutional Neural Networks (CNN) are one of the neural network architectures.
For image processing,
CNNs are successful compare to classic fully connected layers.
CNNs are also used for many other tasks such as speech recognition
and Natural Language Processing (NLP).
The two main advantages of CNN are:
</p>
<ul class="org-ul">
<li>Parameter sharing (filters stacked)
</li>
<li>Each output value depends only on a few inputs
</li>
</ul>
<p>
The CNN layers are (described in the next section):
</p>
<ul class="org-ul">
<li>Conv
</li>
<li>Pool
</li>
<li>FC
</li>
<li>Activation function
</li>
</ul>


<p>
In Conv layer an activation function is also used,
because the purpose of an activation function is to introduce non-linearity
to a system that basically has just been computing linear operations during
 the Conv layers (element wise multiplications and summations).
If only linear functions are used then the neural network would
just capable of learning a linear function.
Adding a non-linearity is needed to increase the range of functions
that the neural network can learn.
Non-linear functions could be either \(\operatorname{ReLU}\) or Max-pool.
Below the mathematical proof, and let the following equations:
</p>
\begin{align*}
z^{[1]} & = W^{[1]} x + b^{[1]} \\\\
a^{[1]} & = g^{[1]} (z^{[1]}) \\\\
z^{[2]} & = W^{[2]} a^{[1]} + b^{[2]} \\\\
a^{[2]} & = g^{[2]} (z^{[2]})
\end{align*}
<p>
If \(a\) is an linear activation function then \(a = Wx + b\), thus \(a = Z\), then:
</p>
\begin{align*}
a^{[1]} & = z^{[1]} = W^{[1]} x + b^{[1]} \\\\
a^{[2]} & = z^{[2]} = W^{[2]} a^{[1]} + b^{[2]} \\\\
a^{[2]} & = W^{[2]} \times (W^{[1]} x + b^{[1]}) + b^{[2]} \\\\
a^{[2]} & = (W^{[2]} \times W^{[1]}) x + (W^{[2]} \times b^{[1]} + b^{[2]}) \\\\
a^{[2]} & = W' x + b'
\end{align*}
<p>
To conclude, using linear activation function in neural network
would just output a linear function.
</p>


<p>
The main classical CNN architectures are:
</p>
<ul class="org-ul">
<li>LeNet-5: identify handwritten digits: Conv -&gt; Pool -&gt; Conv -&gt; Pool -&gt;
FC -&gt; FC -&gt; \(\operatorname{softmax}\)
</li>
<li>AlexNet: ImageNet challenge which classifies images: Conv -&gt; Max-pool -&gt;
Conv -&gt; Max-pool -&gt; Conv -&gt; Conv -&gt; Conv -&gt; Max-pool -&gt;
FC -&gt; FC -&gt; \(\operatorname{softmax}\)
</li>
<li>VGG: a modification for AlexNet
</li>
<li>ResNet: skip connection (based on Residual block),
won the last ImageNet competition
</li>
<li>Inception: use all layers at the same time
</li>
</ul>
</div>
</li>
<li><a id="sec-3-7-2-2" name="sec-3-7-2-2"></a>Initialization<br ><div class="outline-text-5" id="text-3-7-2-2">
<p>
Import necessaries modules: the MNIST dataset, and NumPy.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">require</span> [hy.contrib.loop [<span class="org-keyword">loop</span>]])
(<span class="org-keyword">require</span> [hytmf [test-case]])
(<span class="org-keyword">import</span> keras)
(<span class="org-keyword">import</span> [keras.datasets
         [mnist]])
(<span class="org-keyword">import</span> [matplotlib [pyplot]])
(<span class="org-keyword">import</span> [numpy <span class="org-keyword">:as</span> np])
(<span class="org-keyword">import</span> [keras [backend <span class="org-keyword">:as</span> K]])
</pre>
</div>
</div>
</li>
<li><a id="sec-3-7-2-3" name="sec-3-7-2-3"></a>Functions<br ><div class="outline-text-5" id="text-3-7-2-3">
<p>
The code block below is described at <a href="#src-relu">src-relu</a>.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">relu</span> [Z]
  (.maximum np 0 Z))

(<span class="org-keyword">defn</span> <span class="org-function-name">drelu</span> [Z]
  (.array np (<span class="org-keyword">lfor</span> x Z (<span class="org-keyword">lfor</span> y x (<span class="org-keyword">if</span> (<span class="org-builtin">&lt;=</span> y 0)
                                     0
                                     1)))))
</pre>
</div>


<p>
The \(\operatorname{softmax}\) regression is a generalization of Logistic
Regression (binary output)
that produces a probability distribution,
and it is used for multiclass classification.
\(\operatorname{softmax}\) is defined as following:
</p>
\begin{align*}
\sigma(z)_i = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}}, \text{ for } i = 1,
\dotsc , K \text{ and } z=(z_1,\dotsc,z_K) \in \mathbb{R}^K
\end{align*}
<p>
It means that the exponential function is applied to each element
\(z_i\) of the input vector \(\mathbf{z}\),
and normalizes these values by dividing by the sum of all these exponential
functions,
that ensures that the sum of the element of vector \(\mathbf{z}\) is 1.
<a id="src-softmax" name="src-softmax"></a>
</p>
<div class="org-src-container">

<pre class="src src-hy" id="src-softmax">(<span class="org-keyword">defn</span> <span class="org-function-name">softmax</span> [Z]
  (<span class="org-builtin">setv</span> <span class="org-variable-name">e_Z</span> (.exp np Z))
  (<span class="org-builtin">/</span> e_Z (.sum np e_Z <span class="org-constant">:axis</span> 0)))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-softmax</span> []
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> 1 (<span class="org-builtin">int</span> (<span class="org-builtin">sum</span> (softmax [1 3 2.5 5 4 2]))))))
(test-case (test-softmax))
</pre>
</div>


<p>
The code block below is described at <a href="#src-loss">src-loss</a>.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">cross_entropy_loss</span> [Y Y_hat]
  (<span class="org-builtin">-</span> (.sum np (<span class="org-builtin">*</span> Y (.log np Y_hat)))))
</pre>
</div>
</div>
</li>
<li><a id="sec-3-7-2-4" name="sec-3-7-2-4"></a>Convolutional Functions<br ><div class="outline-text-5" id="text-3-7-2-4">
<p>
Zero padding has two main features:
</p>
<ul class="org-ul">
<li>It avoids shrinking image when using Conv layer: when building deep networks
it's important to preserve image size, this form of network is
called <i>same</i> convolution
</li>
<li>It allows keeping more information about the border of an image
</li>
</ul>
<p>
Let a matrix \(\mathbf{A}\) of \(\dim n \times n\) be convolved
with \(f \times f\) filter/kernel and padding \(p\),
then the dimension output of \(\mathbf{A}\) is \([n+2p-f+1] \times [n+2p-f+1]\).
In case of <i>same convolutional</i>, \(p = (f-1) / 2\).
</p>

<p>
For example, let the matrix \(\mathbf{A}\) be:
</p>
\begin{align*}
\begin{bmatrix}
123 & 94 & 83 & 2 \\
34 & 44 & 187 & 92 \\
34 & 76 & 232 & 124 \\
67 & 83 & 194 & 202
\end{bmatrix}
\end{align*}
<p>
With a padding of 2, the matrix \(\mathbf{A}\) becomes:
</p>
\begin{align*}
\begin{bmatrix}
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 123 & 94 & 83 & 2 & 0 & 0 \\
0 & 0 & 34 & 44 & 187 & 92 & 0 & 0 \\
0 & 0 & 34 & 76 & 232 & 124 & 0 & 0 \\
0 & 0 & 67 & 83 & 194 & 202 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0
\end{bmatrix}
\end{align*}
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">zero_pad</span> [X pad]
  (.pad np X
        <span class="org-constant">:pad_width</span> [[0 0] [pad pad] [pad pad] [0 0]]
        <span class="org-constant">:mode</span> <span class="org-string">"constant"</span>
        <span class="org-constant">:constant_values</span> [0 0]))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-zero_pad</span> []
  (np.random.seed 1)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_X</span> (np.random.randn 4 3 3 2))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_X_pad</span> (zero_pad test_X 2))
  (<span class="org-builtin">get</span> test_X 1 1)
  (<span class="org-builtin">get</span> test_X_pad 1 1)
  (<span class="org-keyword">assert</span> (np.isclose (.mean (<span class="org-builtin">get</span> test_X 1 1)) -0.0965106))
  (<span class="org-keyword">assert</span> (np.isclose (.mean (<span class="org-builtin">get</span> test_X_pad 1 1)) 0)))
(test-case (test-zero_pad))
</pre>
</div>


<p>
Convolution/filter operation is one of the fundamental blocks of CNN.
It applies a filter: element-wise (Hadamard product) with the original matrix,
then summing them up and adding a bias.
An example of filter could be <i>edge detection</i>.
The following code block implements a single step (in green) of the algorithm:
</p>
\begin{align*}
\begin{bmatrix}
\color{green}{3} & \color{green}{0} & \color{green}{1} & 2 & 7 & 4 \\
\color{green}{1} & \color{green}{5} & \color{green}{8} & 9 & 3 & 1 \\
\color{green}{2} & \color{green}{7} & \color{green}{2} & 5 & 1 & 3 \\
0 & 1 & 3 & 1 & 7 & 8 \\
4 & 2 & 1 & 6 & 2 & 8 \\
2 & 4 & 5 & 2 & 3 & 9
\end{bmatrix} \odot \begin{bmatrix}
1 & 0 & -1 \\
1 & 0 & -1 \\
1 & 0 & -1
\end{bmatrix} = \begin{bmatrix}
\color{green}{3} & \color{green}{0} & \color{green}{-1} \\
\color{green}{1} & \color{green}{0} & \color{green}{-8} \\
\color{green}{2} & \color{green}{0} & \color{green}{-2} \\
\end{bmatrix}
\end{align*}
<p>
The result output of this function is the sum of all elements of the matrix,
in this example: \(-5 + bias\).
Apply this function on the entire matrix with a stride of 1
(the filter moves at the column +1), give:
</p>
\begin{align*}
\begin{bmatrix}
\color{green}{-5 + b} & -4 & 0 & 8 \\
-10 & -2 & 2 & 3 \\
0 & -2 & -4 & -7 \\
-3 & -2 & -3 & -16
\end{bmatrix}
\end{align*}
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">conv_func</span> [A_slice Filter bias]
  (<span class="org-keyword">-&gt;&gt;</span> (.multiply np A_slice Filter)
       (.sum np)
       (.add np (.float np bias))))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-conv_func</span> []
  (np.random.seed 1)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_A_slice</span> (np.random.randn 4 4 3))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_F</span> (np.random.randn 4 4 3))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_b</span> (np.random.randn 1 1 1))
  (<span class="org-keyword">assert</span> (np.isclose (conv_func test_A_slice test_F test_b) -6.9990894)))
(test-case (test-conv_func))
</pre>
</div>


<p>
Convolution over volumes is a method to apply one or many filters
on \(N\) dimension input, for example on a RGB image (3D).
Each operation gives a \(K\) dimension matrix output,
and then all outputs are stacked to get a new \(K'\) dimension matrix.
In case of a single filter:
</p>
<ul class="org-ul">
<li>Input image: 6x6x3 (height x weight x rgb/previous filter number)
</li>
<li>1 Filter: 3x3x3
</li>
<li>Result image: 4x4x1
</li>
<li>With padding=0, stride(the number of pixels to jump)=1
</li>
</ul>
<p>
In case of multiple filters (10):
</p>
<ul class="org-ul">
<li>Input image: 6x6x3 (height x weight x rgb/previous filter number)
</li>
<li>10 Filters: 3x3x3
</li>
<li>Result image: 4x4x10 (10 stacked filters)
</li>
<li>With padding=0, stride(the number of pixels to jump)=1
</li>
</ul>
<p>
The implementation uses the following formula:
</p>
\begin{align*}
n_{height} & = \lfloor \frac{(n_{height\_prev} - n_{f})
+ (2 \times pad)}{stride} \rfloor + 1 \\\\
n_{weight} & = \lfloor \frac{(n_{weight\_prev} - n_{f})
+ (2 \times pad)}{stride} \rfloor + 1
\end{align*}
<p>
\(n_{channel}\) is the number of filters used,
\(n_{height}\) and \(n_{weight}\) correspond to a line and column matrix,
respectively.
The dimension of the output matrix is
\(n_{height} \times n_{weight} \times n_{channel}\).
</p>

<p>
In the following matrix:
</p>
<ul class="org-ul">
<li><code>line_start</code> = 1
</li>
<li><code>line_end</code> = 2
</li>
<li><code>col_start</code> = 0
</li>
<li><code>col_end</code> = 2
</li>
<li>The filter (in green) is of dimension 2x2
</li>
<li>This example shows only an operation on a single channel
</li>
</ul>
\begin{align*}
\begin{bmatrix}
255 & 231 & 42 & 22 \\
\color{green}{123} & \color{green}{94} & 83 & 2 \\
\color{green}{34} & \color{green}{44} & 187 & 92 \\
34 & 76 & 232 & 124 \\
67 & 83 & 194 & 202
\end{bmatrix}
\end{align*}
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">conv</span> [A Filter bias pad stride activation_func]
  (<span class="org-builtin">setv</span> [training_example n_height_prev n_weight_prev n_channel_prev] A.shape)
  (<span class="org-builtin">setv</span> [n_f n_f n_channel_prev n_channel] Filter.shape)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">n_height</span> (<span class="org-builtin">int</span> (<span class="org-builtin">+</span> (<span class="org-builtin">/</span> (<span class="org-builtin">+</span> (<span class="org-builtin">-</span> n_height_prev n_f) (<span class="org-builtin">*</span> 2 pad)) stride) 1)))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">n_weight</span> (<span class="org-builtin">int</span> (<span class="org-builtin">+</span> (<span class="org-builtin">/</span> (<span class="org-builtin">+</span> (<span class="org-builtin">-</span> n_weight_prev n_f) (<span class="org-builtin">*</span> 2 pad)) stride) 1)))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">A_pad</span> (zero_pad A pad))

  (.array np
          (<span class="org-keyword">lfor</span> i (<span class="org-builtin">range</span> training_example)
                (<span class="org-keyword">lfor</span> h (<span class="org-builtin">range</span> n_height)
                      <span class="org-constant">:setv</span> line_start (<span class="org-builtin">*</span> h stride)
                      <span class="org-constant">:setv</span> line_end (<span class="org-builtin">+</span> line_start n_f)
                      (<span class="org-keyword">lfor</span> w (<span class="org-builtin">range</span> n_weight)
                            <span class="org-constant">:setv</span> col_start (<span class="org-builtin">*</span> w stride)
                            <span class="org-constant">:setv</span> col_end (<span class="org-builtin">+</span> col_start n_f)
                            (<span class="org-keyword">lfor</span> c (<span class="org-builtin">range</span> n_channel)
                                  (activation_func
                                    (conv_func
                                      (<span class="org-builtin">get</span> A_pad (<span class="org-builtin">,</span> i
                                                    (<span class="org-builtin">slice</span>
                                                      line_start
                                                      line_end)
                                                    (<span class="org-builtin">slice</span> col_start col_end)
                                                    (<span class="org-builtin">slice</span> <span class="org-constant">None</span>)))
                                      (<span class="org-builtin">get</span> Filter (<span class="org-builtin">,</span> (<span class="org-builtin">slice</span> <span class="org-constant">None</span>)
                                                     (<span class="org-builtin">slice</span> <span class="org-constant">None</span>)
                                                     (<span class="org-builtin">slice</span> <span class="org-constant">None</span>)
                                                     c))
                                      (<span class="org-builtin">get</span> bias (<span class="org-builtin">,</span> (<span class="org-builtin">slice</span> <span class="org-constant">None</span>)
                                                   (<span class="org-builtin">slice</span> <span class="org-constant">None</span>)
                                                   (<span class="org-builtin">slice</span> <span class="org-constant">None</span>)
                                                   c))))))))))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-conv</span> []
  (np.random.seed 1)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_A</span> (np.random.randn 4 3 2 1))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_Filter</span> (np.random.randn 3 3 4 8))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_bias</span> (np.random.randn 1 1 1 8))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">Z</span> (conv test_A test_Filter test_bias 1 2 (<span class="org-keyword">fn</span> [x] x)))
  (<span class="org-keyword">assert</span> (np.isclose (.mean Z) 0.8798607)))
(test-case (test-conv))
</pre>
</div>


<p>
Pooling (sampling) layer reduces the size of
the inputs to speed up/reduce computation.
It's applied for each dimension individually (on <code>n_channel</code>).
The types of pooling layers are:
</p>
<ul class="org-ul">
<li>Max-pooling layer: keeps the max value
</li>
<li>Average-pooling layer: keeps the average value
</li>
</ul>
<p>
Since padding are rarely used in pooling layer,
the code block implements the following formula:
</p>
\begin{align*}
n_{height} & = \lfloor \frac{(n_{height\_prev} - n_{f})}{stride} \rfloor + 1 \\\\
n_{weight} & = \lfloor \frac{(n_{weight\_prev} - n_{f})}{stride} \rfloor + 1 \\\\
\end{align*}
<p>
\(n_{channel}\) is the number of filters used.
</p>

<p>
Let the following matrix be an image with a specific channel (2D):
</p>
\begin{align*}
\begin{bmatrix}
\color{blue}{2} & \color{blue}{3} & \color{cyan}{1} & \color{cyan}{9} \\
\color{blue}{4} & \color{blue}{7} & \color{cyan}{3} & \color{cyan}{5} \\
\color{red}{8} & \color{red}{2} & \color{green}{2} & \color{green}{2} \\
\color{red}{1} & \color{red}{3} & \color{green}{4} & \color{green}{5}
\end{bmatrix}
\end{align*}
<p>
By applying the Max-pool with a 2x2 filter and stride 2, the result is:
</p>
\begin{align*}
\begin{bmatrix}
\color{blue}{7} & \color{cyan}{9} \\
\color{red}{8} & \color{green}{5}
\end{bmatrix}
\end{align*}
<p>
and then, with the Average-pool:
</p>
\begin{align*}
\begin{bmatrix}
\color{blue}{4} & \color{cyan}{4.5} \\
\color{red}{3.25} & \color{green}{3.25}
\end{bmatrix}
\end{align*}
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">pool</span> [A n_f stride pool_func]
  (<span class="org-builtin">setv</span> [training_example n_height_prev n_weight_prev n_channel_prev] A.shape)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">n_height</span> (<span class="org-builtin">int</span> (<span class="org-builtin">+</span> (<span class="org-builtin">/</span> (<span class="org-builtin">-</span> n_height_prev n_f) stride) 1)))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">n_weight</span> (<span class="org-builtin">int</span> (<span class="org-builtin">+</span> (<span class="org-builtin">/</span> (<span class="org-builtin">-</span> n_weight_prev n_f) stride) 1)))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">n_channel</span> n_channel_prev)

  (.array np
          (<span class="org-keyword">lfor</span> i (<span class="org-builtin">range</span> training_example)
                (<span class="org-keyword">lfor</span> h (<span class="org-builtin">range</span> n_height)
                      <span class="org-constant">:setv</span> line_start (<span class="org-builtin">*</span> h stride)
                      <span class="org-constant">:setv</span> line_end (<span class="org-builtin">+</span> line_start n_f)
                      (<span class="org-keyword">lfor</span> w (<span class="org-builtin">range</span> n_weight)
                            <span class="org-constant">:setv</span> col_start (<span class="org-builtin">*</span> w stride)
                            <span class="org-constant">:setv</span> col_end (<span class="org-builtin">+</span> col_start n_f)
                            (<span class="org-keyword">lfor</span> c (<span class="org-builtin">range</span> n_channel)
                                  (pool_func
                                    (<span class="org-builtin">get</span> A (<span class="org-builtin">,</span> i
                                              (<span class="org-builtin">slice</span> line_start line_end)
                                              (<span class="org-builtin">slice</span> col_start col_end)
                                              c)))))))))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-pool</span> []
  (np.random.seed 1)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_A</span> (np.random.randn 2 4 4 3))

  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_Z</span> (pool test_A 3 1 np.max))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> test_Z.shape '(2 2 2 3)))
  (<span class="org-keyword">assert</span> (np.isclose (.mean test_Z) 1.506436))

  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_Z</span> (pool test_A 3 1 np.mean))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> test_Z.shape '(2 2 2 3)))
  (<span class="org-keyword">assert</span> (np.isclose (.mean test_Z) 0.0284164))

  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_Z</span> (pool test_A 3 2 np.max))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> test_Z.shape '(2 1 1 3)))
  (<span class="org-keyword">assert</span> (np.isclose (.mean test_Z) 1.430168408))

  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_Z</span> (pool test_A 3 2 np.mean))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> test_Z.shape '(2 1 1 3)))
  (<span class="org-keyword">assert</span> (np.isclose (.mean test_Z) 0.0318414)))
(test-case (test-pool))
</pre>
</div>
</div>
</li>
<li><a id="sec-3-7-2-5" name="sec-3-7-2-5"></a>MNIST Initialization<br ><div class="outline-text-5" id="text-3-7-2-5">
<p>
The MNIST (Modified National Institute of Standards and Technology database)
is a database composed of handwritten digits.
It has a training set of 60000 examples, and a test set of 10000 examples.
The code below loads data from MNIST dataset,
and split it between training and test sets.
<a id="src-import-dataset-mnist" name="src-import-dataset-mnist"></a>
</p>
<div class="org-src-container">

<pre class="src src-hy" id="src-import-dataset-mnist">(<span class="org-builtin">setv</span> [[train_set_x_orig train_set_y_orig] [test_set_x_orig test_set_y_orig]]
      (.load_data mnist))
</pre>
</div>


<p>
Set constant with specific value from MNIST documentation.
<a id="src-set-params" name="src-set-params"></a>
</p>
<div class="org-src-container">

<pre class="src src-hy" id="src-set-params">(<span class="org-builtin">setv</span> <span class="org-variable-name">num_classes</span> 10)
(<span class="org-builtin">setv</span> <span class="org-variable-name">num_examples</span> 60000)
(<span class="org-builtin">setv</span> <span class="org-variable-name">num_examples_small</span> 60)
(<span class="org-builtin">setv</span> <span class="org-variable-name">img_rows</span> 28)
(<span class="org-builtin">setv</span> <span class="org-variable-name">img_cols</span> 28)
(<span class="org-builtin">setv</span> <span class="org-variable-name">img_depth</span> 1)
</pre>
</div>


<p>
Show some MNIST entries.
</p>
<div class="org-src-container">

<pre class="src src-hy">(.figure pyplot <span class="org-string">"mnist"</span>)
(<span class="org-keyword">loop</span> [[i 0]]
      (<span class="org-keyword">when</span> (<span class="org-builtin">&lt;</span> i 9)
        (.subplot pyplot (<span class="org-builtin">+</span> 330 1 i))
        (.imshow pyplot (<span class="org-builtin">get</span> train_set_x_orig i))
        (<span class="org-keyword">recur</span> (<span class="org-builtin">inc</span> i))))
(.savefig pyplot <span class="org-string">"mnist"</span>)
(.close pyplot)
</pre>
</div>

<figure>
<p><img src="./export/lenet5/mnist.png" class="img-responsive" alt="mnist.png" width="480">
</p>
<figcaption><span class="figure-number">Figure 32:</span> MNIST picture example</figcaption>
</figure>


<p>
For convenience during matrix multiplication both <code>train_set_x</code>
and <code>test_set_x</code> are reshaped (new dimension added).
<a id="src-flatten-dataset-mnist" name="src-flatten-dataset-mnist"></a>
</p>
<div class="org-src-container">

<pre class="src src-hy" id="src-flatten-dataset-mnist">(<span class="org-builtin">setv</span> <span class="org-variable-name">train_set_x_flatten</span> (.reshape train_set_x_orig
                                    (<span class="org-builtin">get</span> train_set_x_orig.shape 0)
                                    img_rows
                                    img_cols
                                    img_depth))
(<span class="org-builtin">setv</span> <span class="org-variable-name">test_set_x_flatten</span> (.reshape test_set_x_orig
                                   (<span class="org-builtin">get</span> test_set_x_orig.shape 0)
                                   img_rows
                                   img_cols
                                   img_depth))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-flatten</span> []
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> train_set_x_orig.shape (<span class="org-builtin">,</span> 60000 28 28)))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> test_set_x_orig.shape (<span class="org-builtin">,</span> 10000 28 28)))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> train_set_x_flatten.shape (<span class="org-builtin">,</span> 60000 28 28 1)))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> test_set_x_flatten.shape (<span class="org-builtin">,</span> 10000 28 28 1))))
(test-case (test-flatten))
</pre>
</div>


<p>
Normalization preprocessing step centers and standardizes dataset,
where 255 is the maximum value of a pixel channel.
The code block converts also class vectors into binary class matrix.
<a id="src-normalize-dataset-mnist" name="src-normalize-dataset-mnist"></a>
</p>
<div class="org-src-container">

<pre class="src src-hy" id="src-normalize-dataset-mnist">(<span class="org-builtin">setv</span> <span class="org-variable-name">train_set_x</span> (<span class="org-builtin">/</span> train_set_x_flatten 255))
(<span class="org-builtin">setv</span> <span class="org-variable-name">test_set_x</span> (<span class="org-builtin">/</span> test_set_x_flatten 255))
(<span class="org-builtin">setv</span> <span class="org-variable-name">train_set_y</span> (.to_categorical keras.utils train_set_y_orig num_classes))
(<span class="org-builtin">setv</span> <span class="org-variable-name">test_set_y</span> (.to_categorical keras.utils test_set_y_orig num_classes))
</pre>
</div>


<p>
Resize both training and test sets to reduce the number of training examples
in order to speed up computation time.
Both <code>train_set_x</code> and <code>test_train_x</code> represent only
60 images (<code>num_examples_small</code>),
of dimension <code>img_rows</code> x <code>img_cols</code> x <code>img_depth</code>.
Both <code>train_set_y</code> and <code>test_set_y</code> contain
a probability distribution (10 probabilities proportional) for each image.
For example the vector
\(\begin{bmatrix} 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \end{bmatrix}\) informs
that the image corresponds to the digit 2 with probability 1
and others with probability 0.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> <span class="org-variable-name">train_set_x_small</span> (.resize np train_set_x [num_examples_small
                                                 img_rows
                                                 img_cols
                                                 img_depth]))
(<span class="org-builtin">setv</span> <span class="org-variable-name">test_set_x_small</span> (.resize np test_set_x [num_examples_small
                                               img_rows
                                               img_cols
                                               img_depth]))
(<span class="org-builtin">setv</span> <span class="org-variable-name">train_set_y_small</span> (.resize np train_set_y [num_examples_small
                                                 num_classes]))
(<span class="org-builtin">setv</span> <span class="org-variable-name">test_set_y_small</span> (.resize np test_set_y [num_examples_small
                                               num_classes]))
</pre>
</div>
</div>
</li>
<li><a id="sec-3-7-2-6" name="sec-3-7-2-6"></a>Model<br ><div class="outline-text-5" id="text-3-7-2-6">
<p>
Fully connected layer (<code>dense_func</code>) is used to connect
<i>feature extraction</i> part (conv, pooling, etc.) to <i>classification</i> part
(\(\operatorname{softmax}\), etc.), so the output can be handled like
in classic neural network.
<code>flat</code> function must be called before <code>dense</code> one. <code>dense</code> function applies
weights-bias and then calls an activation function on it.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">flat</span> [Z]
  (<span class="org-builtin">setv</span> [num_examples n_f n_f n_channel] Z.shape)
  (.reshape Z [num_examples (<span class="org-builtin">*</span> n_f n_f n_channel) 1]))

(<span class="org-keyword">defn</span> <span class="org-function-name">dense_func</span> [Z w b]
  (<span class="org-builtin">+</span> (.dot np (.transpose np w) Z) b))

(<span class="org-keyword">defn</span> <span class="org-function-name">dense</span> [Z w b activation_func]
  (<span class="org-builtin">setv</span> <span class="org-variable-name">num_examples</span> (<span class="org-builtin">get</span> Z.shape 0))
  (.array np (<span class="org-keyword">lfor</span> i (<span class="org-builtin">range</span> num_examples)
                   (activation_func (dense_func (<span class="org-builtin">get</span> Z i) w b)))))
</pre>
</div>


<p>
Initialize filter for conv layer using a random normal distribution.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">init_weight</span> [size]
  (<span class="org-builtin">*</span> (.standard_normal np.random <span class="org-constant">:size</span> size) 0.01))
</pre>
</div>


<p>
Implementation of an arbitrary CNN model.
The model specification is described below.
The dimensions used in the model are:
</p>
<ul class="org-ul">
<li>\(\dim\) Input: \(n_{l-1} \times n_{l-1} \times nc_{l-1}\)
</li>
<li>\(\dim\) Output: \(n_{l} \times n_{l} \times nc_{l}\)
</li>
<li>Where \(n_{l} = \lfloor (n_{l-1} + 2p_{l} - f_{l} / s_{l}) + 1 \rfloor\)
</li>
<li>\(\dim\) of Each filter: \(f_{l} \times f_{l} \times nc_{l-1}\)
</li>
<li>\(\dim\) Weights: \(f_{l} \times f_{l} \times nc_{l-1} \times nc_{l}\),
(it can be viewed like a filter with channel dimension)
</li>
<li>\(\dim\) bias:  \(1 \times 1 \times 1 \times nc_l\)
</li>
</ul>
<p>
For example, let the previous variables be fixed values:
</p>
<ul class="org-ul">
<li>\(n_{l-1} = 28\)
</li>
<li>\(nc_{l-1} = 1\)
</li>
<li>\(p_l = 1\)
</li>
<li>\(s_l = 2\)
</li>
<li>\(f_l = 3\)
</li>
<li>\(nc_l = 8\)
</li>
</ul>
<p>
then \(n_l = 14 = \lfloor ((28 + 2 - 3) / 2) + 1 \rfloor\)
</p>

<p>
The model steps in the following code block are: <br >
<code>conv</code> [image filter bias pad stride] -&gt; Z <br >
<code>relu</code> [Z] -&gt; Z <br >
<code>conv</code> [Z filter bias pad stride] -&gt; Z <br >
<code>relu</code> [Z] -&gt; Z <br >
<code>pool</code> [Z A n<sub>f</sub> stride poolfunc] -&gt; Z <br >
<code>flat</code>[Z] -&gt; Z <br >
<code>dense</code>[Z] -&gt; Z <br >
<code>relu</code> [Z] -&gt; Z <br >
<code>dense</code>[Z] -&gt; Z <br >
<code>softmax</code>[Z] -&gt; Y<sub>hat</sub> <br >
<code>cross_entropy_loss</code>[Y Y<sub>hat</sub>]
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">model_forward_prop</span> [n_f_l1 n_f_l2 n_f_l3 n_channel_l1 n_channel_l2 n_pad
                          n_stride n_neurons_l4 n_neurons_l5 n_output_flat]
  (.seed np.random 1)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">weight_l1</span> (init_weight [n_f_l1 n_f_l1 img_depth n_channel_l1]))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">bias_l1</span> (.zeros np [1 1 1 n_channel_l1]))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">weight_l2</span> (init_weight [n_f_l2 n_f_l2 n_channel_l1 n_channel_l2]))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">bias_l2</span> (.zeros np [1 1 1 n_channel_l2]))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">weight_dense_l4</span> (init_weight [n_output_flat n_neurons_l4]))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">bias_dense_l4</span> (.zeros np [n_neurons_l4 1]))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">weight_dense_l5</span> (init_weight [n_neurons_l4 n_neurons_l5]))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">bias_dense_l5</span> (.zeros np [n_neurons_l5 1]))

  (<span class="org-keyword">-&gt;</span>
    (conv train_set_x_small weight_l1 bias_l1 n_pad n_stride relu)
    (conv weight_l2 bias_l2 n_pad n_stride relu)
    (pool n_f_l3 n_stride np.max)
    (flat)
    (dense weight_dense_l4 bias_dense_l4 relu)
    (dense weight_dense_l5 bias_dense_l5 softmax)))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-prediction</span> []
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_prediction</span>
        (model_forward_prop <span class="org-constant">:n_f_l1</span> 4
                            <span class="org-constant">:n_f_l2</span> 4
                            <span class="org-constant">:n_f_l3</span> 2
                            <span class="org-constant">:n_channel_l1</span> 8
                            <span class="org-constant">:n_channel_l2</span> 10
                            <span class="org-constant">:n_pad</span> 1
                            <span class="org-constant">:n_stride</span> 2
                            <span class="org-constant">:n_neurons_l4</span> 40
                            <span class="org-constant">:n_neurons_l5</span> 10
                            <span class="org-constant">:n_output_flat</span> 90))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_cost</span>
        (cross_entropy_loss train_set_y_small (.squeeze np test_prediction)))

  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (.sum test_prediction) 60.0))
  (<span class="org-keyword">assert</span> (np.isclose test_cost 138.1544723)))
(test-case (test-prediction))
</pre>
</div>


<p>
The following code block implements the LeNet-5 model,
and below a description of how shapes look like at each layer:
</p>

<figure id="fig:lenet5_model.png">
<p><img src="./export/lenet5/lenet5_model.png" class="img-responsive" alt="lenet5_model.png" width="700">
</p>
<figcaption><span class="figure-number">Figure 33:</span> The LeNet-5 model used</figcaption>
</figure>

<p>
<b>#0</b> Input <br >
Input image: a0 = 28x28x1 <br >
n0 = 28, nc0 = 1 <br >
<b>#1</b> Conv layer <br >
<b>#1.1</b> Conv (filter): <br >
f1 = 5, s1 = 1, p1 = 0 <br >
Number of filters = 6 <br >
Output a1 = 24x24x6 <br >
n1 = 24, nc1 = 6 <br >
<b>#1.2</b> Max-pooling (sampling): <br >
f1p = 2, s1p = 2 <br >
Output a1 = 12x12x6 <br >
<b>#2</b> Conv layer <br >
<b>#2.1</b> Conv (filter): <br >
f2 = 5, s2 = 1, p2 = 0 <br >
Number of filters = 16 <br >
Output a2 = 8x8x16 <br >
n2 = 8, nc2 = 16 <br >
<b>#2.2</b> Max-pooling (sampling): <br >
f2p = 2, s2p = 2 <br >
Output a2 = 4x4x16 <br >
<b>#3</b> Fully connected layer <br >
120 neurons (arbitrary) <br >
Output a3 = 120 x 1 <br >
<b>#4</b> Fully connected layer <br >
84 neurons <br >
Output a4 = 84 x 1 <br >
<b>#5</b> Softmax layer <br >
Number of neurons is \(N\) if application needs to identify \(N\) digits
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">model_lenet5</span> []
  (.seed np.random 1)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">n_channel_l0</span> 1)

  (<span class="org-builtin">setv</span> <span class="org-variable-name">n_f_l1_conv</span> 5)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">stride_l1_conv</span> 1)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">pad_l1_conv</span> 0)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">n_channel_l1_conv</span> 6)

  (<span class="org-builtin">setv</span> <span class="org-variable-name">n_f_l1_pool</span> 2)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">stride_l1_pool</span> 2)

  (<span class="org-builtin">setv</span> <span class="org-variable-name">n_f_l2_conv</span> 5)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">stride_l2_conv</span> 1)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">pad_l2_conv</span> 0)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">n_channel_l2_conv</span> 16)

  (<span class="org-builtin">setv</span> <span class="org-variable-name">n_f_l2_pool</span> 2)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">stride_l2_pool</span> 2)

  (<span class="org-builtin">setv</span>  <span class="org-variable-name">n_output_flat</span> 256)

  (<span class="org-builtin">setv</span> <span class="org-variable-name">n_neurons_l3</span> 120)

  (<span class="org-builtin">setv</span> <span class="org-variable-name">n_neurons_l4</span> 84)

  (<span class="org-builtin">setv</span> <span class="org-variable-name">n_neurons_l5</span> 10)

  (<span class="org-builtin">setv</span> <span class="org-variable-name">weight_l1</span> (init_weight [n_f_l1_conv
                                n_f_l1_conv
                                n_channel_l0
                                n_channel_l1_conv]))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">bias_l1</span> (.zeros np [1 1 1 n_channel_l1_conv]))

  (<span class="org-builtin">setv</span> <span class="org-variable-name">weight_l2</span> (init_weight [n_f_l2_conv
                                n_f_l2_conv
                                n_channel_l1_conv
                                n_channel_l2_conv]))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">bias_l2</span> (.zeros np [1 1 1 n_channel_l2_conv]))

  (<span class="org-builtin">setv</span> <span class="org-variable-name">weight_l3</span> (init_weight [n_output_flat n_neurons_l3]))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">bias_l3</span> (.zeros np [n_neurons_l3 1]))

  (<span class="org-builtin">setv</span> <span class="org-variable-name">weight_l4</span> (init_weight [n_neurons_l3 n_neurons_l4]))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">bias_l4</span> (.zeros np [n_neurons_l4 1]))

  (<span class="org-builtin">setv</span> <span class="org-variable-name">weight_l5</span> (init_weight [n_neurons_l4 n_neurons_l5]))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">bias_l5</span> (.zeros np [n_neurons_l5 1]))

  (<span class="org-keyword">-&gt;</span>
    (conv train_set_x_small weight_l1 bias_l1 pad_l1_conv stride_l1_conv
          (<span class="org-keyword">fn</span> [x] x))
    (pool n_f_l1_pool stride_l1_pool np.max)
    (conv weight_l2 bias_l2 pad_l2_conv stride_l2_conv (<span class="org-keyword">fn</span> [x] x))
    (pool n_f_l2_pool stride_l2_pool np.max)
    (flat)
    (dense weight_l3 bias_l3 relu)
    (dense weight_l4 bias_l4 relu)
    (dense weight_l5 bias_l5 softmax)))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-model</span> []
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_prediction</span> (model_lenet5))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_cost</span> (cross_entropy_loss
                    train_set_y_small
                    (.squeeze np test_prediction)))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (.sum test_prediction) 60.0))
  (<span class="org-keyword">assert</span> (np.isclose test_cost 138.1544723)))
(test-case (test-model))
</pre>
</div>
</div>
</li></ol>
</div>
<div id="outline-container-sec-3-7-3" class="outline-4">
<h4 id="sec-3-7-3"><span class="section-number-4">3.7.3</span> Results</h4>
<div class="outline-text-4" id="text-3-7-3">
<p>
Run the Lenet-5 model and print both prediction and cost.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">print</span> <span class="org-string">"Model running..."</span>)
(<span class="org-builtin">setv</span> <span class="org-variable-name">prediction</span> (model_lenet5))
(<span class="org-keyword">print</span> <span class="org-string">"prediction shape: "</span> prediction.shape)
(<span class="org-keyword">print</span> <span class="org-string">"prediction for second training example:"</span>)
(<span class="org-keyword">print</span> (<span class="org-builtin">get</span> prediction 1))
(<span class="org-builtin">setv</span> <span class="org-variable-name">cost</span> (cross_entropy_loss train_set_y_small (.squeeze np prediction)))
(<span class="org-keyword">print</span> <span class="org-string">"cost: "</span> cost)
</pre>
</div>


<p>
The model contains 60 examples of handwritten digit
with for each 10 probabilities associated (one for each digit).
This is shown by the prediction shape (60, 10).
Also, the sum of probabilities for each example must be 1,
as shown by the prediction of second training example.
These sanity checks valid that the model for the first step is correct.
The cost here is high because the model predictions are all wrong.
It's normal for the first step, and the cost will be reduced during
the second step after apply backward propagation and
gradient calculation (not implemented in this project).
</p>
<pre class="example">
Model running...
prediction shape:  (60, 10, 1)
prediction for second training example:
[[0.09999991]
 [0.09999946]
 [0.10000007]
 [0.10000016]
 [0.10000036]
 [0.09999985]
 [0.1       ]
 [0.10000005]
 [0.09999992]
 [0.10000021]]
cost:  138.15511943114262
</pre>
</div>
</div>
<div id="outline-container-sec-3-7-4" class="outline-4">
<h4 id="sec-3-7-4"><span class="section-number-4">3.7.4</span> References</h4>
<div class="outline-text-4" id="text-3-7-4">
<ul class="org-ul">
<li>deeplearning.ai. 2020. <a href="https://www.deeplearning.ai/deep-learning-specialization/">Deep Learning Specialization</a>.
</li>
<li>LeCun Yann, et al. 1998. <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf">Gradient-Based Learning Applied to Document Recognition</a>.
</li>
<li>LeCun Yann. 2017. <a href="http://yann.lecun.com/exdb/mnist/">The Mnist Database</a>.
</li>
<li>Escontrela Alejandro. 2018. <a href="https://towardsdatascience.com/convolutional-neural-networks-from-the-ground-up-c67bb41454e1">Convolutional Neural Networks from the ground up</a>.
</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-sec-3-8" class="outline-3">
<h3 id="sec-3-8"><span class="section-number-3">3.8</span> MNIST Handwritten Digit Classification Using Keras</h3>
<div class="outline-text-3" id="text-3-8">

<figure>
<p><img src="./export/img/mnist.png" class="img-responsive" alt="mnist.png" width="480">
</p>
</figure>
</div>
<div id="outline-container-sec-3-8-1" class="outline-4">
<h4 id="sec-3-8-1"><span class="section-number-4">3.8.1</span> Purpose</h4>
<div class="outline-text-4" id="text-3-8-1">
<p>
The objective is to build a simple Convolutional Neural Network (CNN) using
the MNIST dataset, and Keras framework.
</p>
</div>
</div>
<div id="outline-container-sec-3-8-2" class="outline-4">
<h4 id="sec-3-8-2"><span class="section-number-4">3.8.2</span> Implementation</h4>
<div class="outline-text-4" id="text-3-8-2">
</div><ol class="org-ol"><li><a id="sec-3-8-2-1" name="sec-3-8-2-1"></a>Initialization<br ><div class="outline-text-5" id="text-3-8-2-1">
<p>
Import necessaries modules: different types of layers to build the model
(from Keras framework), MNIST dataset, and Numpy.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">require</span> [hytmf [test-case]])
(<span class="org-keyword">import</span> [numpy <span class="org-keyword">:as</span> np])
(<span class="org-keyword">import</span> keras)
(<span class="org-keyword">import</span> [keras.datasets
          [mnist]])
 (<span class="org-keyword">import</span> [keras.models
          [Model load_model]])
 (<span class="org-keyword">import</span> [keras.layers
          [Input Dense Dropout Flatten Conv2D MaxPooling2D]])
</pre>
</div>


<p>
The code block below is described at <a href="#src-import-dataset-mnist">src-import-dataset-mnist</a>.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> [[train_set_x_orig train_set_y_orig] [test_set_x_orig test_set_y_orig]]
      (.load_data mnist))
</pre>
</div>


<p>
The code block below is described at <a href="#src-set-params">src-set-params</a>.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> <span class="org-variable-name">num_classes</span> 10)
(<span class="org-builtin">setv</span> <span class="org-variable-name">num_examples</span> 60000)
(<span class="org-builtin">setv</span> <span class="org-variable-name">num_examples_small</span> 60)
(<span class="org-builtin">setv</span> <span class="org-variable-name">img_rows</span> 28)
(<span class="org-builtin">setv</span> <span class="org-variable-name">img_cols</span> 28)
(<span class="org-builtin">setv</span> <span class="org-variable-name">img_depth</span> 1)
</pre>
</div>


<p>
The code block below is described at <a href="#src-flatten-dataset-mnist">src-flatten-dataset-mnist</a>.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> <span class="org-variable-name">train_set_x_flatten</span> (.reshape train_set_x_orig
                                    (<span class="org-builtin">get</span> train_set_x_orig.shape 0)
                                    img_rows
                                    img_cols
                                    img_depth))
(<span class="org-builtin">setv</span> <span class="org-variable-name">test_set_x_flatten</span> (.reshape test_set_x_orig
                                   (<span class="org-builtin">get</span> test_set_x_orig.shape 0)
                                   img_rows
                                   img_cols
                                   img_depth))
</pre>
</div>


<p>
The code block below is described at <a href="#src-normalize-dataset-mnist">src-normalize-dataset-mnist</a>.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> <span class="org-variable-name">train_set_x</span> (<span class="org-builtin">/</span> train_set_x_flatten 255))
(<span class="org-builtin">setv</span> <span class="org-variable-name">test_set_x</span> (<span class="org-builtin">/</span> test_set_x_flatten 255))
(<span class="org-builtin">setv</span> <span class="org-variable-name">train_set_y</span> (.to_categorical keras.utils train_set_y_orig num_classes))
(<span class="org-builtin">setv</span> <span class="org-variable-name">test_set_y</span> (.to_categorical keras.utils test_set_y_orig num_classes))
</pre>
</div>
</div>
</li>
<li><a id="sec-3-8-2-2" name="sec-3-8-2-2"></a>Model<br ><div class="outline-text-5" id="text-3-8-2-2">
<p>
Build the model with different types of layers:
</p>
<ul class="org-ul">
<li>Conv2D filters some specific features of the image
</li>
<li>Dropout is used to prevent overfitting
</li>
<li>MaxPooling reduces the dimension of each feature
</li>
<li>Flatten and Dense are used to create a fully connected network
</li>
</ul>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> <span class="org-variable-name">visible</span> (Input <span class="org-constant">:shape</span> [img_rows img_cols img_depth]))

(<span class="org-builtin">setv</span> <span class="org-variable-name">output</span> (<span class="org-keyword">-&gt;</span>
               visible
               ((Conv2D
                  <span class="org-constant">:filters</span> 32
                  <span class="org-constant">:kernel_size</span> [3 3]
                  <span class="org-constant">:activation</span> <span class="org-string">"relu"</span>))
               ((Conv2D
                  <span class="org-constant">:filters</span> 64
                  <span class="org-constant">:kernel_size</span> [3 3]
                  <span class="org-constant">:activation</span> <span class="org-string">"relu"</span>))
               ((MaxPooling2D
                  <span class="org-constant">:pool_size</span> [2 2]))
               ((Dropout 0.25))
               ((Flatten))
               ((Dense
                  <span class="org-constant">:units</span> 128
                  <span class="org-constant">:activation</span> <span class="org-string">"relu"</span>))
               ((Dropout 0.5))
               ((Dense
                  <span class="org-constant">:units</span> num_classes
                  <span class="org-constant">:activation</span> <span class="org-string">"softmax"</span>))))
(<span class="org-builtin">setv</span> <span class="org-variable-name">model</span> (Model <span class="org-constant">:inputs</span> visible <span class="org-constant">:outputs</span> output))
</pre>
</div>


<p>
Model summary.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">print</span> (.summary model))
</pre>
</div>


<pre class="example">
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         [(None, 28, 28, 1)]       0
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 26, 26, 32)        320
_________________________________________________________________
conv2d (Conv2D)              (None, 24, 24, 64)        18496
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 12, 12, 64)        0
_________________________________________________________________
dropout_1 (Dropout)          (None, 12, 12, 64)        0
_________________________________________________________________
flatten (Flatten)            (None, 9216)              0
_________________________________________________________________
dense_1 (Dense)              (None, 128)               1179776
_________________________________________________________________
dropout (Dropout)            (None, 128)               0
_________________________________________________________________
dense (Dense)                (None, 10)                1290
=================================================================
Total params: 1,199,882
Trainable params: 1,199,882
Non-trainable params: 0
_________________________________________________________________
</pre>


<p>
Compile and train the model with specific parameters.
</p>
<div class="org-src-container">

<pre class="src src-hy">(.compile model
          <span class="org-constant">:loss</span> <span class="org-string">"categorical_crossentropy"</span>
          <span class="org-constant">:optimizer</span> (.Adadelta keras.optimizers)
          <span class="org-constant">:metrics</span> [<span class="org-string">"accuracy"</span>])

(.fit model
      <span class="org-constant">:x</span> train_set_x
      <span class="org-constant">:y</span> train_set_y
      <span class="org-constant">:validation_data</span> [test_set_x test_set_y]
      <span class="org-constant">:epochs</span> 5
      <span class="org-constant">:batch_size</span> 32)
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-model</span> []
  (<span class="org-keyword">assert</span> (np.isclose
            (<span class="org-builtin">*</span> (<span class="org-builtin">get</span> (.evaluate model
                               <span class="org-constant">:x</span> test_set_x
                               <span class="org-constant">:y</span> test_set_y
                               <span class="org-constant">:verbose</span> 1) 1)
               100) 84.1 <span class="org-constant">:rtol</span> 0.1)))
(test-case (test-model))
</pre>
</div>
</div>
</li></ol>
</div>
<div id="outline-container-sec-3-8-3" class="outline-4">
<h4 id="sec-3-8-3"><span class="section-number-4">3.8.3</span> Results</h4>
<div class="outline-text-4" id="text-3-8-3">
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> <span class="org-variable-name">scores</span> (.evaluate model <span class="org-constant">:x</span> test_set_x <span class="org-constant">:y</span> test_set_y <span class="org-constant">:verbose</span> 1))
(<span class="org-keyword">print</span> <span class="org-string">"Loss:"</span> (<span class="org-builtin">*</span> (<span class="org-builtin">get</span> scores 0) 100))
(<span class="org-keyword">print</span> <span class="org-string">"Accuracy:"</span> (<span class="org-builtin">*</span> (<span class="org-builtin">get</span> scores 1) 100))
</pre>
</div>


<pre class="example">
Loss: 78.94231677055359
Accuracy: 82.59000182151794
</pre>
</div>
</div>
<div id="outline-container-sec-3-8-4" class="outline-4">
<h4 id="sec-3-8-4"><span class="section-number-4">3.8.4</span> References</h4>
<div class="outline-text-4" id="text-3-8-4">
<ul class="org-ul">
<li>Keras Team. 2020. <a href="https://keras.io/examples/mnist_cnn/">MNIST digits classification dataset</a>.
</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-sec-3-9" class="outline-3">
<h3 id="sec-3-9"><span class="section-number-3">3.9</span> Neural Style Transfer Using TensorFlow</h3>
<div class="outline-text-3" id="text-3-9">

<figure>
<p><img src="./export/img/nst.png" class="img-responsive" alt="nst.png" width="480">
</p>
</figure>
</div>
<div id="outline-container-sec-3-9-1" class="outline-4">
<h4 id="sec-3-9-1"><span class="section-number-4">3.9.1</span> Purpose</h4>
<div class="outline-text-4" id="text-3-9-1">
<p>
Neural Style Transfer (NST) is about Deep Neural Network that creates artistic images.
Neural Style Transfer is an algorithm which combines two images
-a content image (denoted \(C\)) and a style image (denoted \(S\))-
to generate a new content image with the style of the style image
(denoted \(G\)), thus \(G = C + S\).
Neural Style Transfer is here implemented with TensorFlow framework.
</p>
</div>
</div>
<div id="outline-container-sec-3-9-2" class="outline-4">
<h4 id="sec-3-9-2"><span class="section-number-4">3.9.2</span> Implementation</h4>
<div class="outline-text-4" id="text-3-9-2">
</div><ol class="org-ol"><li><a id="sec-3-9-2-1" name="sec-3-9-2-1"></a>Initialization<br ><div class="outline-text-5" id="text-3-9-2-1">
<p>
Import and configure modules.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">require</span> [hytmf [test-case]])
(<span class="org-keyword">import</span> [tensorflow <span class="org-keyword">:as</span> tf])
(<span class="org-keyword">import</span> [matplotlib.pyplot <span class="org-keyword">:as</span> plt])
(<span class="org-keyword">import</span> [numpy <span class="org-keyword">:as</span> np])
(<span class="org-keyword">import</span> PIL.Image)
(<span class="org-keyword">import</span> functools)
</pre>
</div>


<p>
Download both style image and content image.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> <span class="org-variable-name">content_path</span> (tf.keras.utils.get_file
                     <span class="org-string">"louvre_museum.jpg"</span>
                     (<span class="org-builtin">+</span> <span class="org-string">"https://upload.wikimedia.org/wikipedia/commons/"</span>
                        <span class="org-string">"thumb/0/0e/Vista_exterior_del_Museo_del_Louvre.JPG/"</span>
                        <span class="org-string">"640px-Vista_exterior_del_Museo_del_Louvre.JPG"</span>)))
(<span class="org-builtin">setv</span> <span class="org-variable-name">style_path</span> (tf.keras.utils.get_file
                   <span class="org-string">"monet_poppies.jpg"</span>
                   (<span class="org-builtin">+</span> <span class="org-string">"https://upload.wikimedia.org/wikipedia/commons/"</span>
                      <span class="org-string">"thumb/2/22/Monet_Poppies.jpg"</span>
                      <span class="org-string">"/629px-Monet_Poppies.jpg"</span>)))
</pre>
</div>


<p>
Load an image, and scale it to 512 pixels maximum.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">load_img</span> [path_to_img]
  (<span class="org-builtin">setv</span> <span class="org-variable-name">img</span> (<span class="org-keyword">-&gt;</span> path_to_img
                (tf.io.read_file)
                (tf.image.decode_image <span class="org-constant">:channels</span> 3)
                (tf.image.convert_image_dtype tf.float32)))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">shape</span>
        (tf.cast (<span class="org-builtin">get</span> (tf.shape img) (<span class="org-builtin">slice</span> <span class="org-constant">None</span> -1)) tf.float32))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">scale</span> (<span class="org-builtin">/</span> 512 (<span class="org-builtin">max</span> shape)))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">new_shape</span> (tf.cast (<span class="org-builtin">*</span> shape scale) tf.int32))
  (<span class="org-builtin">get</span> (tf.image.resize img new_shape) tf.newaxis (<span class="org-builtin">slice</span> <span class="org-constant">None</span>)))
</pre>
</div>


<p>
Set both content and style images with ones previously downloaded.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> <span class="org-variable-name">content_image</span> (load_img content_path))
(<span class="org-builtin">setv</span> <span class="org-variable-name">style_image</span> (load_img style_path))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-img_raw</span> []
  (<span class="org-keyword">assert</span> (np.isclose (.mean (content_image.numpy)) 0.5256987))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (. (content_image.numpy) shape) (<span class="org-builtin">,</span> 1 384 512 3)))
  (<span class="org-keyword">assert</span> (np.isclose (.mean (style_image.numpy)) 0.55430293))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (. (style_image.numpy) shape) (<span class="org-builtin">,</span> 1 390 512 3))))
(test-case (test-img_raw))
</pre>
</div>


<p>
Convert an object TensorFlow to a Python Imaging Library array.
It will be used for visualization.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">tensor_to_image</span> [tensor]
  (<span class="org-keyword">as-&gt;</span> tensor t
        (<span class="org-builtin">*</span> t 255)
        (np.array t <span class="org-constant">:dtype</span> np.uint8)
        (<span class="org-keyword">if</span> (<span class="org-builtin">&gt;</span> (np.ndim t) 3) (<span class="org-builtin">get</span> t 0) t)
        (PIL.Image.fromarray t)))

(<span class="org-keyword">defn</span> <span class="org-function-name">show_img</span> [img title]
  (.figure plt title)
  (.subplot plt 1 2 1)
  (.imshow plt img)
  (.savefig plt title)
  (.close plt))
</pre>
</div>


<p>
Show both content and style images.
</p>
<div class="org-src-container">

<pre class="src src-hy">(show_img (<span class="org-builtin">get</span> content_image 0) <span class="org-string">"nst_content_image"</span>)
(show_img (<span class="org-builtin">get</span> style_image 0) <span class="org-string">"nst_style_image"</span>)
</pre>
</div>

<figure>
<p><img src="./export/nst/nst_content_image.png" class="img-responsive" alt="nst_content_image.png" width="700">
</p>
<figcaption><span class="figure-number">Figure 36:</span> The content image</figcaption>
</figure>

<figure>
<p><img src="./export/nst/nst_style_image.png" class="img-responsive" alt="nst_style_image.png" width="700">
</p>
<figcaption><span class="figure-number">Figure 37:</span> The style image</figcaption>
</figure>
</div>
</li>
<li><a id="sec-3-9-2-2" name="sec-3-9-2-2"></a>Fast Arbitrary Image Style Transfer<br ><div class="outline-text-5" id="text-3-9-2-2">
<p>
The Fast Arbitrary Image Style Transfer module is used to show
the Neural Style Transfer result.
This module is downloaded from TensorFlow Hub, a library for reusable machine
learning modules. The following code block shows the expected result,
and it will be not used in the rest of the implementation.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">import</span> [tensorflow_hub <span class="org-keyword">:as</span> hub])
(<span class="org-builtin">setv</span> <span class="org-variable-name">hub_module</span> (hub.load
                   (<span class="org-builtin">+</span> <span class="org-string">"https://tfhub.dev/google/magenta/"</span>
                      <span class="org-string">"arbitrary-image-stylization-v1-256/2"</span>)))
(<span class="org-builtin">setv</span> <span class="org-variable-name">hub_image_final</span> (<span class="org-builtin">get</span> (hub_module
                             (tf.constant content_image)
                             (tf.constant style_image)) 0))

(show_img (tensor_to_image hub_image_final) <span class="org-string">"nst_hub_image_final"</span>)
</pre>
</div>

<figure>
<p><img src="./export/nst/nst_hub_image_final.png" class="img-responsive" alt="nst_hub_image_final.png" width="700">
</p>
<figcaption><span class="figure-number">Figure 38:</span> The expected image</figcaption>
</figure>
</div>
</li>
<li><a id="sec-3-9-2-3" name="sec-3-9-2-3"></a>VGG19 Model Extraction<br ><div class="outline-text-5" id="text-3-9-2-3">
<p>
Load a VGG19 model on content image.
VGG19 is a Convolutional Neural Network of 19 layers.
This model has been trained on the ImageNet database,
and has learned a variety of
low level features as edges and textures, and high level features as object
parts (wheels or eyes). The first layers represent the low level features,
and the last layers represent the high level features.
The goal is to use some of these layers to build
a Neural Style Transfer application.
The technique of using a network trained on a different task and applying it
to a new one is called transfer learning.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> <span class="org-variable-name">prediction_probabilities</span>
      (<span class="org-keyword">-&gt;</span>
        content_image
        (<span class="org-builtin">*</span> 255)
        (tf.keras.applications.vgg19.preprocess_input)
        (tf.image.resize [224 224])
        ((tf.keras.applications.VGG19
           <span class="org-constant">:include_top</span> <span class="org-constant">True</span>
           <span class="org-constant">:weights</span> <span class="org-string">"imagenet"</span>))))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-prediction_probabilities</span> []
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">get</span> prediction_probabilities.shape 0) 1))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">get</span> prediction_probabilities.shape 1) 1000)))
(test-case (test-prediction_probabilities))
</pre>
</div>


<p>
Print the best matching layers of the content image.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> <span class="org-variable-name">predicted_top_5</span> (<span class="org-builtin">get</span> (tf.keras.applications.vgg19.decode_predictions
                             (prediction_probabilities.numpy)) 0))
(<span class="org-keyword">lfor</span> [number class_name prob] predicted_top_5 [class_name prob])
</pre>
</div>


<p>
The input image is the Louvre Museum,
thus the model has detected a <i>triumphal arch</i>, <i>monastery</i>, etc.
</p>
<pre class="example">
[["palace" 0.90893257]
 ["triumphal_arch" 0.02393725]
 ["obelisk" 0.015971914]
 ["pedestal" 0.008485573]
 ["monastery" 0.007763176]]
</pre>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-predicted_top</span> []
  (<span class="org-keyword">assert</span> (np.isclose (<span class="org-builtin">get</span> predicted_top_5 2 2) 0.015971914)))
(test-case (test-predicted_top))
</pre>
</div>


<p>
List all the layers names.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> <span class="org-variable-name">vgg</span> (tf.keras.applications.VGG19
            <span class="org-constant">:include_top</span> <span class="org-constant">False</span>
            <span class="org-constant">:weights</span> <span class="org-string">"imagenet"</span>))

(<span class="org-keyword">for</span> [layer vgg.layers]
  (<span class="org-keyword">print</span> layer.name))
</pre>
</div>


<pre class="example">
input_2
block1_conv1
block1_conv2
block1_pool
block2_conv1
block2_conv2
block2_pool
block3_conv1
block3_conv2
block3_conv3
block3_conv4
block3_pool
block4_conv1
block4_conv2
block4_conv3
block4_conv4
block4_pool
block5_conv1
block5_conv2
block5_conv3
block5_conv4
block5_pool
</pre>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-vgg_layers</span> []
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">len</span> vgg.layers) 22))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (. (<span class="org-builtin">get</span> vgg.layers 3) <span class="org-builtin">name</span>) <span class="org-string">"block1_pool"</span>)))
(test-case (test-vgg_layers))
</pre>
</div>


<p>
Choose arbitrary intermediate layers from
the network to represent the style and the content of the image.
The VGG19 model serves as a feature extractor: the content and style of
input images are described in the intermediate layers of the model.
The <code>block5_conv2</code> represents the information about the content of the image,
while <code>block1_conv1</code>, <code>block2_conv1</code>, <code>block3_conv1</code>, <code>block4_conv1</code>
and <code>block5_conv1</code>, represent the information about the style of the image.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> <span class="org-variable-name">content_layers</span> [<span class="org-string">"block5_conv2"</span>])
(<span class="org-builtin">setv</span> <span class="org-variable-name">style_layers</span> [<span class="org-string">"block1_conv1"</span>
                    <span class="org-string">"block2_conv1"</span>
                    <span class="org-string">"block3_conv1"</span>
                    <span class="org-string">"block4_conv1"</span>
                    <span class="org-string">"block5_conv1"</span>])
(<span class="org-builtin">setv</span> <span class="org-variable-name">num_content_layers</span> (<span class="org-builtin">len</span> content_layers))
(<span class="org-builtin">setv</span> <span class="org-variable-name">num_style_layers</span> (<span class="org-builtin">len</span> style_layers))
</pre>
</div>


<p>
Build a VGG19 model that returns a list of intermediate layers.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">vgg_layers</span> [layer_names]
  (<span class="org-builtin">setv</span> <span class="org-variable-name">vgg</span> (tf.keras.applications.VGG19
              <span class="org-constant">:include_top</span> <span class="org-constant">False</span>
              <span class="org-constant">:weights</span> <span class="org-string">"imagenet"</span>))
  <span class="org-comment-delimiter">;; </span><span class="org-comment">HACK: because VGG19 function has not trainable args!!!</span>
  (<span class="org-builtin">setv</span> <span class="org-variable-name">vgg.trainable</span> <span class="org-constant">False</span>)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">outputs</span> (<span class="org-keyword">lfor</span> l_name layer_names (. (vgg.get_layer l_name) output)))
  (tf.keras.Model [vgg.input] outputs))
</pre>
</div>


<p>
Print the intermediate layers for the style image.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> <span class="org-variable-name">style_extractor</span> (vgg_layers style_layers))
(<span class="org-builtin">setv</span> <span class="org-variable-name">style_outputs</span> (style_extractor (<span class="org-builtin">*</span> style_image 255)))

(<span class="org-keyword">for</span> [[s_name output] (<span class="org-builtin">zip</span> style_layers style_outputs)]
  (<span class="org-builtin">setv</span> <span class="org-variable-name">out_numpy</span> (output.numpy))
  (<span class="org-keyword">print</span> s_name)
  (<span class="org-keyword">print</span> <span class="org-string">"  shape: "</span> out_numpy.shape)
  (<span class="org-keyword">print</span> <span class="org-string">"  min: "</span> (out_numpy.min))
  (<span class="org-keyword">print</span> <span class="org-string">"  max: "</span> (out_numpy.max))
  (<span class="org-keyword">print</span> <span class="org-string">"  mean: "</span> (out_numpy.mean))
  (<span class="org-keyword">print</span>))
</pre>
</div>


<pre class="example">
block1_conv1
  shape:  (1, 390, 512, 64)
  min:  0.0
  max:  827.73413
  mean:  30.123056

block2_conv1
  shape:  (1, 195, 256, 128)
  min:  0.0
  max:  3248.6877
  mean:  150.94392

block3_conv1
  shape:  (1, 97, 128, 256)
  min:  0.0
  max:  8521.9375
  mean:  123.5119

block4_conv1
  shape:  (1, 48, 64, 512)
  min:  0.0
  max:  15103.936
  mean:  505.6873

block5_conv1
  shape:  (1, 24, 32, 512)
  min:  0.0
  max:  2060.6445
  mean:  32.697186
</pre>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-style_outputs</span> []
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">len</span> style_outputs) 5))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">sum</span> (. (<span class="org-builtin">get</span> style_outputs 0) shape)) 967)))
(test-case (test-style_outputs))
</pre>
</div>
</div>
</li>
<li><a id="sec-3-9-2-4" name="sec-3-9-2-4"></a>Gram Matrix<br ><div class="outline-text-5" id="text-3-9-2-4">
<p>
Since
<i>the reason why Gram matrix can represent artistic style still remains a mystery</i>
(Li Yanghao, et all. 2017. <a href="https://arxiv.org/pdf/1701.01036.pdf">Demystifying Neural Style Transfer</a>),
explanations given below are minimalist.
In the paper: Gatys Leon, et al. 2015. <a href="https://arxiv.org/pdf/1508.06576.pdf">A Neural Algorithm of Artistic Style</a>,
authors illustrate what the content image looks like from lower layers
to higher layers of the network.
In lower layers the image is perfect,
while the higher layers memorize only the high-level content of the image.
In case of the style image,
the computation of the correlations between the different features in
different layers, creates an image, that
matches the style of a given image while discarding information
of the global arrangement of the scene.
In other words only the style of the image is preserved.
This computation is made using the Gram matrix.
</p>

<p>
The Gram matrix is defined as following:
\(G_{ij}=\langle v_i, v_j \rangle\), where
\(v_i\), \(v_j\) are vector, and \(\langle .,. \rangle\) is the inner product
(dot product in the Euclidean spaces).
The dot product compares how similar vectors \(v_i\), \(v_j\) are:
if they are highly similar, then the result would be large.
The Gram matrix is calculated with the following formula:
</p>
\begin{align*}
G_{(gram)cd}^{[l](I)} & = K \overbrace{\sum
\limits_{i=1}^{n_{H}^{[l]}}\sum\limits_{j=1}^{n_{W}^{[l]}}a_{ijc}^{[l](I)}
\times a_{ijd}^{[l](I)}}^{dot\ product} \\\\
G_{(gram)}^{(I)} & = \sum_{l} G_{(gram)}^{[l](I)} \\\
\dim G_{(gram)}^{[l]} & =  n_{C}^{[l]} \times n_{C}^{[l]}
\end{align*}
<p>
\(a^{[l]}\) is the hidden layer activations \(l\) representing the style
of the style image.
\(n_{H}\), \(n_{W}\) and \(n_{C}\) are the height,
width and number of filters (channels), respectively,
with \((c,d)\in[1,n_{C}^{[l]}]\).
\(K\) is a normalization constant,
its value doesn't really matter since adjusted during model training,
and here \(K = 1 / IJ\).
\((I)\) can be either the style image (\(S\)) or the generated image (\(G\)).
The figure <a href="#fig:style_img_matrix.png">39</a>
represents (graphically) the Gram matrix equation.
</p>

<figure id="fig:style_img_matrix.png">
<p><img src="./export/nst/style_img_matrix.png" class="img-responsive" alt="style_img_matrix.png" width="480">
</p>
<figcaption><span class="figure-number">Figure 39:</span> The style image representation</figcaption>
</figure>

<p>
Let \(c = 1\), and \(d = 2\), then the entry \(_{12}\) in the Gram matrix is
calculated as following (as shown in figure <a href="#fig:gram_matrix.png">40</a>):
</p>
\begin{align*}
G_{(gram)12} & = 6 \times 1
+ 7 \times 2
+ 8 \times 3
+ 0 \times 4
+ 5 \times 0 \\
& + 6 \times 1
+ 7 \times 2
+ 8 \times 3
+ 4 \times 8
+ 5 \times 0 \\
& + 6 \times 1
+ 7 \times 2
+ 3 \times 7
+ 4 \times 8
+ 5 \times 0
+ 6 \times 1 \\
& = 199
\end{align*}

<figure id="fig:gram_matrix.png">
<p><img src="./export/nst/gram_matrix.png" class="img-responsive" alt="gram_matrix.png" width="240">
</p>
<figcaption><span class="figure-number">Figure 40:</span> Gram matrix calculation</figcaption>
</figure>

<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">gram_matrix</span> [input_tensor]
  (<span class="org-builtin">setv</span> <span class="org-variable-name">numerator</span> (tf.linalg.einsum
                    <span class="org-string">"bijc,bijd-&gt;bcd"</span>
                    input_tensor
                    input_tensor))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">input_shape</span> (tf.shape input_tensor))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">denominator</span> (tf.cast (<span class="org-builtin">*</span> (<span class="org-builtin">get</span> input_shape 1) (<span class="org-builtin">get</span> input_shape 2))
                             tf.float32))
  (<span class="org-builtin">/</span> numerator denominator))
</pre>
</div>
</div>
</li>
<li><a id="sec-3-9-2-5" name="sec-3-9-2-5"></a>Model<br ><div class="outline-text-5" id="text-3-9-2-5">
<p>
Build a model that returns the style and the content of an image.
It uses the VGG19 model with intermediate layers selected previously, and apply
the Gram matrix on the style layers.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defclass</span> <span class="org-type">StyleContentModel</span> [tf.keras.models.Model]
  (<span class="org-keyword">defn</span> <span class="org-function-name">__init__</span> [<span class="org-keyword">self</span> style_layers content_layers]
    (.__init__ (<span class="org-builtin">super</span> StyleContentModel <span class="org-keyword">self</span>))
    (<span class="org-builtin">setv</span> <span class="org-keyword">self</span>.vgg (vgg_layers (<span class="org-builtin">+</span> style_layers content_layers)))
    (<span class="org-builtin">setv</span> <span class="org-keyword">self</span>.style_layers style_layers)
    (<span class="org-builtin">setv</span> <span class="org-keyword">self</span>.content_layers content_layers)
    (<span class="org-builtin">setv</span> <span class="org-keyword">self</span>.num_style_layers (<span class="org-builtin">len</span> style_layers))
    (<span class="org-builtin">setv</span> <span class="org-keyword">self</span>.vgg.trainable <span class="org-constant">False</span>))

  (<span class="org-keyword">defn</span> <span class="org-function-name">call</span> [<span class="org-keyword">self</span> image]
    (<span class="org-builtin">setv</span> <span class="org-variable-name">outputs</span>
          (<span class="org-keyword">-&gt;</span> image
              (<span class="org-builtin">*</span> 255.0)
              (tf.keras.applications.vgg19.preprocess_input)
              (<span class="org-keyword">self</span>.vgg)))

    (<span class="org-builtin">setv</span> <span class="org-variable-name">style_out_raw</span> (<span class="org-builtin">get</span> outputs (<span class="org-builtin">slice</span> <span class="org-constant">None</span> <span class="org-keyword">self</span>.num_style_layers)))
    (<span class="org-builtin">setv</span> <span class="org-variable-name">content_out_raw</span> (<span class="org-builtin">get</span> outputs (<span class="org-builtin">slice</span> <span class="org-keyword">self</span>.num_style_layers <span class="org-constant">None</span>)))
    (<span class="org-builtin">setv</span> <span class="org-variable-name">style_out_gram</span> (<span class="org-keyword">lfor</span> s style_out_raw (gram_matrix s)))

    (<span class="org-builtin">setv</span> <span class="org-variable-name">content_dict</span> (<span class="org-keyword">dfor</span> [content_name value]
                             (<span class="org-builtin">zip</span> <span class="org-keyword">self</span>.content_layers content_out_raw)
                             [content_name value]))
    (<span class="org-builtin">setv</span> <span class="org-variable-name">style_dict</span> (<span class="org-keyword">dfor</span> [style_name value]
                           (<span class="org-builtin">zip</span> <span class="org-keyword">self</span>.style_layers style_out_gram)
                           [style_name value]))

    {<span class="org-string">"content"</span> content_dict <span class="org-string">"style"</span> style_dict}))
(<span class="org-builtin">setv</span> <span class="org-variable-name">extractor</span> (StyleContentModel style_layers content_layers))
</pre>
</div>


<p>
Print the intermediate layers for both style and content images.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> <span class="org-variable-name">content_image_extract</span> (extractor (tf.constant content_image)))
(<span class="org-keyword">print</span> <span class="org-string">"Styles:"</span>)
(<span class="org-keyword">for</span> [[s_name output]
      (<span class="org-builtin">sorted</span> (.items (<span class="org-builtin">get</span> content_image_extract <span class="org-string">"style"</span>)))]
  (<span class="org-builtin">setv</span> <span class="org-variable-name">out_numpy</span> (output.numpy))
  (<span class="org-keyword">print</span> s_name)
  (<span class="org-keyword">print</span> <span class="org-string">"  shape: "</span> out_numpy.shape)
  (<span class="org-keyword">print</span> <span class="org-string">"  min: "</span> (out_numpy.min))
  (<span class="org-keyword">print</span> <span class="org-string">"  max: "</span> (out_numpy.max))
  (<span class="org-keyword">print</span> <span class="org-string">"  mean: "</span> (out_numpy.mean))
  (<span class="org-keyword">print</span>))

(<span class="org-keyword">print</span> <span class="org-string">"Contents:"</span>)
(<span class="org-keyword">for</span> [[c_name output]
      (<span class="org-builtin">sorted</span> (.items (<span class="org-builtin">get</span> content_image_extract <span class="org-string">"content"</span>)))]
  (<span class="org-builtin">setv</span> <span class="org-variable-name">out_numpy</span> (output.numpy))
  (<span class="org-keyword">print</span> c_name)
  (<span class="org-keyword">print</span> <span class="org-string">"  shape: "</span> out_numpy.shape)
  (<span class="org-keyword">print</span> <span class="org-string">"  min: "</span> (out_numpy.min))
  (<span class="org-keyword">print</span> <span class="org-string">"  max: "</span> (out_numpy.max))
  (<span class="org-keyword">print</span> <span class="org-string">"  mean: "</span> (out_numpy.mean))
  (<span class="org-keyword">print</span>))
</pre>
</div>


<pre class="example">
Styles:
block1_conv1
  shape:  (1, 64, 64)
  min:  0.040253956
  max:  35860.715
  mean:  436.2575

block2_conv1
  shape:  (1, 128, 128)
  min:  0.0
  max:  89071.31
  mean:  13139.537

block3_conv1
  shape:  (1, 256, 256)
  min:  0.0
  max:  257941.3
  mean:  11392.026

block4_conv1
  shape:  (1, 512, 512)
  min:  0.0
  max:  2987568.0
  mean:  182728.28

block5_conv1
  shape:  (1, 512, 512)
  min:  0.0
  max:  176838.52
  mean:  2137.6206

Contents:
block5_conv2
  shape:  (1, 24, 32, 512)
  min:  0.0
  max:  1579.4816
  mean:  16.581339
</pre>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-img_extract</span> []
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">len</span> (<span class="org-builtin">sorted</span> (.items (<span class="org-builtin">get</span> content_image_extract <span class="org-string">"style"</span>))))
             5))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">get</span> (<span class="org-builtin">get</span> (<span class="org-builtin">sorted</span> (.items (<span class="org-builtin">get</span> content_image_extract
                                            <span class="org-string">"style"</span>)))
                       4) 0) <span class="org-string">"block5_conv1"</span>))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">sum</span> (. (<span class="org-builtin">get</span> (<span class="org-builtin">get</span> (<span class="org-builtin">sorted</span> (.items (<span class="org-builtin">get</span> content_image_extract
                                                    <span class="org-string">"style"</span>)))
                               4) 1) shape)) 1025))

  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">len</span> (<span class="org-builtin">sorted</span> (.items (<span class="org-builtin">get</span> content_image_extract
                                       <span class="org-string">"content"</span>)))) 1))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">get</span> (<span class="org-builtin">get</span> (<span class="org-builtin">sorted</span> (.items (<span class="org-builtin">get</span> content_image_extract
                                            <span class="org-string">"content"</span>)))
                       0) 0) <span class="org-string">"block5_conv2"</span>))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">sum</span> (. (<span class="org-builtin">get</span> (<span class="org-builtin">get</span> (<span class="org-builtin">sorted</span> (.items (<span class="org-builtin">get</span> content_image_extract
                                                    <span class="org-string">"content"</span>)))
                               0) 1) shape)) 569)))
(test-case (test-img_extract))
</pre>
</div>


<p>
Set the style and content expected.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> <span class="org-variable-name">style_targets</span> (<span class="org-builtin">get</span> (extractor style_image) <span class="org-string">"style"</span>))
(<span class="org-builtin">setv</span> <span class="org-variable-name">content_targets</span> (<span class="org-builtin">get</span> (extractor content_image) <span class="org-string">"content"</span>))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-img_target</span> []
  (<span class="org-keyword">assert</span> (np.isclose (.mean (.numpy (<span class="org-builtin">get</span> style_targets <span class="org-string">"block5_conv1"</span>)))
                      1015.8152))
  (<span class="org-keyword">assert</span> (np.isclose (.mean (.numpy (<span class="org-builtin">get</span> content_targets <span class="org-string">"block5_conv2"</span>)))
                      16.581339)))
(test-case (test-img_target))
</pre>
</div>


<p>
Create the image to optimize,
and initialize it with the value of the content image.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> <span class="org-variable-name">generate_image</span> (tf.Variable content_image))
</pre>
</div>


<p>
Keep the pixel values between 0 and 1 (reduce noise).
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">clip_0_1</span> [img]
  (tf.clip_by_value img <span class="org-constant">:clip_value_min</span> 0.0 <span class="org-constant">:clip_value_max</span> 1.0))
</pre>
</div>


<p>
Create an optimizer with the Adam algorithm, and set the weights.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> <span class="org-variable-name">opt_adam</span> (tf.optimizers.Adam <span class="org-constant">:learning_rate</span> 0.02
                                   <span class="org-constant">:beta_1</span> 0.99
                                   <span class="org-constant">:epsilon</span> 1e-1))
(<span class="org-builtin">setv</span> <span class="org-variable-name">style_weight</span> 1e-2)
(<span class="org-builtin">setv</span> <span class="org-variable-name">content_weight</span> 1e4)
</pre>
</div>


<p>
Define the loss function as mean squared error method.
</p>

<p>
Content loss function is defined as:
</p>
\begin{align*}
\mathcal{L}_{content}(C, G, l) & = K(A^{(G)[l]} - A^{(C)[l]})^2 \\\\
\mathcal{L}_{content}(C, G) & = \sum_{l}L_{content}(C, G, l)
\end{align*}
<p>
Where \(C\) is the content image, \(S\) is the style image,
and \(G\) is the generated image.
</p>

<p>
Style loss function is defined as:
</p>
\begin{align*}
\mathcal{L}_{style}(S, G, l) & = K(G_{gram}^{(G)[l]} - G_{gram}^{(S)[l]})^2 \\\\
\mathcal{L}_{style}(S, G) & = \sum_{l}\mathcal{L}_{style}(S, G, l)
\end{align*}

<p>
Finally, the total loss function is:
</p>
\begin{align*}
\mathcal{L}_{total}(C, S, G) = \alpha \mathcal{L}_{content}(C, G)
+ \beta \mathcal{L}_{style}(S, G)
\end{align*}
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">style_content_loss</span> [gen_img_extract]
  (<span class="org-builtin">setv</span> <span class="org-variable-name">gen_img_style</span> (<span class="org-builtin">get</span> gen_img_extract <span class="org-string">"style"</span>))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">gen_img_content</span> (<span class="org-builtin">get</span> gen_img_extract <span class="org-string">"content"</span>))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">style_loss</span> (<span class="org-keyword">-&gt;</span>
                     (<span class="org-keyword">lfor</span> s_name (.keys gen_img_style)
                           (tf.reduce_mean
                             (<span class="org-builtin">**</span> (<span class="org-builtin">-</span> (<span class="org-builtin">get</span> gen_img_style s_name)
                                    (<span class="org-builtin">get</span> style_targets s_name)) 2)))
                     (tf.add_n)
                     (<span class="org-builtin">*</span> (<span class="org-builtin">/</span> style_weight num_style_layers))))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">content_loss</span> (<span class="org-keyword">-&gt;</span>
                       (<span class="org-keyword">lfor</span> c_name (.keys gen_img_content)
                             (tf.reduce_mean
                               (<span class="org-builtin">**</span> (<span class="org-builtin">-</span> (<span class="org-builtin">get</span> gen_img_content c_name)
                                      (<span class="org-builtin">get</span> content_targets c_name)) 2)))
                       (tf.add_n)
                       (<span class="org-builtin">*</span> (<span class="org-builtin">/</span> content_weight num_content_layers))))
  (<span class="org-builtin">+</span> style_loss content_loss))
</pre>
</div>


<p>
Build a one-step training model.
Gradient tape gives direct access to the individual gradients
that are in the layer.
Because <code>apply_gradients</code> function is used, <code>with tf.GradientTape()</code> block is
needed to save the loss variable.
Also, a decorator <code>(with-decorator tf.function (defn train_step [args1] body))</code>
allows to compile a function into a callable
TensorFlow graph for better performance (not used here).
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">train_step</span> [gen_img]
  (<span class="org-keyword">with</span> [g_tape (tf.GradientTape)]
    (<span class="org-builtin">setv</span> <span class="org-variable-name">loss</span> (style_content_loss (extractor gen_img))))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">grad</span> (g_tape.gradient loss gen_img))
  <span class="org-comment-delimiter">;; </span><span class="org-comment">HACK: opt_adam.apply_gradients is side effect.</span>
  <span class="org-comment-delimiter">;; </span><span class="org-comment">It changes gen_img values!!!</span>
  (opt_adam.apply_gradients [(<span class="org-builtin">,</span> grad gen_img)])
  (gen_img.assign (clip_0_1 gen_img))
  [loss grad])
</pre>
</div>


<p>
Build the Neural Style Transfer model.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">model</span> [epochs steps_per_epoch]
  (<span class="org-keyword">for</span> [e (<span class="org-builtin">range</span> epochs)]
    (<span class="org-keyword">for</span> [se (<span class="org-builtin">range</span> steps_per_epoch)]
      (train_step generate_image)))
  (train_step generate_image))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-model</span> []
  (<span class="org-builtin">setv</span> <span class="org-variable-name">generate_image</span> (tf.Variable content_image))
  (<span class="org-builtin">setv</span> [test_loss test_grad] (model 1 10))
  (<span class="org-keyword">assert</span> (np.isclose (.mean (.numpy test_grad)) 4.3584385))
  (<span class="org-keyword">assert</span> (np.isclose (.numpy test_loss) 27180544.0)))
(test-case (test-model))
</pre>
</div>
</div>
</li></ol>
</div>
<div id="outline-container-sec-3-9-3" class="outline-4">
<h4 id="sec-3-9-3"><span class="section-number-4">3.9.3</span> Results</h4>
<div class="outline-text-4" id="text-3-9-3">
<p>
The model has been trained during 200 steps, and below are only printed the
steps 10, 100, and 200. These steps show the evolution of the generated image.
At each step the style of the generated image is closer
to the style of the style image, but keeps the content of the content image.
There is a little noise on the generated image, but it can be reduced by
using the total variation loss technique.
Also, due to low performance computer
the number of training steps is limited to 200.
</p>
<div class="org-src-container">

<pre class="src src-hy">(model 1 10)
(show_img (tensor_to_image generate_image) <span class="org-string">"nst_generated_image_10"</span>)

(model 1 100)
(show_img (tensor_to_image generate_image) <span class="org-string">"nst_generated_image_100"</span>)

(model 1 200)
(show_img (tensor_to_image generate_image) <span class="org-string">"nst_generated_image_200"</span>)
</pre>
</div>

<figure>
<p><img src="./export/nst/nst_generated_image_10.png" class="img-responsive" alt="nst_generated_image_10.png" width="700">
</p>
<figcaption><span class="figure-number">Figure 41:</span> The generate image, 10 iterations</figcaption>
</figure>

<figure>
<p><img src="./export/nst/nst_generated_image_100.png" class="img-responsive" alt="nst_generated_image_100.png" width="700">
</p>
<figcaption><span class="figure-number">Figure 42:</span> The generate image, 100 iterations</figcaption>
</figure>

<figure>
<p><img src="./export/nst/nst_generated_image_200.png" class="img-responsive" alt="nst_generated_image_200.png" width="700">
</p>
<figcaption><span class="figure-number">Figure 43:</span> The generate image, 200 iterations</figcaption>
</figure>
</div>
</div>
<div id="outline-container-sec-3-9-4" class="outline-4">
<h4 id="sec-3-9-4"><span class="section-number-4">3.9.4</span> References</h4>
<div class="outline-text-4" id="text-3-9-4">
<ul class="org-ul">
<li>deeplearning.ai. 2020. <a href="https://www.deeplearning.ai/deep-learning-specialization/">Deep Learning Specialization</a>.
</li>
<li>Gatys Leon, et al. 2015. <a href="https://arxiv.org/pdf/1508.06576.pdf">A Neural Algorithm of Artistic Style</a>.
</li>
<li>Gatys Leon, et al. 2016. <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf">Image Style Transfer Using Convolutional Neural Networks</a>.
</li>
<li>Li Yanghao, et all. 2017. <a href="https://arxiv.org/pdf/1701.01036.pdf">Demystifying Neural Style Transfer</a>.
</li>
<li>Asokan Raghul. 2019. <a href="https://towardsdatascience.com/neural-networks-intuitions-2-dot-product-gram-matrix-and-neural-style-transfer-5d39653e7916">Neural Style Transfer, Gram Matrix</a>.
</li>
<li>TensorFlow Team. 2019. <a href="https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2">Fast arbitrary image style transfer</a>.
</li>
<li>TensorFlow Team. 2020. <a href="https://www.tensorflow.org/tutorials/generative/style_transfer">Neural Style Transfer</a>.
</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-sec-3-10" class="outline-3">
<h3 id="sec-3-10"><span class="section-number-3">3.10</span> Character-Level Language From Scratch</h3>
<div class="outline-text-3" id="text-3-10">

<figure>
<p><img src="./export/img/cll.png" class="img-responsive" alt="cll.png" width="480">
</p>
</figure>
</div>
<div id="outline-container-sec-3-10-1" class="outline-4">
<h4 id="sec-3-10-1"><span class="section-number-4">3.10.1</span> Purpose</h4>
<div class="outline-text-4" id="text-3-10-1">
<p>
The objective is to implement a Recurrent Neural Network from scratch in
order to build a character level language model.
The model generates new Gallic names thanks to a dataset that
contains character names from the Asterix <i>bande dessinée</i>.
The figure <a href="#fig:rnn_archi.png">45</a> represents the neural network architecture used.
\(a^{\langle p \rangle}\) is a RNN cell describes in figure <a href="#fig:rnn_cell.png">46</a>.
</p>

<figure id="fig:rnn_archi.png">
<p><img src="./export/cll/rnn_archi.png" class="img-responsive" alt="rnn_archi.png" width="480">
</p>
<figcaption><span class="figure-number">Figure 45:</span> The Recurrent Neural Network architecture</figcaption>
</figure>
</div>
</div>
<div id="outline-container-sec-3-10-2" class="outline-4">
<h4 id="sec-3-10-2"><span class="section-number-4">3.10.2</span> Implementation</h4>
<div class="outline-text-4" id="text-3-10-2">
</div><ol class="org-ol"><li><a id="sec-3-10-2-1" name="sec-3-10-2-1"></a>Definition<br ><div class="outline-text-5" id="text-3-10-2-1">
<p>
Recurrent Neural Network (RNN) is used in sequence tasks
(Natural Language Processing), because they have <i>memory</i>.
In sequence task, input and output can be of different length,
and the features learned must be shared through the model,
since the current output depends on the previous inputs.
A Recurrent Neural Network can handle these problems.
RNN can be used for speech recognition, music generation,
and sentiment classification.
The different types of RNN gates are:
</p>
<ul class="org-ul">
<li>Simple RNN: a basic RNN implementation (used in this project)
</li>
<li>Long Short Term Memory (LSTM) and Gated Recurrent Unit (GRU):
architectures to handle better the long-term dependencies
</li>
<li>Bidirectional RNN (BRNN): an architecture to get information
from the past and the future
</li>
<li>Deep RNN: stacks RNN layers to build a deeper network
</li>
</ul>
</div>
</li>
<li><a id="sec-3-10-2-2" name="sec-3-10-2-2"></a>Initialization<br ><div class="outline-text-5" id="text-3-10-2-2">
<p>
Import necessaries modules.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">require</span> [hy.contrib.loop [<span class="org-keyword">loop</span>]])
(<span class="org-keyword">require</span> [hytmf [test-case]])
(<span class="org-keyword">import</span> [numpy <span class="org-keyword">:as</span> np])
(<span class="org-keyword">import</span> [matplotlib [pyplot]])
</pre>
</div>


<p>
The code block below is described at <a href="#src-softmax">src-softmax</a>.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">softmax</span> [Z]
  (<span class="org-builtin">setv</span> <span class="org-variable-name">e_Z</span> (.exp np Z))
  (<span class="org-builtin">/</span> e_Z (.sum np e_Z <span class="org-constant">:axis</span> 0)))
</pre>
</div>


<p>
In this project immutable data structures are preferred, and both <code>shuffle</code>
and <code>assoc</code> functions modify the data structure in place, the functions
below fix this issue.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">clj_shuffle</span> [lst]
  (<span class="org-builtin">setv</span> <span class="org-variable-name">lst_tmp</span> (<span class="org-builtin">list</span> lst))
  (np.random.shuffle lst_tmp)
  lst_tmp)

(<span class="org-keyword">defn</span> <span class="org-function-name">clj_assoc</span> [d ind val]
  (<span class="org-builtin">setv</span> <span class="org-variable-name">d_tmp</span> (d.copy))
  (<span class="org-builtin">assoc</span> d_tmp ind val)
  d_tmp)
</pre>
</div>


<p>
Load the data which contains the Asterix character names.
All names are split in characters and are saved into a list.
The characters are a-z (26 characters), plus special characters,
plus <i>end of line</i> (eol).
</p>
<div class="org-src-container">

<pre class="src src-hy"> (<span class="org-builtin">setv</span> <span class="org-variable-name">name_lines</span> (<span class="org-keyword">lfor</span> l (<span class="org-keyword">-&gt;</span> <span class="org-string">"asterix.txt"</span> (<span class="org-builtin">open</span> <span class="org-string">"r"</span>) (.readlines))
                        (<span class="org-keyword">-&gt;</span> l (.lower) (.strip))))

 (<span class="org-builtin">setv</span> <span class="org-variable-name">name_chars</span> (<span class="org-keyword">-&gt;</span> <span class="org-string">"asterix.txt"</span>
                      (<span class="org-builtin">open</span> <span class="org-string">"r"</span>)
                      (.read)
                      (.lower)
                      (<span class="org-builtin">set</span>)
                      (<span class="org-builtin">list</span>)))

(<span class="org-builtin">setv</span> <span class="org-variable-name">vocab_size</span> (<span class="org-builtin">len</span> name_chars))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-vocab</span> []
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> vocab_size 37))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">len</span> name_lines) 187)))
(test-case (test-vocab))
</pre>
</div>


<p>
All characters are saved into a dictionary with a key from 0 to 36.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> <span class="org-variable-name">char_to_ind</span> (<span class="org-keyword">dfor</span> [ind ch] (<span class="org-builtin">enumerate</span> (<span class="org-builtin">sorted</span> name_chars)) [ch ind]))
(<span class="org-builtin">setv</span> <span class="org-variable-name">ind_to_char</span> (<span class="org-keyword">dfor</span> [ind ch] (<span class="org-builtin">enumerate</span> (<span class="org-builtin">sorted</span> name_chars)) [ind ch]))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-ind_char</span> []
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">len</span> char_to_ind) 37))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">len</span> ind_to_char) 37)))
(test-case (test-ind_char))
</pre>
</div>


<p>
Initialize both weights and bias with random values.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">init_params</span> [na nx ny seed]
  (np.random.seed seed)
  {<span class="org-string">"Wax"</span>  (<span class="org-builtin">*</span> (np.random.randn na nx) 0.01)
   <span class="org-string">"Waa"</span> (<span class="org-builtin">*</span> (np.random.randn na na) 0.01)
   <span class="org-string">"Wya"</span> (<span class="org-builtin">*</span> (np.random.randn ny na) 0.01)
   <span class="org-string">"ba"</span> (np.zeros [na 1])
   <span class="org-string">"by"</span> (np.zeros [ny 1])})
</pre>
</div>


<p>
Initialize the vector \(x\) and \(y\).
In a <i>simplified way</i> the vectors \(x\) (the input vector)
and \(y\) (the expected result vector) can be viewed as:
</p>
\begin{align*}
x & = \left[\begin{array}{r|r}
x^{\langle 1 \rangle} & \emptyset \\
x^{\langle 2 \rangle} & a \\
x^{\langle 3 \rangle} & s \\
x^{\langle 4 \rangle} & t \\
x^{\langle 5 \rangle} & e \\
x^{\langle 6 \rangle} & r \\
x^{\langle 7 \rangle} & i \\
x^{\langle 8 \rangle} & x
\end{array}\right] \\\\
y & = \left[\begin{array}{r|r}
y^{\langle 1 \rangle} & a \\
y^{\langle 2 \rangle} & s \\
y^{\langle 3 \rangle} & t \\
y^{\langle 4 \rangle} & e \\
y^{\langle 5 \rangle} & r \\
y^{\langle 6 \rangle} & i \\
y^{\langle 7 \rangle} & x \\
y^{\langle 8 \rangle} & eol
\end{array}\right]
\end{align*}
<p>
Nevertheless, in this project the vectors
\(x\) and \(y\) are in one-hot representation.
The vector \(x\) contains the characters,
and \(y\) the probability distribution associated.
In the vector \(y\) the probability is either 0 or 1,
in \(\hat{y}\) this is the output of \(\operatorname{softmax}\)
function i.e. probability distribution.
</p>
\begin{align*}
x & = \left[\begin{array}{r|r|r}
x^{\langle 1 \rangle} & vect() & 0 \\
x^{\langle 2 \rangle} & vect[1] & 1 \\
x^{\langle 3 \rangle} & vect[19] & 1 \\
x^{\langle 4 \rangle} & vect[20] & 1 \\
x^{\langle 5 \rangle} & vect[5] & 1 \\
x^{\langle 6 \rangle} & vect[18] & 1 \\
x^{\langle 7 \rangle} & vect[9] & 1 \\
x^{\langle 8 \rangle} & vect[24] & 1
\end{array}\right] \\\\
y & = \left[\begin{array}{r|r|r}
y^{\langle 1 \rangle} & vect[1] & p = 1 \\
y^{\langle 2 \rangle} & vect[19] & p = 1 \\
y^{\langle 3 \rangle} & vect[20] & p = 1 \\
y^{\langle 4 \rangle} & vect[5] & p = 1 \\
y^{\langle 5 \rangle} & vect[18] & p = 1 \\
y^{\langle 6 \rangle} & vect[9] & p = 1 \\
y^{\langle 7 \rangle} & vect[24] & p = 1 \\
y^{\langle 8 \rangle} & vect[27] & p = 1
\end{array}\right] \\\\
\end{align*}
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">init_train_data</span> [data ind]
  (<span class="org-builtin">setv</span> <span class="org-variable-name">X</span> (<span class="org-builtin">+</span> [<span class="org-constant">None</span>] (<span class="org-keyword">lfor</span> ch (<span class="org-builtin">get</span> data ind) (<span class="org-builtin">get</span> char_to_ind ch))))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">Y</span> (<span class="org-builtin">+</span> (<span class="org-builtin">get</span> X (<span class="org-builtin">slice</span> 1 <span class="org-constant">None</span>)) [(<span class="org-builtin">get</span> char_to_ind <span class="org-string">"\n"</span>)]))
  [X Y])
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-init_data</span> []
  (<span class="org-builtin">setv</span> [test_X test_Y] (init_train_data <span class="org-constant">:data</span> name_lines <span class="org-constant">:ind</span> 15))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">len</span> test_X) 11))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">len</span> test_Y) 11)))
(test-case (test-init_data))
</pre>
</div>
</div>
</li>
<li><a id="sec-3-10-2-3" name="sec-3-10-2-3"></a>Computation Graph<br ><div class="outline-text-5" id="text-3-10-2-3">
<p>
The equations for
both forward and backward propagation are based on the figure <a href="#fig:rnn_cell.png">46</a>.
</p>

<figure id="fig:rnn_cell.png">
<p><img src="./export/cll/rnn_cell.png" class="img-responsive" alt="rnn_cell.png" width="500">
</p>
<figcaption><span class="figure-number">Figure 46:</span> A simple Recurrent Neural Network cell</figcaption>
</figure>
</div>
<ol class="org-ol"><li><a id="sec-3-10-2-3-1" name="sec-3-10-2-3-1"></a>Forward Propagation<br ><div class="outline-text-6" id="text-3-10-2-3-1">
<p>
In the RNN cell the \(\tanh\) function is used.
This function has three benefits over \(\operatorname{sigmoid}\)
function and \(\operatorname{ReLU}\) function:
</p>
<ul class="org-ul">
<li>RNN models could have very large outputs,
and using \(\tanh\) over \(\operatorname{ReLU}\)
avoids or reduces the exploding gradient
problem (large error gradients accumulated)
</li>
<li>\(\tanh\) converges a little faster in practice than \(\operatorname{sigmoid}\)
</li>
<li>\(\tanh\) is more computationally efficient to compute,
in contrast to \(\operatorname{sigmoid}\) which is
defined by an exponential function
</li>
</ul>
<p>
The following formula shows the relation between
\(\tanh\) and \(\operatorname{sigmoid}\):
</p>
\begin{align*}
\operatorname{sigmoid}(x) & = \frac{1}{1 + e^{-x}} \\
& = \frac{e^x}{e^x + 1} \\\\
\tanh(x) & = \frac{\sinh(x)}{\cosh(x)} \\
& = \frac{e^x - e^{-x}}{e^x + e^{-x}} \\
& = \frac{e^{2x} - 1}{e^{2x} + 1} \\
& = 2 \times \operatorname{sigmoid}(2x) - 1
\end{align*}
<p>
Also the \(\tanh\) derivative is \(1 - \tanh(x) \times \tanh(x)\).
</p>

<p>
Plot the \(\operatorname{tanh}\) function.
</p>
<div class="org-src-container">

<pre class="src src-hy">(.figure pyplot <span class="org-string">"tanh"</span>)
(<span class="org-builtin">setv</span> <span class="org-variable-name">x</span> (.linspace np -10 10 100))
(.plot pyplot x (np.tanh x))
(.xlabel pyplot <span class="org-string">"x"</span>)
(.ylabel pyplot <span class="org-string">"tanh(x)"</span>)
(.savefig pyplot <span class="org-string">"tanh"</span>)
(.close pyplot)
</pre>
</div>

<figure>
<p><img src="./export/cll/tanh.png" class="img-responsive" alt="tanh.png" width="480">
</p>
<figcaption><span class="figure-number">Figure 47:</span> Plot of \(\operatorname{tanh}\)</figcaption>
</figure>


<p>
<a id="doc-forward-prop-rnn" name="doc-forward-prop-rnn"></a>
The general forward propagation formula is described at <a href="#doc-forward-prop">doc-forward-prop</a>,
in RNN the formula is:
</p>
\begin{align*}
u & = W_{ax} \cdot x^{\langle t \rangle} + W_{aa} \cdot
a^{\langle t-1 \rangle} + b_a \\\\
a^{\langle t \rangle} & = \tanh(u) \\\\
z^{\langle t \rangle} & = W_{ya} \cdot a^{\langle t \rangle} + b_y \\\\
\hat{y}^{\langle t \rangle} & = \operatorname{softmax}(z^{\langle t \rangle}) \\\\
\mathcal{L}&(\hat{y}^{\langle t \rangle}, y^{\langle t \rangle})
\end{align*}
<p>
The dimension of the vectors are:
</p>
<ul class="org-ul">
<li>\(\dim W_{ax} =  n_a \times n_x\)
</li>
<li>\(\dim W_{aa} = n_a \times n_a\)
</li>
<li>\(\dim W_{ya} = n_y \times n_a\)
</li>
<li>\(\dim a = n_a \times m \times T_x\)
</li>
<li>\(\dim a^{\langle t \rangle} = n_a \times m\)
</li>
<li>\(\dim \hat{y} = n_y \times m \times T_x\)
</li>
<li>\(\dim \hat{y}^{\langle t \rangle} = n_y \times m\)
</li>
<li>\(\dim x^{\langle t \rangle} = n_x \times m\)
</li>
<li>\(\dim b_a = n_a \times 1\)
</li>
<li>\(\dim b_y = n_y \times 1\)
</li>
</ul>
<p>
Below the description of the symbols in the previous equation:
</p>
<ul class="org-ul">
<li>\(m\) is the number of examples
</li>
<li>\(n_a\) is the number of hidden neurons
</li>
<li>In one-hot vector representation \(n_x = n_y =\) vocabulary size \(=\) alphabet
</li>
<li>In this implementation \(n_x = 37\), because alphabet +
special characters (Gallic characters)
</li>
<li>\(T_x\) and \(T_y\) are the size of the input and the output sequence,
respectively
</li>
<li>In this implementation \(T_x = T_y\), it corresponds to a
<i>Many to Many architecture</i> with input size equals to output size
</li>
<li>The input sequence is the vector
\(x = \begin{bmatrix} x^{\langle 1\rangle} & x^{\langle 2 \rangle} & \dots & x^{\langle t \rangle} & \dots & x^{\langle T_x \rangle} \end{bmatrix}\)
</li>
<li>The output sequence is the vector
\(y = \begin{bmatrix} y^{\langle 1 \rangle} & y^{\langle 2 \rangle} & \dots & y^{\langle t \rangle} & \dots & y^{\langle T_y \rangle} \end{bmatrix}\)
</li>
<li>At each time-step \(t\), \(y^{\langle t \rangle} = x^{\langle t + 1 \rangle}\)
</li>
<li>The hidden state is
\(a = \begin{bmatrix} a^{\langle 1 \rangle} & a^{\langle 2 \rangle} & \dots & a^{\langle t \rangle} & \dots & a^{\langle T_x \rangle} \end{bmatrix}\)
</li>
<li>\(a^{\langle 0 \rangle}\) represents the zero vector
</li>
</ul>
<p>
The following equation
\(u = W_{ax} \cdot x^{\langle t \rangle} + W_{aa} \cdot a^{\langle t-1 \rangle} + b_a\),
corresponds to the matrix:
</p>
\begin{align*}
x^{\langle t \rangle} & =
\begin{bmatrix}
x^{\langle t \rangle}_{11} & x^{\langle t \rangle}_{12} &
\dots & x^{\langle t \rangle}_{1p} & \dots & x^{\langle t \rangle}_{1m} \\
x^{\langle t \rangle}_{21} & x^{\langle t \rangle}_{22} &
\dots & x^{\langle t \rangle}_{2p} & \dots & x^{\langle t \rangle}_{2m} \\
\dots & \dots & \dots & \dots & \dots & \dots \\
x^{\langle t \rangle}_{p1} & x^{\langle t \rangle}_{p2} &
\dots & x^{\langle t \rangle}_{pp} & \dots & x^{\langle t \rangle}_{pm} \\
\dots & \dots & \dots & \dots & \dots & \dots \\
x^{\langle t \rangle}_{n_x1} & x^{\langle t \rangle}_{n_x2} &
\dots & x^{\langle t \rangle}_{n_xp} & \dots & x^{\langle t \rangle}_{n_xm}
\end{bmatrix} \\\\
W_{ax} & =
\begin{bmatrix}
w_{ax11} & w_{ax12} & \dots & w_{ax1p} & \dots & w_{ax1n_x} \\
w_{ax21} & w_{ax22} & \dots & w_{ax2p} & \dots & w_{ax2n_x} \\
\dots & \dots & \dots & \dots & \dots & \dots \\
w_{axp1} & w_{axp2} & \dots & w_{axpp} & \dots & w_{axpn_x} \\
\dots & \dots & \dots & \dots & \dots & \dots \\
w_{axn_a1} & w_{axn_a2} & \dots & w_{axn_ap} & \dots & w_{axn_an_x} \\
\end{bmatrix} \\\\
W_{ax} \cdot x^{\langle t \rangle} & =
\begin{bmatrix}
w_{ax11} \times x^{\langle t \rangle}_{11} + w_{ax12} \times
x^{\langle t \rangle}_{21} + \dots + w_{ax1p} \times
x^{\langle t \rangle}_{p1} + \dots + w_{ax1n_x} \times
x^{\langle t \rangle}_{n_x1} \\
w_{ax21} \times x^{\langle t \rangle}_{12} + w_{ax22} \times
x^{\langle t \rangle}_{22} + \dots + w_{ax2p} \times
x^{\langle t \rangle}_{p2} + \dots + w_{ax2n_x} \times
x^{\langle t \rangle}_{n_x2} \\
\dots \\
w_{axp1} \times x^{\langle t \rangle}_{1p} + w_{axp2} \times
x^{\langle t \rangle}_{2p} + \dots + w_{axpp} \times
x^{\langle t \rangle}_{pp} + \dots + w_{axpn_x} \times
x^{\langle t \rangle}_{n_xp} \\
\dots \\
w_{axn_a1} \times x^{\langle t \rangle}_{1m} + w_{axna2} \times
x^{\langle t \rangle}_{2m} + \dots + w_{axn_ap} \times
x^{\langle t \rangle}_{pm} + \dots + w_{axn_an_x} \times
x^{\langle t \rangle}_{n_xm}
\end{bmatrix}
\end{align*}
<p>
The calculation for forward propagation in RNN is a little different
compared to Logistic Regression one (<a href="#doc-forward-prop">doc-forward-prop</a>).
Since the formula used is
\(u_{1i} = w_{ax11} \times x^{\langle t \rangle}_{1i} + w_{ax12} \times x^{\langle t \rangle}_{2i}\)
, instead of \(z_{1i} = w_{11} \times x_{1i} + w_{21} \times x_{2i}\),
because \(\dim W_{ax} = n_a \times n_x\) is different
from \(\dim W = n_x \times 1\),
and this is also similar for \(W_{aa}\), and \(a^{\langle t-1 \rangle}\).
</p>

<p>
The procedure to understand the forward propagation formula is to break down it
without vectorization (matrix form), and with fixed dimensions:
let \(n_{a} = 1\), \(n_{y} = 1\) and \(n_{x} = 2\), <br >
<code>for</code> \(i\in [1, m]\):
</p>
\begin{align*}
u_{1i} & = w_{ax11} \times x^{\langle t \rangle}_{1i} + w_{ax12} \times
x^{\langle t \rangle}_{2i} + w_{aa11} \times a^{\langle t-1 \rangle}_{1i} + b_a \\\\
a^{\langle t \rangle}_{1i} & = \tanh(u_{1i}) \\\\
z^{\langle t \rangle}_{1i} & = w_{ya11} \times a^{\langle t \rangle}_{1i} + b_y \\\\
\hat{y}^{\langle t \rangle}_{1i} & = \operatorname{softmax}(z_{1i}) \\\\
\mathcal{L}&(\hat{y}^{\langle t \rangle}_{1i}, y^{\langle t \rangle}_{1i})
\end{align*}
<p>
The matrix associated for some operands are:
</p>
\begin{align*}
W_{ya} & =
\begin{bmatrix}
w_{ya11} & w_{ya12} & \dots & w_{ya1p} & \dots & w_{ya1n_a} \\
w_{ya21} & w_{ya22} & \dots & w_{ya2p} & \dots & w_{ya2n_a} \\
\dots \\
w_{yap1} & w_{yap2} & \dots & w_{yapp} & \dots & w_{yapn_a} \\
\dots \\
w_{yan_y1} & w_{yan_y2} & \dots & w_{yan_yp} & \dots & w_{yan_yn_a} \\
\end{bmatrix} \\\\
a^{\langle t \rangle} & = \begin{bmatrix}
a^{\langle t \rangle}_{11} & a^{\langle t \rangle}_{12} & \dots
& a^{\langle t \rangle}_{1p} & \dots & a^{\langle t \rangle}_{1m} \\
a^{\langle t \rangle}_{21} & a^{\langle t \rangle}_{22} & \dots
& a^{\langle t \rangle}_{2p} & \dots & a^{\langle t \rangle}_{2m} \\
\dots \\
a^{\langle t \rangle}_{p1} & a^{\langle t \rangle}_{p2} & \dots
& a^{\langle t \rangle}_{pp} & \dots & a^{\langle t \rangle}_{pm} \\
\dots \\
a^{\langle t \rangle}_{n_a1} & a^{\langle t \rangle}_{n_a2} & \dots
& a^{\langle t \rangle}_{n_ap} & \dots & a^{\langle t \rangle}_{n_am}
\end{bmatrix}
\end{align*}
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">rnn_cell_forward</span> [xt_curr at_prev params]
  (<span class="org-builtin">setv</span> <span class="org-variable-name">at_curr</span> (np.tanh
                  (<span class="org-builtin">+</span> (np.dot (<span class="org-builtin">get</span> params <span class="org-string">"Wax"</span>) xt_curr)
                     (np.dot (<span class="org-builtin">get</span> params <span class="org-string">"Waa"</span>) at_prev)
                     (<span class="org-builtin">get</span> params <span class="org-string">"ba"</span>))))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">ythat_curr</span> (softmax
                     (<span class="org-builtin">+</span> (np.dot (<span class="org-builtin">get</span> params <span class="org-string">"Wya"</span>) at_curr)
                        (<span class="org-builtin">get</span> params <span class="org-string">"by"</span>))))
  [at_curr ythat_curr])

(<span class="org-keyword">defn</span> <span class="org-function-name">rnn_forward</span> [X Y at_0 params]
  (<span class="org-keyword">loop</span> [[i 0]
         [X_onehot {}]
         [A_onehot {-1 at_0}]
         [Yhat_onehot {}]
         [loss 0]]
        (<span class="org-keyword">if</span> (<span class="org-builtin">&gt;=</span> i (<span class="org-builtin">len</span> X))
            [loss [Yhat_onehot A_onehot X_onehot]]
            (<span class="org-keyword">do</span>
              (<span class="org-builtin">setv</span> <span class="org-variable-name">xt_curr</span>
                    (clj_assoc
                      (np.zeros [vocab_size 1]) (<span class="org-builtin">get</span> X i)
                      (<span class="org-builtin">int</span> (<span class="org-builtin">!=</span> (<span class="org-builtin">get</span> X i) <span class="org-constant">None</span>))))
              (<span class="org-builtin">setv</span> [at_curr ythat_curr]
                    (rnn_cell_forward xt_curr (<span class="org-builtin">get</span> A_onehot (<span class="org-builtin">-</span> i 1)) params))
              (<span class="org-keyword">recur</span> (<span class="org-builtin">inc</span> i)
                     (clj_assoc X_onehot i xt_curr)
                     (clj_assoc A_onehot i at_curr)
                     (clj_assoc Yhat_onehot i ythat_curr)
                     (<span class="org-builtin">-</span> loss (<span class="org-builtin">get</span> (np.log (<span class="org-builtin">get</span> ythat_curr (<span class="org-builtin">get</span> Y i))) 0)))))))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-rnn_cell_forward</span> []
  (np.random.seed 1)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_xt_curr</span> (np.random.randn 3 10))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_at_prev</span> (np.random.randn 5 10))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_params</span> {<span class="org-string">"Wax"</span> (np.random.randn 5 3)
                     <span class="org-string">"Waa"</span> (np.random.randn 5 5)
                     <span class="org-string">"Wya"</span> (np.random.randn 2 5)
                     <span class="org-string">"ba"</span> (np.random.randn 5 1)
                     <span class="org-string">"by"</span> (np.random.randn 2 1)})

  (<span class="org-builtin">setv</span> [test_at_curr_out test_ythat_curr_out]
        (rnn_cell_forward test_xt_curr test_at_prev test_params))

  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> test_at_curr_out.shape (<span class="org-builtin">,</span> 5 10)))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> test_ythat_curr_out.shape (<span class="org-builtin">,</span> 2 10)))
  (<span class="org-keyword">assert</span> (np.isclose (test_at_curr_out.mean) -0.10435089))
  (<span class="org-keyword">assert</span> (np.isclose (test_ythat_curr_out.mean) 0.5)))
(test-case (test-rnn_cell_forward))

(<span class="org-keyword">defn</span> <span class="org-function-name">test-rnn_forward</span> []
  (np.random.seed 1)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_na</span> 2)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_Wax</span> (np.random.randn test_na vocab_size))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_Waa</span> (np.random.randn test_na test_na))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_Wya</span> (np.random.randn vocab_size test_na))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_ba</span> (np.random.randn test_na 1))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_by</span> (np.random.randn vocab_size 1))
  (<span class="org-builtin">setv</span> [test_X test_Y] (init_train_data name_lines 15))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_at_0</span> (np.random.randn test_na 1))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_params</span> {<span class="org-string">"Wax"</span> test_Wax
                     <span class="org-string">"Waa"</span> test_Waa
                     <span class="org-string">"Wya"</span> test_Wya
                     <span class="org-string">"ba"</span> test_ba
                     <span class="org-string">"by"</span> test_by})

  (<span class="org-builtin">setv</span> [test_loss [test_Yhat_res test_A_res test_X_res]]
        (rnn_forward test_X test_Y test_at_0 test_params))

  (<span class="org-keyword">assert</span> (np.isclose (<span class="org-builtin">get</span> (<span class="org-builtin">sum</span> (<span class="org-builtin">sum</span> (test_Yhat_res.values))) 0) 11))
  (<span class="org-keyword">assert</span> (np.isclose (<span class="org-builtin">get</span> (<span class="org-builtin">sum</span> (<span class="org-builtin">sum</span> (test_A_res.values))) 0) 6.40218838))
  (<span class="org-keyword">assert</span> (np.isclose (<span class="org-builtin">get</span> (<span class="org-builtin">sum</span> (<span class="org-builtin">sum</span> (test_X_res.values))) 0) 10.0))
  (<span class="org-keyword">assert</span> (np.isclose test_loss 49.606304)))
(test-case (test-rnn_forward))
</pre>
</div>
</div>
</li>
<li><a id="sec-3-10-2-3-2" name="sec-3-10-2-3-2"></a>Backward Propagation<br ><div class="outline-text-6" id="text-3-10-2-3-2">
<p>
<a id="doc-backward-prop-rnn" name="doc-backward-prop-rnn"></a>
The general backward propagation formula is described at <a href="#doc-backward-prop">doc-backward-prop</a>,
in RNN the formula is:
</p>
\begin{align*}
\partial{\tilde{W}_{ya(n_y \times n_a)}} & \mathrel{{+}{=}}
\partial{\tilde{Z}_{(n_y \times m)}}
\cdot A^{\langle t \rangle\operatorname{T}}_{(n_a \times m)}
\partial{\tilde{b}_{y11(n_y \times 1)}} \\
& \mathrel{{+}{=}} \sum \partial{\tilde{Z}_{(n_y \times m)}} \\\\
\partial{\tilde{W}_{ax(n_a \times n_x)}} & \mathrel{{+}{=}}
(\partial{\tilde{u}_{(n_a \times m)}} \times D_{out(n_a \times m)})
\cdot X^{\langle t \rangle\operatorname{T}}_{(n_x \times m)} \\\\
\partial{\tilde{W}_{aa(n_a \times n_a)}} & \mathrel{{+}{=}}
(\partial{\tilde{u}_{(n_a \times m)}} \times D_{out(n_a \times m)})
\cdot A^{\langle t-1 \rangle\operatorname{T}}_{(n_a \times m)} \\\\
\partial{\tilde{b}_{a(n_a \times 1)}} & \mathrel{{+}{=}}
\sum \partial{\tilde{u}_{(n_a \times m)}} \times D_{out(n_a \times m)} \\\\
\partial{\tilde{A}^{\langle t-1 \rangle}_{(n_a \times m)}} & =
W^{\operatorname{T}}_{aa(n_a \times n_a)}
\cdot (\partial{\tilde{u}_{(n_a \times m)}} \times D_{out(n_a \times m)})
\end{align*}
<p>
The loss function (described at <a href="#src-loss">src-loss</a>) can be rewritten as following:
</p>
\begin{align*}
\mathcal{L}(\hat{y}, y) = - \sum_{i}y_i\log(\hat{y_i})
\end{align*}
<p>
For example, let \(y = \begin{bmatrix}0 & 1 & 0 & 0\end{bmatrix}\),
\(z = \begin{bmatrix}1 & 3 & 2.5 & 4\end{bmatrix}\), and
\(\hat{y} = \operatorname{softmax}(z) = \begin{bmatrix}0.030 & 0.224 & 0.135 & 0.609\end{bmatrix}\)
then
</p>
\begin{align*}
\mathcal{L}(\hat{y}, y) & = - \sum_{i}y_i\log(\hat{y_i}) \\
& = -0\log(0.030) -1\log(0.224) -0\log(0.135) -0\log(0.609) \\
& = -log(0.224) \\
& = 1.496
\end{align*}
<p>
Because of one-hot vector \(y\), the only term left is the negative
log probability of the true class at index \(k\),
therefore the loss function can be rewritten as following:
</p>
\begin{align*}
\mathcal{L}(\hat{y}, y) & = -\log(\hat{y_k}) \\
& = -\log(\operatorname{softmax}(z_k)) \\
& = -\log(\frac{e^{z_k}}{\sum_{j} e^{z_j}}) \\
& = -z_k + \log(\sum_{j} e^{z_j})
\end{align*}
<p>
Its derivative for each \(z_i\) is:
</p>
\begin{align*}
\frac{\partial{\mathcal{L}}}{\partial{z_i}} & =
\frac{\partial}{\partial{z_i}} (-z_k + \log(\sum_{j} e^{z_j})) \\
& = \frac{\partial}{\partial{z_i}} \log(\sum_{j} e^{z_j}) -
\frac{\partial}{\partial{z_i}} z_k \\
& = \frac{1}{\sum_{j} e^{z_j}} \frac{\partial}{\partial{z_i}}
\sum_{j} e^{z_j} - \frac{\partial}{\partial{z_i}} z_k \\
& = \frac{e^{z_i}}{\sum_{j} e^{z_j}} - \frac{\partial}{\partial{z_i}} z_k \\
& = \hat{y_i} - \frac{\partial}{\partial{z_i}} z_k \\
& = \hat{y_i} - \mathbf{1}(k=i)
\end{align*}
<p>
Where \(\mathbf{1}(k=i)\) is the indicator function defined as:
</p>
\begin{align*}
\mathbf{1}(k=i) :=
\begin{cases}
1 &\text{if } k=i \\
0 &\text{if } otherwise
\end{cases}
\end{align*}
<p>
Thus, for the true label \(k\) the derivative is
\(\frac{\partial{\mathcal{L}}}{\partial{z_k}} = \hat{y_k} - 1\),
otherwise, the derivative is
\(\frac{\partial{\mathcal{L}}}{\partial{z_i}} = \hat{y_i}, \forall i \neq k\).
In a perfect world, with a perfect classification, \(\hat{y_k} = 1\)
and \(\hat{y_i} = 0, \forall i \neq k\), thus the total loss is
\(\frac{\partial{\mathcal{L}}}{\partial{z_i}} = 0, \forall i\).
In this case none of the parameters will be updated.
</p>

<p>
The paragraphs below explain how to calculate
the equation <a href="#doc-backward-prop-rnn">doc-backward-prop-rnn</a>.
The first examples use the non-vectorization form with fixed values for
\(n_a\), \(n_y\), \(n_x\), as described at  <a href="#doc-forward-prop-rnn">doc-forward-prop-rnn</a>.
</p>

<p>
Also, in the implementation this is
\(\mathcal{L}(\operatorname{softmax}(z^{\langle t \rangle}_{1i}), y^{\langle t \rangle}_{1i})\)
instead of
\(\mathcal{L}(\hat{y}^{\langle t \rangle}_{1i}, y^{\langle t \rangle}_{1i})\).
</p>

<p>
The notation used for the chain rule derivative is:
\(\partial{\tilde{g}} = \frac{\partial{f}}{\partial{g}} = \frac{\partial{f}}{\partial{a}} \times \frac{\partial{a}}{\partial{g}}\)
</p>

<p>
The optimization has to be applied on the weights
(\(W_{ax}\), \(b_a\), \(W_{aa}\), \(a^{\langle t-1 \rangle}\), \(W_{ya}\), \(b_y\)),
but not on the features (\(x^{\langle t \rangle}\)).
</p>

<p>
Unlike the <i>classical</i> neural networks, in the RNN there are two outputs:
\(\hat{y}^{\langle t \rangle}\) and \(a^{\langle t \rangle}\),
thus a backward propagation in three steps:
loss step (\(\hat{y}^{\langle t \rangle}\)),
\(a^{\langle t \rangle}\) step and the addition step of both to get
the final formula described at <a href="#doc-backward-prop-rnn">doc-backward-prop-rnn</a>.
</p>

<p>
First, the backward propagation step for loss function,
with non-vectorization and fixed dimensions is : <br >
<code>for</code> \(i\in [1, m]\): <br >
</p>
\begin{align*}
\partial{\tilde{W}^{(Loss)}_{ya11}} & \mathrel{{+}{=}}
\frac{\partial{\mathcal{L}}}{\partial{W_{ya11}}} \\
& = \frac{\partial{\mathcal{L}}}{\partial{z^{\langle t \rangle}_{1i}}} \times
\frac{\partial{z^{\langle t \rangle}_{1i}}}{\partial{W_{ya11}}} \\
& = \partial{\tilde{z}^{\langle t \rangle}_{1i}} \times
a^{\langle t \rangle}_{1i} \\\\
\partial\tilde{b}^{(Loss)}_{y11} & \mathrel{{+}{=}}
\frac{\partial{\mathcal{L}}}{\partial{b_{y11}}} \\
& = \frac{\partial{\mathcal{L}}}{\partial{z^{\langle t \rangle}_{1i}}} \times
\frac{\partial{z^{\langle t \rangle}_{1i}}}{\partial{b_{y11}}} \\
& = \partial{\tilde{z}^{\langle t \rangle}_{1i}} \\\\
\partial{\tilde{W}^{(Loss)}_{ax11}} & \mathrel{{+}{=}}
\frac{\partial{\mathcal{L}}}{\partial{W_{ax11}}} \\
& = \frac{\partial{\mathcal{L}}}{\partial{z_{1i}}} \times
\frac{\partial{z_{1i}}}{\partial{a^{\langle t \rangle}_{1i}}} \times
\frac{\partial{a^{\langle t \rangle}_{1i}}}{\partial{u_{1i}}} \times
\frac{\partial{u_{1i}}}{\partial{W_{ax11}}} \\
& = \partial{\tilde{z}^{\langle t \rangle}_{1i}} \times W_{ya11}
\times (1 - a^{\langle t \rangle}_{1i} \times a^{\langle t \rangle}_{1i})
\times x^{\langle t \rangle}_{1i} \\\\
\partial{\tilde{W}^{(Loss)}_{ax12}} & \mathrel{{+}{=}}
\frac{\partial{\mathcal{L}}}{\partial{W_{ax12}}} \\
& = \frac{\partial{\mathcal{L}}}{\partial{z_{1i}}} \times
\frac{\partial{z_{1i}}}{\partial{a^{\langle t \rangle}_{1i}}} \times
\frac{\partial{a^{\langle t \rangle}_{1i}}}{\partial{u_{1i}}} \times
\frac{\partial{u_{1i}}}{\partial{W_{ax12}}} \\
& = \partial{\tilde{z}^{\langle t \rangle}_{1i}} \times W_{ya11}
\times (1 - a^{\langle t \rangle}_{1i} \times a^{\langle t \rangle}_{1i})
\times x^{\langle t \rangle}_{2i} \\\\
\partial{\tilde{b}^{(Loss)}_{a11}} & \mathrel{{+}{=}}
\frac{\partial{\mathcal{L}}}{\partial{b_{a11}}} \\
& = \frac{\partial{\mathcal{L}}}{\partial{z_{1i}}}
\times \frac{\partial{z_{1i}}}{\partial{a^{\langle t \rangle}_{1i}}}
\times \frac{\partial{a^{\langle t \rangle}_{1i}}}{\partial{u_{1i}}}
\times \frac{\partial{u_{1i}}}{\partial{b_{a11}}} \\
& = \partial{\tilde{z}^{\langle t \rangle}_{1i}} \times W_{ya11}
\times (1 - a^{\langle t \rangle}_{1i} \times a^{\langle t \rangle}_{1i}) \\\\
\partial{\tilde{W}^{(Loss)}_{aa11}} & \mathrel{{+}{=}}
\frac{\partial{\mathcal{L}}}{\partial{W_{aa11}}} \\
& = \frac{\partial{\mathcal{L}}}{\partial{z_{1i}}} \times
\frac{\partial{z_{1i}}}{\partial{a^{\langle t \rangle}_{1i}}}
\times \frac{\partial{a^{\langle t \rangle}_{1i}}}{\partial{u_{1i}}}
\times \frac{\partial{u_{1i}}}{\partial{W_{aa11}}} \\
& = \partial{\tilde{z}^{\langle t \rangle}_{1i}} \times W_{ya11}
\times (1 - a^{\langle t \rangle}_{1i} \times a^{\langle t \rangle}_{1i})
\times a^{\langle t-1 \rangle}_{1i} \\\\
\partial{a^{\langle t-1 \rangle(Loss)}_{1i}} & =
\frac{\partial{\mathcal{L}}}{\partial{a^{\langle t-1 \rangle}_{1i}}} \\
& = \frac{\partial{\mathcal{L}}}{\partial{z_{1i}}} \times
\frac{\partial{z_{1i}}}{\partial{a^{\langle t \rangle}_{1i}}} \times
\frac{\partial{a^{\langle t \rangle}_{1i}}}{\partial{u_{1i}}} \times
\frac{\partial{u_{1i}}}{\partial{a^{\langle t-1 \rangle}_{1i}}} \\
& = \partial{\tilde{z}^{\langle t \rangle}_{1i}} \times W_{ya11}
\times (1 - a^{\langle t \rangle}_{1i} \times a^{\langle t \rangle}_{1i})
\times W_{aa11}
\end{align*}
<p>
Then, the backward propagation step for \(a^{\langle t \rangle(a_{out})}\),
with non-vectorization and fixed dimensions is : <br >
<code>for</code> \(i\in [1, m]\): <br >
</p>
\begin{align*}
\partial{\tilde{W}^{(a_{out})}_{ax11}} & \mathrel{{+}{=}}
\frac{\partial{a^{\langle t+1 \rangle}_{1i}}}{\partial{W_{ax11}}} \\
& = \frac{\partial{a^{\langle t+1 \rangle}_{1i}}}
{\partial{a^{\langle t \rangle}_{1i}}} \times
\frac{\partial{a^{\langle t \rangle}_{1i}}}{\partial{u_{1i}}} \times
\frac{\partial{u_{1i}}}{\partial{W_{ax11}}} \\
& = \partial{\tilde{a}^{\langle t \rangle}_{1i}}
\times (1 - a^{\langle t \rangle}_{1i} \times a^{\langle t \rangle}_{1i})
\times x^{\langle t \rangle}_{1i} \\\\
\partial{\tilde{W}^{(a_{out})}_{ax12}} & \mathrel{{+}{=}}
\frac{\partial{a^{\langle t+1 \rangle}_{1i}}}{\partial{W_{ax12}}} \\
& = \frac{\partial{a^{\langle t+1 \rangle}_{1i}}}
{\partial{a^{\langle t \rangle}_{1i}}} \times
\frac{\partial{a^{\langle t \rangle}_{1i}}}{\partial{u_{1i}}} \times
\frac{\partial{u_{1i}}}{\partial{W_{ax12}}} \\
& = \partial{\tilde{a}^{\langle t \rangle}_{1i}}
\times (1 - a^{\langle t \rangle}_{1i} \times a^{\langle t \rangle}_{1i})
\times x^{\langle t \rangle}_{2i} \\\\
\partial{\tilde{b}^{(a_{out})}_{a11}} & \mathrel{{+}{=}}
\frac{\partial{a^{\langle t+1 \rangle}_{1i}}}{\partial{b_{a11}}} \\
& = \frac{\partial{a^{\langle t+1 \rangle}_{1i}}}
{\partial{a^{\langle t \rangle}_{1i}}} \times
\frac{\partial{a^{\langle t \rangle}_{1i}}}{\partial{u_{1i}}}
\times \frac{\partial{u_{1i}}}{\partial{b_{a11}}} \\
& = \partial{\tilde{a}^{\langle t \rangle}_{1i}}
\times (1 - a^{\langle t \rangle}_{1i} \times a^{\langle t \rangle}_{1i}) \\\\
\partial{\tilde{W}^{(a_{out})}_{aa11}} & \mathrel{{+}{=}}
\frac{\partial{a^{\langle t+1 \rangle}_{1i}}}{\partial{W_{aa11}}} \\
& = \frac{\partial{a^{\langle t+1 \rangle}_{1i}}}
{\partial{a^{\langle t \rangle}_{1i}}} \times
\frac{\partial{a^{\langle t \rangle}_{1i}}}{\partial{u_{1i}}}
\times \frac{\partial{u_{1i}}}{\partial{W_{aa11}}} \\
& = \partial{\tilde{a}^{\langle t \rangle}_{1i}}
\times (1 - a^{\langle t \rangle}_{1i} \times a^{\langle t \rangle}_{1i})
\times a^{\langle t-1 \rangle}_{1i} \\\\
\partial{\tilde{a}^{\langle t-1 \rangle(a_{out})}_{1i}} & \mathrel{{+}{=}}
\frac{\partial{a^{\langle t+1 \rangle}_{1i}}}
{\partial{a^{\langle t-1 \rangle}_{1i}}} \\
& = \frac{\partial{a^{\langle t+1 \rangle}_{1i}}}{
\partial{a^{\langle t \rangle}_{1i}}} \times
\frac{\partial{a^{\langle t \rangle}_{1i}}}{\partial{u_{1i}}}
\times \frac{\partial{u_{1i}}}{\partial{a^{\langle t-1 \rangle}_{1i}}} \\
& = \partial{\tilde{a}^{\langle t \rangle}_{1i}}
\times (1 - a^{\langle t \rangle}_{1i}
\times a^{\langle t \rangle}_{1i}) \times W_{aa11}
\end{align*}
<p>
Finally, the backward propagation step for
addition of both loss function and \(a^{\langle t \rangle(a_{out})}\)
with non-vectorization and fixed dimensions is: <br >
<code>for</code> \(i\in [1, m]\): <br >
</p>
\begin{align*}
\partial{\tilde{W}_{ax11}} & \mathrel{{+}{=}}
\partial{\tilde{W}^{(Loss)}_{ax11}} + \partial{\tilde{W}^{(a_{out})}_{ax11}} \\
& = (1 - a^{\langle t \rangle}_{1i} \times a^{\langle t \rangle}_{1i})
\times x^{\langle t \rangle}_{1i} \times (\partial{\tilde{z_{1i}}}
\times W_{ya11} + \partial{\tilde{a}^{\langle t \rangle}_{1i}}) \\\\
\partial{\tilde{W}_{ax12}} & \mathrel{{+}{=}}
\partial{\tilde{W}^{(Loss)}_{ax12}} + \partial{\tilde{W}^{(a_{out})}_{ax12}} \\
& = (1 - a^{\langle t \rangle}_{1i} \times a^{\langle t \rangle}_{1i})
\times x^{\langle t \rangle}_{2i} \times (\partial{\tilde{z_{1i}}}
\times W_{ya11} + \partial{\tilde{a}^{\langle t \rangle}_{1i}}) \\\\
\partial{\tilde{b}_{a11}} & \mathrel{{+}{=}}
\partial{\tilde{b}^{(Loss)}_{a11}} + \partial{\tilde{b}^{(a_{out})}_{a11}} \\
& = (1 - a^{\langle t \rangle}_{1i} \times a^{\langle t \rangle}_{1i})
\times (\partial{\tilde{z_{1i}}} \times W_{ya11} +
\partial{\tilde{a}^{\langle t \rangle}_{1i}}) \\\\
\partial{\tilde{W}_{aa11}} & \mathrel{{+}{=}}
\partial{\tilde{W}^{(Loss)}_{aa11}} + \partial{\tilde{W}^{(a_{out})}_{aa11}} \\
& = (1 - a^{\langle t \rangle}_{1i} \times a^{\langle t \rangle}_{1i})
\times a^{\langle t-1 \rangle}_{1i} \times (\partial{\tilde{z_{1i}}}
\times W_{ya11} + \partial{\tilde{a}^{\langle t \rangle}_{1i}}) \\\\
\partial{\tilde{a}^{\langle t-1 \rangle}_{1i}} & \mathrel{{+}{=}}
\partial{\tilde{a}^{\langle t-1 \rangle(Loss)}_{1i}} +
\partial{\tilde{a}^{\langle t-1 \rangle(a_{out})}_{1i}} \\
& = (1 - a^{\langle t \rangle}_{1i} \times a^{\langle t \rangle}_{1i})
\times W_{aa11} \times (\partial{\tilde{z_{1i}}} \times W_{ya11} +
\partial{\tilde{a}^{\langle t \rangle}_{1i}}) \\\\
\partial{\tilde{W}_{ya}} & \mathrel{{+}{=}} \partial{\tilde{W}^{(Loss)}_{ya}} \\\\
\partial{\tilde{b}_{y}} & \mathrel{{+}{=}} \partial{\tilde{b}^{(Loss)}_{y}} \\\\
\end{align*}
<p>
The following paragraphs show the equation with fixed values for the dimensions
\(n_a\), \(n_y\), \(n_x\), but with no <code>for</code> loop anymore.
</p>

<p>
The following equation:
</p>
\begin{align*}
\partial{\tilde{W}_{ax11}} \mathrel{{ + } { = }}
(1 - a^{\langle t \rangle}_{1i} \times a^{\langle t \rangle}_{1i})
\times x^{\langle t \rangle}_{1i} \times (\partial{\tilde{z_{1i}}}
\times W_{ya11} + \partial{\tilde{a}^{\langle t \rangle}_{1i}})
\end{align*}
<p>
can be rewritten (for explanation purpose) as:
</p>
\begin{align*}
\partial{\tilde{W}_{ax11}} \mathrel{{+}{=}} h_i \times g_i
\times x^{\langle t \rangle}_{1i}
\end{align*}
<p>
with \(h_i = (1 - a^{\langle t \rangle}_{1i}
\times a^{\langle t \rangle}_{1i})\) and \(g_i = (\partial{\tilde{z_{1i}}}
\times W_{ya11} + \partial{\tilde{a}^{\langle t \rangle}_{1i}})\).
</p>

<p>
Let the \(\mathrel{{ + }{ = }}\) operator be replaced by \(=\) operator
in the equation below:
</p>
\begin{align*}
\partial{\tilde{W}_{ax11}} & = h_1 \times g_1
\times x^{\langle t \rangle}_{11} + h_2 \times g_2
\times x^{\langle t \rangle}_{12} \\
& + \dots \\
& + h_p \times g_p \times x^{\langle t \rangle}_{1p} \\
& + \dots \\
& + h_m \times g_m \times x^{\langle t \rangle}_{1m}
\end{align*}
<p>
and let it be expressed in a matrix form:
</p>
\begin{align*}
\partial{\tilde{W}_{ax11}} = [(1 - A^{\langle t \rangle}_{(1 \times m)}
\times A^{\langle t \rangle}_{(1 \times m)})
\times (\partial{\tilde{Z}_{(1 \times m)}} \times W_{ya11} +
\partial{\tilde{A}^{\langle t \rangle}_{(1 \times m)}})]
\cdot X^{\langle t \rangle\operatorname{T}}_{(1 \times m)}
\end{align*}
\begin{align*}
Z & =
\begin{bmatrix}
z_{11} & z_{12} & \dots & z_{1p} & \dots & z_{1m} \\
z_{21} & z_{22} & \dots & z_{2p} & \dots & z_{2m} \\
\dots \\
z_{p1} & z_{p2} & \dots & z_{pp} & \dots & z_{pm} \\
\dots \\
z_{n_y1} & z_{n_y2} & \dots & z_{n_yp} & \dots & z_{n_ym}
\end{bmatrix} \\\\
A^{\langle t \rangle} & =
\begin{bmatrix}
a^{\langle t \rangle}_{11} & a^{\langle t \rangle}_{12} &
\dots & a^{\langle t \rangle}_{1p} & \dots & a^{\langle t \rangle}_{1m} \\
a^{\langle t \rangle}_{21} & a^{\langle t \rangle}_{22} &
\dots & a^{\langle t \rangle}_{2p} & \dots & a^{\langle t \rangle}_{2m} \\
\dots \\
a^{\langle t \rangle}_{p1} & a^{\langle t \rangle}_{p2} &
\dots & a^{\langle t \rangle}_{pp} & \dots & a^{\langle t \rangle}_{pm} \\
\dots \\
a^{\langle t \rangle}_{n_a1} & a^{\langle t \rangle}_{n_a2} &
\dots & a^{\langle t \rangle}_{n_ap} & \dots & a^{\langle t \rangle}_{n_am}
\end{bmatrix} \\\\
X^{\langle t \rangle} & =
\begin{bmatrix}
x^{\langle t \rangle}_{11} & x^{\langle t \rangle}_{12} &
\dots & x^{\langle t \rangle}_{1p} & \dots & x^{\langle t \rangle}_{1m} \\
x^{\langle t \rangle}_{21} & x^{\langle t \rangle}_{22} &
\dots & x^{\langle t \rangle}_{2p} & \dots & x^{\langle t \rangle}_{2m} \\
\dots \\
x^{\langle t \rangle}_{p1} & x^{\langle t \rangle}_{p2} &
\dots & x^{\langle t \rangle}_{pp} & \dots & x^{\langle t \rangle}_{pm} \\
\dots \\
x^{\langle t \rangle}_{n_x1} & x^{\langle t \rangle}_{n_x2} &
\dots & x^{\langle t \rangle}_{n_xp} & \dots & x^{\langle t \rangle}_{n_xm}
\end{bmatrix}
\end{align*}
<p>
The other equations are defined as following:
</p>
\begin{align*}
\partial{\tilde{W}_{ya11}} & = \partial{\tilde{Z}_{(1 \times m)}}
\cdot A^{\langle t \rangle\operatorname{T}}_{(1 \times m)} \\\\
\partial{\tilde{W}_{ax11}} & = [(1 - A^{\langle t \rangle}_{(1 \times m)}
\times A^{\langle t \rangle}_{(1 \times m)})
\times (\partial{\tilde{Z}_{(1 \times m)}} \times W_{ya11} +
\partial{\tilde{A}^{\langle t \rangle}_{(1 \times m)}})]
\cdot X^{\langle t \rangle\operatorname{T}}_{(1 \times m)} \\\\
\partial{\tilde{W}_{aa11}} & = [(1 - A^{\langle t \rangle}_{(1 \times m)}
\times A^{\langle t \rangle}_{(1 \times m)})
\times (\partial{\tilde{Z}_{(1 \times m)}} \times W_{ya11} +
\partial{\tilde{A}^{\langle t \rangle}_{(1 \times m)}})]
\cdot A^{\langle t-1 \rangle\operatorname{T}}_{(1 \times m)} \\\\
\partial{\tilde{b}_{y11}} & = \sum \partial{\tilde{Z}_{(1 \times m)}} \\\\
\partial{\tilde{b}_{a11}} & \mathrel{{+}{=}}
(1 - A^{\langle t \rangle}_{(1 \times m)}
\times A^{\langle t \rangle}_{(1 \times m)})
\times (\partial{\tilde{Z}_{(1 \times m)}}
\times W_{ya11} + \partial{\tilde{A}^{\langle t \rangle}_{(1 \times m)}}) \\\\
\partial{\tilde{A}^{\langle t-1 \rangle}_{(1 \times m)}} & =
(1 - A^{\langle t \rangle}_{(1 \times m)}
\times A^{\langle t \rangle}_{(1 \times m)})
\times (\partial{\tilde{Z}_{(1 \times m)}} \times W_{ya11} +
\partial{\tilde{A}^{\langle t \rangle}_{(1 \times m)}}) \times W_{aa11}
\end{align*}
<p>
Let all equations be rewritten with non-fixed values for the dimensions
\(n_a\), \(n_y\) and \(n_x\):
</p>
\begin{align*}
\partial{\tilde{W}_{ya(n_y \times n_a)}} & =
\partial{\tilde{Z}_{(n_y \times m)}}
\cdot A^{\langle t \rangle\operatorname{T}}_{(n_a \times m)} \\\\
\partial{\tilde{b}_{y11(n_y \times 1)}} & = \sum
\partial{\tilde{Z}_{(n_y \times m)}} \\\\
\partial{\tilde{W}_{ax(n_a \times n_x)}} & =
[(1 - A^{\langle t \rangle}_{(n_a \times m)}
\times A^{\langle t \rangle}_{(n_a \times m)})
\times (W^{\operatorname{T}}_{ya(n_y \times n_a)} \cdot
\partial{\tilde{Z}_{(n_y \times m)}} +
\partial{\tilde{A}^{\langle t \rangle}_{(n_a \times m)}})]
\cdot X^{\langle t \rangle\operatorname{T}}_{(n_x \times m)} \\\\
\partial{\tilde{W}_{aa(n_a \times n_a)}} & =
[(1 - A^{\langle t \rangle}_{(n_a \times m)}
\times A^{\langle t \rangle}_{(n_a \times m)})
\times (W^{\operatorname{T}}_{ya(n_y \times n_a)}
\cdot \partial{\tilde{Z}_{(n_y \times m)}} +
\partial{\tilde{A}^{\langle t \rangle}_{(n_a \times m)}})]
\cdot A^{\langle t-1 \rangle\operatorname{T}}_{(n_a \times m)} \\\\
\partial{\tilde{b}_{a(n_a \times 1)}} & =
\sum [(1 - A^{\langle t \rangle}_{(n_a \times m)}
\times A^{\langle t \rangle}_{(n_a \times m)})
\times (W^{\operatorname{T}}_{ya(n_y \times n_a)}
\cdot \partial{\tilde{Z}_{(n_y \times m)}} +
\partial{\tilde{A}^{\langle t \rangle}_{(n_a \times m)}})] \\\\
\partial{\tilde{A}^{\langle t-1 \rangle}_{(n_a \times m)}} & =
W^{\operatorname{T}}_{aa(n_a \times n_a)}
\cdot [(1 - A^{\langle t \rangle}_{(n_a \times m)}
\times A^{\langle t \rangle}_{(n_a \times m)})
\times (W^{\operatorname{T}}_{ya(n_y \times n_a)}
\cdot \partial{\tilde{Z}_{(n_y \times m)}} +
\partial{\tilde{A}^{\langle t \rangle}_{(n_a \times m)}})]
\end{align*}
<p>
Finally, for convenient purpose let these equations be factorized
(below only \(\partial{\tilde{W}_{axn_an_x}}\) is described):
</p>
\begin{align*}
\partial{\tilde{W}_{axn_an_x}} & = (\partial{\tilde{u}_{(na \times m)}}
\times D_{out(n_a \times m)})
\cdot X^{\langle t \rangle\operatorname{T}}_{(n_x \times m)} \\\
\partial{\tilde{u}_{(n_a \times m)}} & =
\frac{\partial{A^{\langle t \rangle}}}{\partial{u}} \\
& = 1 - \partial{A^{\langle t \rangle}_{(n_a \times m)}}
\times \partial{A^{\langle t \rangle}_{(n_a \times m)}} \\\\
D_{out(n_a \times m)} & = W^{\operatorname{T}}_{ya(n_y \times n_a)}
\cdot \partial{\tilde{Z}_{(n_y \times m)}} +
\partial{\tilde{A}^{\langle t \rangle}_{(n_a \times m)}}
\end{align*}
<p>
The final step is to accumulate derivatives across each cell
(1 to \(T_x\), respectively \(T_y\)). \(A\) elements are not accumulated because
they are received from the next cells thanks to backward propagation cache.
This mechanism is called backward propagation through time.
The formula is given at <a href="#doc-backward-prop-rnn">doc-backward-prop-rnn</a>.
In the code block below, the equations with the form
\(\partial{\tilde{H}_{(k \times p)}}\) are written <code>dh</code>.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">rnn_cell_backward</span> [dy params xt_curr at_curr at_prev dat_next_cell]
  (<span class="org-builtin">setv</span> <span class="org-variable-name">du</span> (<span class="org-builtin">-</span> 1 (<span class="org-builtin">**</span> at_curr 2)))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">dout</span> (<span class="org-builtin">+</span> (np.dot (np.transpose (<span class="org-builtin">get</span> params <span class="org-string">"Wya"</span>)) dy) dat_next_cell))

  (<span class="org-builtin">setv</span> <span class="org-variable-name">dWya</span> (np.dot dy (np.transpose at_curr)))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">dby</span> (np.sum dy <span class="org-constant">:axis</span> 1 <span class="org-constant">:keepdims</span> <span class="org-constant">True</span>))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">dWax</span> (np.dot (<span class="org-builtin">*</span> du dout) (np.transpose xt_curr)))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">dWaa</span> (np.dot (<span class="org-builtin">*</span> du dout) (np.transpose at_prev)))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">dba</span> (np.sum (<span class="org-builtin">*</span> du dout) <span class="org-constant">:axis</span> 1 <span class="org-constant">:keepdims</span> <span class="org-constant">True</span>))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">dat_next_cell</span> (np.dot (np.transpose (<span class="org-builtin">get</span> params <span class="org-string">"Waa"</span>)) (<span class="org-builtin">*</span> du dout)))

  [dWya dby dWax dWaa dba dat_next_cell])

(<span class="org-keyword">defn</span> <span class="org-function-name">rnn_backward</span> [X Y params cache]
  (<span class="org-builtin">setv</span> [Yhat_onehot A_onehot X_onehot] cache)
  (<span class="org-keyword">loop</span> [[i (<span class="org-builtin">-</span> (<span class="org-builtin">len</span> X) 1)]
         [gradients {<span class="org-string">"dWya"</span> (np.zeros_like (<span class="org-builtin">get</span> params <span class="org-string">"Wya"</span>))
                     <span class="org-string">"dby"</span> (np.zeros_like (<span class="org-builtin">get</span> params <span class="org-string">"by"</span>))
                     <span class="org-string">"dWax"</span> (np.zeros_like (<span class="org-builtin">get</span> params <span class="org-string">"Wax"</span>))
                     <span class="org-string">"dWaa"</span> (np.zeros_like (<span class="org-builtin">get</span> params <span class="org-string">"Waa"</span>))
                     <span class="org-string">"dba"</span> (np.zeros_like (<span class="org-builtin">get</span> params <span class="org-string">"ba"</span>))
                     <span class="org-string">"dat_next_cell"</span> (np.zeros_like (<span class="org-builtin">get</span> A_onehot 0))}]]
        (<span class="org-keyword">if</span> (<span class="org-builtin">&lt;</span> i 0)
            [gradients A_onehot]
            (<span class="org-keyword">do</span>
              (<span class="org-builtin">setv</span> <span class="org-variable-name">yhat_curr</span> (<span class="org-builtin">get</span> Yhat_onehot i))
              (<span class="org-builtin">setv</span> <span class="org-variable-name">idx_curr</span> (<span class="org-builtin">get</span> Y i))
              (<span class="org-builtin">setv</span> [dWya_cell
                     dby_cell
                     dWax_cell
                     dWaa_cell
                     dba_cell
                     dat_next_cell]
                    (rnn_cell_backward
                      (clj_assoc
                        yhat_curr
                        idx_curr
                        (<span class="org-builtin">-</span> (<span class="org-builtin">get</span> yhat_curr idx_curr) 1))
                      params
                      (<span class="org-builtin">get</span> X_onehot i)
                      (<span class="org-builtin">get</span> A_onehot i)
                      (<span class="org-builtin">get</span> A_onehot (<span class="org-builtin">-</span> i 1))
                      (<span class="org-builtin">get</span> gradients <span class="org-string">"dat_next_cell"</span>)))
              (<span class="org-keyword">recur</span> (<span class="org-builtin">dec</span> i)
                     <span class="org-comment-delimiter">;; </span><span class="org-comment">HACK: normally assoc function can update several values:</span>
                     <span class="org-comment-delimiter">;; </span><span class="org-comment">(assoc map key val &amp; kvs)</span>
                     <span class="org-comment-delimiter">;; </span><span class="org-comment">assoc is a macro, and checks args before function call</span>
                     <span class="org-comment-delimiter">;; </span><span class="org-comment">call assoc function in clj_assoc with</span>
                     <span class="org-comment-delimiter">;; </span><span class="org-comment">&amp; kvs args doesn't work...</span>
                     (<span class="org-keyword">-&gt;</span> gradients
                         (clj_assoc <span class="org-string">"dWya"</span> (<span class="org-builtin">+</span> (<span class="org-builtin">get</span> gradients <span class="org-string">"dWya"</span>) dWya_cell))
                         (clj_assoc <span class="org-string">"dby"</span> (<span class="org-builtin">+</span> (<span class="org-builtin">get</span> gradients <span class="org-string">"dby"</span>) dby_cell))
                         (clj_assoc <span class="org-string">"dWax"</span> (<span class="org-builtin">+</span> (<span class="org-builtin">get</span> gradients <span class="org-string">"dWax"</span>) dWax_cell))
                         (clj_assoc <span class="org-string">"dWaa"</span> (<span class="org-builtin">+</span> (<span class="org-builtin">get</span> gradients <span class="org-string">"dWaa"</span>) dWaa_cell))
                         (clj_assoc <span class="org-string">"dba"</span> (<span class="org-builtin">+</span> (<span class="org-builtin">get</span> gradients <span class="org-string">"dba"</span>) dba_cell))
                         (clj_assoc <span class="org-string">"dat_next_cell"</span> dat_next_cell)))))))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-rnn_backward</span> []
  (np.random.seed 1)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_na</span> 2)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_Wax</span> (np.random.randn test_na vocab_size))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_Waa</span> (np.random.randn test_na test_na))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_Wya</span> (np.random.randn vocab_size test_na))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_ba</span> (np.random.randn test_na 1))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_by</span> (np.random.randn vocab_size 1))
  (<span class="org-builtin">setv</span> [test_X test_Y] (init_train_data name_lines 15))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_at_0</span> (np.random.randn test_na 1))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_params</span> {<span class="org-string">"Wax"</span> test_Wax
                     <span class="org-string">"Waa"</span> test_Waa
                     <span class="org-string">"Wya"</span> test_Wya
                     <span class="org-string">"ba"</span> test_ba
                     <span class="org-string">"by"</span> test_by})
  (<span class="org-builtin">setv</span> [test_loss test_cache] (rnn_forward
                                 test_X
                                 test_Y
                                 test_at_0
                                 test_params))
  (<span class="org-builtin">setv</span> [test_gradients test_at_curr] (rnn_backward
                                        test_X
                                        test_Y
                                        test_params
                                        test_cache))

  (<span class="org-keyword">assert</span> (np.isclose (<span class="org-builtin">get</span> (<span class="org-builtin">sum</span> (<span class="org-builtin">sum</span> (test_at_curr.values))) 0) 6.40218838))
  (<span class="org-keyword">assert</span> (np.isclose (.sum (<span class="org-builtin">get</span> test_gradients <span class="org-string">"dWya"</span>)) -1.11022e-16))
  (<span class="org-keyword">assert</span> (np.isclose (.sum (<span class="org-builtin">get</span> test_gradients <span class="org-string">"dby"</span>)) -9.992e-16))
  (<span class="org-keyword">assert</span> (np.isclose (.sum (<span class="org-builtin">get</span> test_gradients <span class="org-string">"dWax"</span>)) 4.526283436))
  (<span class="org-keyword">assert</span> (np.isclose (.sum (<span class="org-builtin">get</span> test_gradients <span class="org-string">"dWaa"</span>)) 2.360181104))
  (<span class="org-keyword">assert</span> (np.isclose (.sum (<span class="org-builtin">get</span> test_gradients <span class="org-string">"dba"</span>)) 4.514303068)))
(test-case (test-rnn_backward))
</pre>
</div>
</div>
</li>
<li><a id="sec-3-10-2-3-3" name="sec-3-10-2-3-3"></a>Gradient Descent<br ><div class="outline-text-6" id="text-3-10-2-3-3">
<p>
Implementation of the gradient clipping method.
Even with \(\tanh\) function over \(\operatorname{ReLU}\),
exploding gradients still occurs.
It occurs when the gradient values grow extremely large,
and it causes an overflow (NaN).
Gradient clipping method  is used to <i>clip</i> the gradients to a threshold
value to prevent the gradients from getting too large.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">grads_clip</span> [gradients treshold]
  (<span class="org-keyword">dfor</span> [k val] (gradients.items)
        [k (np.clip val (<span class="org-builtin">-</span> treshold) treshold)]))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-grads_clip</span> []
  (np.random.seed 1)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_gradients</span> {<span class="org-string">"dWax"</span> (<span class="org-builtin">*</span> (np.random.randn 5 3) 10)
                        <span class="org-string">"dWaa"</span> (<span class="org-builtin">*</span> (np.random.randn 5 5) 10)
                        <span class="org-string">"dWya"</span> (<span class="org-builtin">*</span> (np.random.randn 2 5) 10)
                        <span class="org-string">"dba"</span>(<span class="org-builtin">*</span> (np.random.randn 5 1) 10)
                        <span class="org-string">"dby"</span>(<span class="org-builtin">*</span> (np.random.randn 2 1) 10)})

  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_res_clip</span> (grads_clip <span class="org-constant">:gradients</span> test_gradients <span class="org-constant">:treshold</span> 10))

  (<span class="org-keyword">assert</span> (np.isclose (.mean (<span class="org-builtin">get</span> test_res_clip <span class="org-string">"dWax"</span>)) -0.448353517))
  (<span class="org-keyword">assert</span> (np.isclose (.mean (<span class="org-builtin">get</span> test_res_clip <span class="org-string">"dWaa"</span>)) -1.1713921118))
  (<span class="org-keyword">assert</span> (np.isclose (.mean (<span class="org-builtin">get</span> test_res_clip <span class="org-string">"dWya"</span>)) 0.5154668448))
  (<span class="org-keyword">assert</span> (np.isclose (.mean (<span class="org-builtin">get</span> test_res_clip <span class="org-string">"dba"</span>)) -3.22063296))
  (<span class="org-keyword">assert</span> (np.isclose (.mean (<span class="org-builtin">get</span> test_res_clip <span class="org-string">"dby"</span>)) 7.128033025)))
(test-case (test-grads_clip))
</pre>
</div>


<p>
Sampling at character-level for one RNN cell
(gray arrows in figure <a href="#fig:rnn_archi.png">45</a>).
It's possible to check, during the learning process, what the model
has <i>just</i> learned by apply a sample character sequence.
This function generates one name by running \(N\) steps of <code>rnn_cell_forward</code>,
using what the model has <i>just</i> learned.
</p>
\begin{align*}
x^{\langle t \rangle} =
\begin{bmatrix}
0 \\
0 \\
\dots \\
1 \\
\dots \\
0
\end{bmatrix}
\end{align*}
<p>
\(x^{\langle t \rangle}\) is a one-hot vector of
\(\dim x^{\langle t \rangle} =\) <code>vocab_size</code> \(\times 1\)
which represents a single character.
This character is chosen according to the \(y^{\langle t \rangle}\) prediction,
thus \(y^{\langle t \rangle} = x^{\langle t+1 \rangle}\)
</p>
\begin{align*}
\hat{y}^{\langle t \rangle} =
\begin{bmatrix}
0.1 \\
0 \\
\dots \\
0.7 \\
\dots \\
0.2
\end{bmatrix}
\end{align*}
<p>
\(\hat{y}^{\langle t \rangle}\) contains the probability distribution returned
by <code>rnn_cell_forward</code>.
It means that each entry has a probability associated, and the sum of all of
them is equal to 1.
Furthermore, if \(\hat{y}^{\langle t \rangle}_{i} = 0.15\), then the probability
that x<sup>&lang; t+1 &rang;</sup> picks the character at index \(i\) is 15%.
In the code block below, this is implemented with both <code>np.random.choice</code>
and <code>ravel</code> functions.
For example, with at each step
\(t \in [1, T_x]\), \(y^{\langle t \rangle} = x^{\langle t+1 \rangle}\),
and \(x^{\langle 1 \rangle}\) the zero vector: <br >
First step: \(t = 1\) <br >
\(a^{\langle 0 \rangle} = 0\) <br >
\(x^{\langle 1 \rangle} = 0\) <br >
\(a^{\langle 1 \rangle} =
w_{ax} \cdot x^{\langle 1 \rangle} + w_{aa} \cdot a^{\langle 0 \rangle} + b_a\) <br >
\(\hat{y}^{\langle 1 \rangle} =
\operatorname{softmax}(W_{ya} \cdot a^{\langle 1 \rangle} + b_y)\) <br >
\(\mathcal{L}(y^{\langle 1 \rangle}, \hat{y}^{\langle 1 \rangle})\),
with a well-trained model \(y^{\langle 1 \rangle} = \hat{y}^{\langle 1 \rangle}\) <br >
\(y^{\langle 1 \rangle} = a\) <br >
Second step: \(t = 2\) <br >
\(a^{\langle 1 \rangle} =\) value from step 1 <br >
\(x^{\langle 2 \rangle} = a\) <br >
\(a^{\langle 2 \rangle} = w_{ax} \cdot x^{\langle 2 \rangle} + w_{aa} \cdot a^{\langle 1 \rangle} + b_a\) <br >
\(\hat{y}^{\langle 2 \rangle} = \operatorname{softmax}(W_{ya} \cdot a^{\langle 2 \rangle} + b_y)\) <br >
\(\mathcal{L}(y^{\langle 2 \rangle}, \hat{y}^{\langle 2 \rangle})\),
with a well-trained model \(y^{\langle 2 \rangle} = \hat{y}^{\langle 2 \rangle}\) <br >
\(y^{\langle 2 \rangle} = s\) <br >
Third step: \(t = 3\) <br >
\(a^{\langle 2 \rangle} =\) value from step 2 <br >
\(x^{\langle 3 \rangle} = s\) <br >
\(a^{\langle 3 \rangle} = w_{ax} \cdot x^{\langle 3 \rangle} + w_{aa} \cdot a^{\langle 2 \rangle} + b_a\) <br >
\(\hat{y}^{\langle 3 \rangle} = \operatorname{softmax}(W_{ya} \cdot a^{\langle 3 \rangle} + b_y)\) <br >
\(\mathcal{L}(y^{\langle 3 \rangle}, \hat{y}^{\langle 3 \rangle})\),
with a well-trained model \(y^{\langle 3 \rangle} = \hat{y}^{\langle 3 \rangle}\) <br >
\(y^{\langle 3 \rangle} = t\) <br >
\(t\) step: \(t = t\) <br >
\(a^{\langle t-1 \rangle} =\) value from step \(t-1\) <br >
\(x^{\langle t \rangle} = r\) <br >
\(a^{\langle t \rangle} = w_{ax} \cdot x^{\langle t \rangle} + w_{aa} \cdot a^{\langle t-1 \rangle} + b_a\) <br >
\(\hat{y}^{\langle t \rangle} = \operatorname{softmax}(W_{ya} \cdot a^{\langle t \rangle} + b_y)\) <br >
\(\mathcal{L}(y^{\langle t \rangle}, \hat{y}^{\langle t \rangle})\),
with a well-trained model \(y^{\langle t \rangle} = \hat{y}^{\langle t \rangle}\) <br >
\(y^{\langle t \rangle} = i\) <br >
The last step: \(t = T_x = T_y =\) <code>len(name)</code> <br >
\(a^{\langle T_x-1 \rangle} =\) value from step \(T_x-1\) <br >
\(x^{\langle T_x \rangle} = x\) <br >
\(a^{\langle T_x \rangle} = w_{ax} \cdot x^{\langle t \rangle} + w_{aa} \cdot a^{\langle T_x-1 \rangle} + b_a\) <br >
\(\hat{y}^{\langle T_x \rangle} = \operatorname{softmax}(W_{ya} \cdot a^{\langle T_x \rangle} + b_y)\) <br >
\(\mathcal{L}(y^{\langle T_x \rangle}, \hat{y}^{\langle T_x \rangle})\), with a well-trained model \(y^{\langle T_x \rangle} = \hat{y}^{\langle T_x \rangle}\) <br >
\(y^{\langle T_x \rangle} = eol\) <br >
Let the name be equal to <i>asterix</i>, then <br >
\(X = \begin{bmatrix} x^{\langle 1 \rangle} & x^{\langle 2 \rangle} & x^{\langle 3 \rangle} & x^{\langle 4 \rangle} & x^{\langle 5 \rangle} & x^{\langle 6 \rangle} & x^{\langle 7 \rangle} & x^{\langle 8 \rangle} \end{bmatrix}\) <br >
\(x^{\langle 1 \rangle} = \emptyset\) <br >
\(x^{\langle 2 \rangle} = a\) <br >
\(x^{\langle 3 \rangle} = s\) <br >
\(x^{\langle 4 \rangle} = t\) <br >
\(x^{\langle 5 \rangle} = e\) <br >
\(x^{\langle 6 \rangle} = r\) <br >
\(x^{\langle 7 \rangle} = i\) <br >
\(x^{\langle 8 \rangle} = x\) <br >
\(Y = \begin{bmatrix} y^{\langle 1 \rangle} & y^{\langle 2 \rangle} & y^{\langle 3 \rangle} & y^{\langle 4 \rangle} & y^{\langle 5 \rangle} & y^{\langle 6 \rangle} & y^{\langle 7 \rangle} & y^{\langle 8 \rangle} \end{bmatrix}\) (expected result) <br >
\(y^{\langle 1 \rangle} = a\) <br >
\(y^{\langle 2 \rangle} = s\) <br >
\(y^{\langle 3 \rangle} = t\) <br >
\(y^{\langle 4 \rangle} = e\) <br >
\(y^{\langle 5 \rangle} = r\) <br >
\(y^{\langle 6 \rangle} = i\) <br >
\(y^{\langle 7 \rangle} = x\) <br >
\(y^{\langle 8 \rangle} = eol\)
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">sample_char_level</span> [params seed]
  (np.random.seed seed)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">word_maxsize</span> 50)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">eol</span> (<span class="org-builtin">get</span> char_to_ind <span class="org-string">"\n"</span>))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">na</span> (<span class="org-builtin">get</span> (. (<span class="org-builtin">get</span> params <span class="org-string">"Waa"</span>) shape) 1))

  (<span class="org-keyword">loop</span> [[word_size 0]
         [ind -1]
         [xt_curr (np.zeros [vocab_size 1])]
         [at_prev (np.zeros [na 1])]
         [inds_char_generate []]]
        (<span class="org-keyword">if</span> (<span class="org-keyword">or</span> (<span class="org-builtin">=</span> ind eol) (<span class="org-builtin">=</span> word_size word_maxsize))
            (<span class="org-keyword">if</span> (<span class="org-builtin">=</span> word_size word_maxsize)
                (<span class="org-builtin">+</span> inds_char_generate [eol])
                inds_char_generate)
            (<span class="org-keyword">do</span>
              (<span class="org-builtin">setv</span> [at_curr ythat_curr] (rnn_cell_forward
                                           xt_curr
                                           at_prev params))
              (<span class="org-builtin">setv</span> <span class="org-variable-name">ind_next</span> (np.random.choice
                               (<span class="org-builtin">range</span> vocab_size)
                               <span class="org-constant">:p</span> (ythat_curr.ravel)))
              (<span class="org-keyword">recur</span>
                (<span class="org-builtin">inc</span> word_size)
                ind_next
                (clj_assoc (np.zeros [vocab_size 1]) ind_next 1)
                at_curr
                (<span class="org-builtin">+</span> inds_char_generate [ind_next]))))))

(<span class="org-keyword">defn</span> <span class="org-function-name">print_sample</span> [sample_inds]
  (<span class="org-builtin">setv</span> <span class="org-variable-name">txt_raw</span> (.join <span class="org-string">""</span> (<span class="org-keyword">lfor</span> i sample_inds (<span class="org-builtin">get</span> ind_to_char i))))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">txt</span> (<span class="org-builtin">+</span> (.upper (<span class="org-builtin">get</span> txt_raw 0)) (<span class="org-builtin">get</span> txt_raw (<span class="org-builtin">slice</span> 1 <span class="org-constant">None</span>))))
  (<span class="org-keyword">print</span> txt <span class="org-constant">:end</span> <span class="org-string">""</span>))


(<span class="org-keyword">defn</span> <span class="org-function-name">sample_name_level</span> [params num_name_sample seed]
  (<span class="org-keyword">for</span> [i (<span class="org-builtin">range</span> num_name_sample)]
    (print_sample (sample_char_level params <span class="org-constant">:seed</span> (<span class="org-builtin">+</span> i seed)))))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-sample_char_level</span> []
  (np.random.seed 1)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_na</span> 20)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_Wax</span> (np.random.randn test_na vocab_size))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_Waa</span> (np.random.randn test_na test_na))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_Wya</span> (np.random.randn vocab_size test_na))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_ba</span> (np.random.randn test_na 1))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_by</span> (np.random.randn vocab_size 1))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_inds_char_gen</span> (sample_char_level
                             {<span class="org-string">"Wax"</span> test_Wax
                              <span class="org-string">"Waa"</span> test_Waa
                              <span class="org-string">"Wya"</span> test_Wya
                              <span class="org-string">"ba"</span> test_ba
                              <span class="org-string">"by"</span> test_by}
                             <span class="org-constant">:seed</span> 1))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (<span class="org-builtin">sum</span> test_inds_char_gen) 43)))
(test-case (test-sample_char_level))

(<span class="org-keyword">defn</span> <span class="org-function-name">test-sample_name_level</span> []
  (np.random.seed 1)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_na</span> 20)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_Wax</span> (np.random.randn test_na vocab_size))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_Waa</span> (np.random.randn test_na test_na))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_Wya</span> (np.random.randn vocab_size test_na))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_ba</span> (np.random.randn test_na 1))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_by</span> (np.random.randn vocab_size 1))
  (sample_name_level <span class="org-constant">:params</span> {<span class="org-string">"Wax"</span> test_Wax
                              <span class="org-string">"Waa"</span> test_Waa
                              <span class="org-string">"Wya"</span> test_Wya
                              <span class="org-string">"ba"</span> test_ba
                              <span class="org-string">"by"</span> test_by}
                     <span class="org-constant">:num_name_sample</span> 5
                     <span class="org-constant">:seed</span> 1))
(test-case (test-sample_name_level))
</pre>
</div>


<p>
Implement the gradient descent algorithm as following:
</p>
<ul class="org-ul">
<li>Forward propagate through RNN cell to compute the loss
</li>
<li>Backward propagate to compute the gradients of the loss with respect to the parameters (weights, bias, <i>a</i>)
</li>
<li>Clip the gradients
</li>
<li>Update the parameters
</li>
</ul>
<p>
This function is called on each Asterix character name,
and <code>at_prev</code>, <code>params</code> come from the previous iteration.
Each name could be viewed like one training example.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">update_params</span> [params gradients lr]
  {<span class="org-string">"Wax"</span> (<span class="org-builtin">-</span> (<span class="org-builtin">get</span> params <span class="org-string">"Wax"</span>) (<span class="org-builtin">*</span> lr (<span class="org-builtin">get</span> gradients <span class="org-string">"dWax"</span>)))
   <span class="org-string">"Waa"</span> (<span class="org-builtin">-</span> (<span class="org-builtin">get</span> params <span class="org-string">"Waa"</span>) (<span class="org-builtin">*</span> lr (<span class="org-builtin">get</span> gradients <span class="org-string">"dWaa"</span>)))
   <span class="org-string">"Wya"</span> (<span class="org-builtin">-</span> (<span class="org-builtin">get</span> params <span class="org-string">"Wya"</span>) (<span class="org-builtin">*</span> lr (<span class="org-builtin">get</span> gradients <span class="org-string">"dWya"</span>)))
   <span class="org-string">"ba"</span> (<span class="org-builtin">-</span> (<span class="org-builtin">get</span> params <span class="org-string">"ba"</span>) (<span class="org-builtin">*</span> lr (<span class="org-builtin">get</span> gradients <span class="org-string">"dba"</span>)))
   <span class="org-string">"by"</span> (<span class="org-builtin">-</span> (<span class="org-builtin">get</span> params <span class="org-string">"by"</span>) (<span class="org-builtin">*</span> lr (<span class="org-builtin">get</span> gradients <span class="org-string">"dby"</span>)))})

(<span class="org-keyword">defn</span> <span class="org-function-name">gradient_descent</span> [X Y at_prev params learning_rate]
  (<span class="org-builtin">setv</span> [loss cache] (rnn_forward X Y at_prev params))
  (<span class="org-builtin">setv</span> [gradients at_curr] (rnn_backward X Y params cache))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">gradients_clip</span> (grads_clip gradients 5))
  [loss
   gradients_clip
   (<span class="org-builtin">get</span> at_curr (<span class="org-builtin">-</span> (<span class="org-builtin">len</span> X) 1))
   (update_params params gradients_clip learning_rate)])
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-gradient_descent</span> []
  (np.random.seed 1)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_na</span> 100)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_Wax</span> (np.random.randn test_na vocab_size))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_Waa</span> (np.random.randn test_na test_na))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_Wya</span> (np.random.randn vocab_size test_na))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_ba</span> (np.random.randn test_na 1))
  (<span class="org-builtin">setv</span> <span class="org-variable-name">test_by</span> (np.random.randn vocab_size 1))

  (<span class="org-builtin">setv</span> [test_loss test_gradients test_at_last test_params]
        (gradient_descent <span class="org-constant">:X</span> [12 3 5 11 22 3]
                          <span class="org-constant">:Y</span> [4 14 11 22 25 26]
                          <span class="org-constant">:at_prev</span> (np.random.randn test_na 1)
                          <span class="org-constant">:params</span> {<span class="org-string">"Wax"</span> test_Wax
                                   <span class="org-string">"Waa"</span> test_Waa
                                   <span class="org-string">"Wya"</span> test_Wya
                                   <span class="org-string">"ba"</span> test_ba
                                   <span class="org-string">"by"</span> test_by}
                          <span class="org-constant">:learning_rate</span> 0.01))

  (<span class="org-keyword">assert</span> (np.isclose test_loss 95.3937196))
  (<span class="org-keyword">assert</span> (np.isclose (test_at_last.sum) 5.70797310))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> (np.argmax (<span class="org-builtin">get</span> test_gradients <span class="org-string">"dWax"</span>)) 530))
  (<span class="org-keyword">assert</span> (np.isclose (.sum (<span class="org-builtin">get</span> test_params <span class="org-string">"Wax"</span>)) 43.90712236)))
(test-case (test-gradient_descent))
</pre>
</div>
</div>
</li></ol>
</li>
<li><a id="sec-3-10-2-4" name="sec-3-10-2-4"></a>Model<br ><div class="outline-text-5" id="text-3-10-2-4">
<p>
Implement the RNN model.
It uses the dataset of Asterix names, and each line of the dataset
(one name) corresponds to one training example.
Every 2000 iterations, the model samples 10 randomly names according to
the parameters calculated by the gradient descent.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">model_rnn</span> [na nb_iter num_name_sample seed]
  (np.random.seed seed)
  (<span class="org-builtin">setv</span> <span class="org-variable-name">name_lines_shuffle</span> (clj_shuffle name_lines))
  (<span class="org-keyword">loop</span> [[i 0]
         [at_prev (np.zeros [na 1])]
         [params (init_params <span class="org-constant">:na</span> na <span class="org-constant">:nx</span> vocab_size <span class="org-constant">:ny</span> vocab_size <span class="org-constant">:seed</span> 1)]
         [loss 0]
         [grads 0]]
        (<span class="org-keyword">if</span> (<span class="org-builtin">&gt;=</span> i nb_iter)
            [at_prev params loss grads]
            (<span class="org-keyword">do</span>
              (<span class="org-builtin">setv</span> [X Y] (init_train_data name_lines_shuffle
                                           (<span class="org-builtin">%</span> i (<span class="org-builtin">len</span> name_lines_shuffle))))
              (<span class="org-builtin">setv</span> [loss gradients at_curr params_curr]
                    (gradient_descent X Y at_prev params 0.01))
              (<span class="org-keyword">when</span> (<span class="org-builtin">=</span> (<span class="org-builtin">%</span> i 2000) 0)
                (<span class="org-keyword">print</span> (<span class="org-builtin">%</span> <span class="org-string">"--- Iteration: %d Loss: %f --- \n"</span> (<span class="org-builtin">,</span> i loss)))
                (sample_name_level params num_name_sample <span class="org-constant">:seed</span> i)
                (<span class="org-keyword">print</span> <span class="org-string">"\n"</span>))
              (<span class="org-keyword">recur</span>
                (<span class="org-builtin">inc</span> i)
                at_curr
                params_curr
                loss
                gradients)))))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-model</span> []
  (<span class="org-builtin">setv</span> [test_at_prev test_params test_loss test_grads]
        (model_rnn <span class="org-constant">:na</span> 50 <span class="org-constant">:nb_iter</span> 12001 <span class="org-constant">:num_name_sample</span> 10 <span class="org-constant">:seed</span> 1))

  (<span class="org-keyword">assert</span> (np.isclose (.mean test_at_prev) -0.032665076))
  (<span class="org-keyword">assert</span> (np.isclose (.mean (<span class="org-builtin">get</span>  test_params <span class="org-string">"Wax"</span>)) 0.0024636273))
  (<span class="org-keyword">assert</span> (np.isclose (.mean (<span class="org-builtin">get</span>  test_params <span class="org-string">"Waa"</span>)) 0.0004596987))
  (<span class="org-keyword">assert</span> (np.isclose (.mean (<span class="org-builtin">get</span>  test_params <span class="org-string">"Wya"</span>)) -1.686e-05))
  (<span class="org-keyword">assert</span> (np.isclose (.mean (<span class="org-builtin">get</span>  test_params <span class="org-string">"ba"</span>)) 0.0110672))
  (<span class="org-keyword">assert</span> (np.isclose (.mean (<span class="org-builtin">get</span>  test_params <span class="org-string">"by"</span>)) 6.481e-16))
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> test_loss 27.68856230591735))
  (<span class="org-keyword">assert</span> (np.isclose (.mean (<span class="org-builtin">get</span> test_grads <span class="org-string">"dWya"</span>)) 7.68e-18))
  (<span class="org-keyword">assert</span> (np.isclose (.mean (<span class="org-builtin">get</span> test_grads <span class="org-string">"dWaa"</span>)) 0.013509309))
  (<span class="org-keyword">assert</span> (np.isclose (.mean (<span class="org-builtin">get</span> test_grads <span class="org-string">"dba"</span>)) -0.026696898))
  (<span class="org-keyword">assert</span> (np.isclose (.mean (<span class="org-builtin">get</span> test_grads <span class="org-string">"dby"</span>)) -2.006e-17)))
(test-case (test-model))
</pre>
</div>
</div>
</li></ol>
</div>
<div id="outline-container-sec-3-10-3" class="outline-4">
<h4 id="sec-3-10-3"><span class="section-number-4">3.10.3</span> Results</h4>
<div class="outline-text-4" id="text-3-10-3">
<p>
At the first step the model generates some <i>random</i>
names as <i>Géfîquàrieazodéinivtuhihkdeøøe</i>, <i>Éøérg</i>
but at the end, it generates cool names as
<i>Delix</i>, <i>Olenonrix</i>, <i>Abélus</i>, <i>Linus</i>, and <i>Tintis</i>.
Moreover, between iterations 6000 - 10000 there is an oscillation of the
loss function output, because at each loop iteration both <code>X</code> and <code>Y</code>
are initialized with random values from the list of all names, nevertheless
the model learns well.
</p>
<div class="org-src-container">

<pre class="src src-hy">(model_rnn <span class="org-constant">:na</span> 50 <span class="org-constant">:nb_iter</span> 12001 <span class="org-constant">:num_name_sample</span> 10 <span class="org-constant">:seed</span> 1)
</pre>
</div>


<pre class="example">
--- Iteration: 0 Loss: 36.109132 ---

Syusnvoéømârtïab
Ny
O
Syiqîîcf o owixt
Øsøyxfû
Géfîquàrieazodéinivtuhihkdeøøe
Îkã burnkvozrtvûãnéãayâzyscømgyûhwuyïlhmzynûolybmà
Aàoyûrqahqxâmaiîfoï
Éøérg



--- Iteration: 2000 Loss: 26.568139 ---

Nlinfidige cerix
Baxus
Penix
Lingustsukufin
Osintrléranis
Czatus roclorisus
Dérix
Olix
Eahcolibzes
Atcocex


--- Iteration: 4000 Loss: 29.127063 ---

Orairalfus vonba
Fpélius bamolus
Lourdalanosîius
Lolus ududrius
Baàinus
Rionabontuépannh
Miachalaupus cabalautaliétaris

Opintolue basmilius solommrix
Hérllellofrix


--- Iteration: 6000 Loss: 43.018524 ---

Ecseurfebus
Mandir
Rasimumontus
Scoguserbus
Laugéopus casiudus
Lus chtuf claudicloliuc
Férie
Olenonrix
Courine ccalomix
Bésinlmix


--- Iteration: 8000 Loss: 14.381595 ---

Rongextonusvrabilacususacopinepingadonineshchuinix
Eraloniatauzix
Aplepus
Lauronis
Pemcakonus
Pinedecacix
Ueplatracus
Linus
Nantyeldix
Afcteninoigue


--- Iteration: 10000 Loss: 20.788600 ---

Leunnclazdix
Pauqfus
Ius eulutus
Six
Beucius
Abélus
Sius euenosla
Bouogileseméfine
Sarcanggmaigelmuv toudius
Bius eurus as y


--- Iteration: 12000 Loss: 27.688562 ---

Delix
Catorius mancoruf
Jelaumix
Fététus
Tintis
Netrocémérile
Leurix
Brius
Inus
Calograc
</pre>
</div>
</div>
<div id="outline-container-sec-3-10-4" class="outline-4">
<h4 id="sec-3-10-4"><span class="section-number-4">3.10.4</span> References</h4>
<div class="outline-text-4" id="text-3-10-4">
<ul class="org-ul">
<li>deeplearning.ai. 2020. <a href="https://www.deeplearning.ai/deep-learning-specialization/">Deep Learning Specialization</a>.
</li>
<li>Les Editions Albert René. 2020. <a href="https://www.asterix.com/les-personnages/">Asterix, Les Personnages</a>.
</li>
<li>Bhutani Sanyam. 2017. <a href="https://medium.com/hackernoon/gradient-clipping-57f04f0adae">Gradient Clipping</a>.
</li>
<li>Google Team. 2015. <a href="https://arxiv.org/pdf/1504.00941.pdf">A Simple Way to Initialize Recurrent Networks of Rectified Linear Units</a>.
</li>
<li>Bahumi Roei. 2019. <a href="http://machinelearningmechanic.com/deep_learning/2019/09/04/cross-entropy-loss-derivative.html">Cross Entropy Loss Derivative</a>.
</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-sec-3-11" class="outline-3">
<h3 id="sec-3-11"><span class="section-number-3">3.11</span> Text Classification Using TensorFlow</h3>
<div class="outline-text-3" id="text-3-11">

<figure>
<p><img src="./export/img/tc.png" class="img-responsive" alt="tc.png" width="480">
</p>
</figure>
</div>
<div id="outline-container-sec-3-11-1" class="outline-4">
<h4 id="sec-3-11-1"><span class="section-number-4">3.11.1</span> Purpose</h4>
<div class="outline-text-4" id="text-3-11-1">
<p>
The objective is to build a sentiment classification using a
Recurrent Neural Network with TensorFlow.
The text classification is trained on the <i>Large Movie Review Dataset</i>
for sentiment analysis.
</p>
</div>
</div>
<div id="outline-container-sec-3-11-2" class="outline-4">
<h4 id="sec-3-11-2"><span class="section-number-4">3.11.2</span> Implementation</h4>
<div class="outline-text-4" id="text-3-11-2">
</div><ol class="org-ol"><li><a id="sec-3-11-2-1" name="sec-3-11-2-1"></a>Initialization<br ><div class="outline-text-5" id="text-3-11-2-1">
<p>
Import and configure modules.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">require</span> [hytmf [test-case]])
(<span class="org-keyword">import</span> [tensorflow <span class="org-keyword">:as</span> tf])
(<span class="org-keyword">import</span> [tensorflow_datasets <span class="org-keyword">:as</span> tfds])
(<span class="org-keyword">import</span> [numpy <span class="org-keyword">:as</span> np])
</pre>
</div>


<p>
Download reviews form <i>Large Movie Review Dataset</i>.
All the reviews have either a positive or a negative sentiment.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> [dataset info] (tfds.load <span class="org-string">"imdb_reviews/subwords8k"</span>
                                <span class="org-constant">:with_info</span> <span class="org-constant">True</span> <span class="org-constant">:as_supervised</span> <span class="org-constant">True</span>))

(<span class="org-builtin">setv</span> <span class="org-variable-name">train_examples</span> (<span class="org-builtin">get</span> dataset <span class="org-string">"train"</span>))
(<span class="org-builtin">setv</span> <span class="org-variable-name">test_examples</span> (<span class="org-builtin">get</span> dataset <span class="org-string">"test"</span>))
(<span class="org-builtin">setv</span> <span class="org-variable-name">encoder</span> (. (<span class="org-builtin">get</span> info.features <span class="org-string">"text"</span>) encoder))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-vocab_size</span> []
  (<span class="org-keyword">assert</span> (<span class="org-builtin">=</span> encoder.vocab_size 8185)))
(test-case (test-vocab_size))
</pre>
</div>


<p>
A use case of how encoder encodes-decodes strings.
Split a string in subwords according the vocabulary in the dataset.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> <span class="org-variable-name">sample_string</span> <span class="org-string">"Hello Tensorflow."</span>)
(<span class="org-builtin">setv</span> <span class="org-variable-name">encoded_string</span> (encoder.encode sample_string))

(<span class="org-keyword">print</span> <span class="org-string">"Encoded string is"</span> encoded_string)
(<span class="org-keyword">for</span> [idx encoded_string]
  (<span class="org-keyword">print</span> idx <span class="org-string">": "</span>  (encoder.decode [idx])))

(<span class="org-builtin">setv</span> <span class="org-variable-name">sample_string</span>
      (<span class="org-builtin">+</span> <span class="org-string">"After watching such a masterpiece, "</span>
         <span class="org-string">"I highly doubt one will argue that cinema isn't art."</span>))
(<span class="org-builtin">setv</span> <span class="org-variable-name">encoded_string</span> (encoder.encode sample_string))

(<span class="org-keyword">print</span> <span class="org-string">"Encoded string is"</span> encoded_string)
(<span class="org-keyword">for</span> [idx encoded_string]
  (<span class="org-keyword">print</span> idx <span class="org-string">": "</span>  (encoder.decode [idx])))
</pre>
</div>


<pre class="example">
Encoded string is [4025, 222, 6307, 2327, 2934, 7975]
4025 :  Hell
222 :  o
6307 :  Ten
2327 :  sor
2934 :  flow
7975 :  .
Encoded string is
[518,
218,
178,
4,
2722,
2,
12,
937,
1617,
45,
96,
2258,
49,
13,
1351,
260,
7968,
21,
807,
7975]
518 :  After
218 :  watching
178 :  such
4 :  a
2722 :  masterpiece
2 :  ,
12 :  I
937 :  highly
1617 :  doubt
45 :  one
96 :  will
2258 :  argu
49 :  e
13 :  that
1351 :  cinema
260 :  isn
7968 :  '
21 :  t
807 :  art
7975 :  .
</pre>


<p>
Prepare the dataset for training.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> <span class="org-variable-name">BUFFER_SIZE</span> 10000)
(<span class="org-builtin">setv</span> <span class="org-variable-name">BATCH_SIZE</span> 64)

(<span class="org-builtin">setv</span> <span class="org-variable-name">train_dataset</span> (<span class="org-keyword">-&gt;</span> train_examples
                        (.shuffle BUFFER_SIZE)
                        (.padded_batch BATCH_SIZE)))

(<span class="org-builtin">setv</span> <span class="org-variable-name">test_dataset</span> (<span class="org-keyword">-&gt;</span> test_examples
                       (.shuffle BUFFER_SIZE)
                       (.padded_batch BATCH_SIZE)))
</pre>
</div>
</div>
</li>
<li><a id="sec-3-11-2-2" name="sec-3-11-2-2"></a>Model<br ><div class="outline-text-5" id="text-3-11-2-2">
<p>
Build the model with different types of layers.
</p>

<p>
The first layer is the embedding layer (word embedding).
The goal of word embedding is to represent words with embedded features,
and stores one vector per word.
Each word can be compared with others using features,
where in one-hot representation, a word is independent of others.
Other advantage is that it reduces the size of input,
if feature length is inferior to vocabulary length.
The examples below describe both one-hot vector and word embedding methods.
One-hot vector represents one word in the vocabulary, its representation is:
</p>
\begin{align*}
v_{Apple} & =
\begin{bmatrix}
0 \\
0 \\
\dots \\
1 \\
\dots \\
0 \\
0
\end{bmatrix} \\\\
v_{King} & =
\begin{bmatrix}
0 \\
0 \\
0 \\
0 \\
\dots \\
1 \\
\dots \\
0 \\
\end{bmatrix}
\end{align*}
<p>
The word embedding representation is:
</p>
<table class="table table-striped table-bordered table-hover table-condensed">


<colgroup>
<col  class="left">

<col  class="left">

<col  class="right">

<col  class="left">

<col  class="right">

<col  class="left">
</colgroup>
<thead>
<tr>
<th scope="col" class="text-left">&#xa0;</th>
<th scope="col" class="text-left">&#x2026;</th>
<th scope="col" class="text-right">Apple</th>
<th scope="col" class="text-left">&#x2026;</th>
<th scope="col" class="text-right">King</th>
<th scope="col" class="text-left">&#x2026;</th>
</tr>
</thead>
<tbody>
<tr>
<td class="text-left">Age</td>
<td class="text-left">&#xa0;</td>
<td class="text-right">0.03</td>
<td class="text-left">&#xa0;</td>
<td class="text-right">0.7</td>
<td class="text-left">&#xa0;</td>
</tr>

<tr>
<td class="text-left">&#x2026;</td>
<td class="text-left">&#xa0;</td>
<td class="text-right">&#xa0;</td>
<td class="text-left">&#xa0;</td>
<td class="text-right">&#xa0;</td>
<td class="text-left">&#xa0;</td>
</tr>

<tr>
<td class="text-left">Food</td>
<td class="text-left">&#xa0;</td>
<td class="text-right">0.9</td>
<td class="text-left">&#xa0;</td>
<td class="text-right">0.02</td>
<td class="text-left">&#xa0;</td>
</tr>

<tr>
<td class="text-left">&#x2026;</td>
<td class="text-left">&#xa0;</td>
<td class="text-right">&#xa0;</td>
<td class="text-left">&#xa0;</td>
<td class="text-right">&#xa0;</td>
<td class="text-left">&#xa0;</td>
</tr>

<tr>
<td class="text-left">Juice</td>
<td class="text-left">&#xa0;</td>
<td class="text-right">0.9</td>
<td class="text-left">&#xa0;</td>
<td class="text-right">0.01</td>
<td class="text-left">&#xa0;</td>
</tr>

<tr>
<td class="text-left">&#x2026;</td>
<td class="text-left">&#xa0;</td>
<td class="text-right">&#xa0;</td>
<td class="text-left">&#xa0;</td>
<td class="text-right">&#xa0;</td>
<td class="text-left">&#xa0;</td>
</tr>

<tr>
<td class="text-left">Royal</td>
<td class="text-left">&#xa0;</td>
<td class="text-right">0.02</td>
<td class="text-left">&#xa0;</td>
<td class="text-right">0.9</td>
<td class="text-left">&#xa0;</td>
</tr>

<tr>
<td class="text-left">&#x2026;</td>
<td class="text-left">&#xa0;</td>
<td class="text-right">&#xa0;</td>
<td class="text-left">&#xa0;</td>
<td class="text-right">&#xa0;</td>
<td class="text-left">&#xa0;</td>
</tr>
</tbody>
</table>

<p>
The second layer is a bidirectional layer, and it represents a
Bidirectional Residual Neural Network (BRNN),
with Long Short Term Memory (LSTM) on top of it.
BRNN is an RNN architecture where a single cell is connected to
two hidden layers of opposite directions.
This structure allows getting information from the past and the future.
LSTM is a type of RNN designed to work with long-term dependencies,
because basic RNN is unable to remember well in case of long-term dependencies.
</p>

<figure id="fig:lstm_cell.png">
<p><img src="./export/tc/lstm_cell.png" class="img-responsive" alt="lstm_cell.png" width="500">
</p>
<figcaption><span class="figure-number">Figure 49:</span> A LSTM cell</figcaption>
</figure>

<p>
The following paragraph is a description of the figure <a href="#fig:lstm_cell.png">49</a>
(Olah Christopher. 2015. <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a>):
</p>
<ul class="org-ul">
<li>Cell state (memory): contains information from previous LSTM cell,
and gives this information to next one. \(C_{t-1}\) through \(C_{t}\)
</li>
<li>Gates regulates cell sate information by using
a \(\operatorname{sigmoid}\) (denoted \(\sigma\)) layer.
LSTM has three gates (\(\operatorname{sigmoid_1}\),
\(\operatorname{sigmoid_2}\), \(\operatorname{sigmoid_3}\))
</li>
<li>\(\operatorname{sigmoid_1}\): (forget gate layer): decides according
both \(h_{t-1}\) and \(x_t\) (new word) inputs to keep cell state information.
\(\operatorname{sigmoid}\) output is either 0 or 1
</li>
<li>\(\operatorname{sigmoid_2}\): (input gate layer): decides according
both \(h_{t-1}\) and \(x_t\) to update \(C_t\) with \(\tilde{C_t}\)
</li>
<li>\(\tanh_1\): creates a new candidate values \(\tilde{C_t}\)
</li>
<li>\(\tilde{C_t}\): will be updated either
with a new scaled value or by \(C_{t-1}\)
</li>
<li>\(\operatorname{sigmoid_3}\): decides according both \(h_{t-1}\)
and \(x_t\) <i>how much</i> the \(h_t\) will be updated
</li>
<li>\(\tanh_2\): resizes the cell state value in range \([-1,1]\)
</li>
<li>\(h_t\): will be updated with a <i>filtered</i> cell state value
</li>
</ul>

<p>
The equations for the figure <a href="#fig:lstm_cell.png">49</a>:
</p>
\begin{align*}
f_t & = \sigma(W_f \cdot [h_{t-1},x_t] + b_f) \\
i_t & = \sigma(W_i \cdot [h_{t-1},x_t] + b_i) \\
\tilde{C_t} & = \tanh(W_C \cdot [h_{t-1},x_t] + b_C) \\
C_t & = f_t \odot C_{t-1} + i_t \odot \tilde{C_t} \\
o_t & = \sigma(W_o \cdot [h_{t-1},x_t] + b_o) \\
h_t & = o_t \odot \tanh(C_t)
\end{align*}

<p>
The last layer is a dense layer.
It implements the operation <code>activation(weights dot features + bias)</code>,
and it is fully connected.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> <span class="org-variable-name">model</span> (tf.keras.Sequential
              [(tf.keras.layers.Embedding encoder.vocab_size 64)
               (tf.keras.layers.Bidirectional (tf.keras.layers.LSTM 64))
               (tf.keras.layers.Dense 64 <span class="org-constant">:activation</span> <span class="org-string">"relu"</span>)
               (tf.keras.layers.Dense 1)]))
</pre>
</div>


<p>
Print the model summary.
</p>
<div class="org-src-container">

<pre class="src src-hy">(model.summary)
</pre>
</div>


<pre class="example">
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
embedding (Embedding)        (None, None, 64)          523840
_________________________________________________________________
bidirectional (Bidirectional (None, 128)               66048
_________________________________________________________________
dense (Dense)                (None, 64)                8256
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 65
=================================================================
Total params: 598,209
Trainable params: 598,209
Non-trainable params: 0
_________________________________________________________________
</pre>


<p>
Compile and train the model with specific parameters.
</p>
<div class="org-src-container">

<pre class="src src-hy">(model.compile <span class="org-constant">:loss</span> (tf.keras.losses.BinaryCrossentropy <span class="org-constant">:from_logits</span> <span class="org-constant">True</span>)
               <span class="org-constant">:optimizer</span> (tf.keras.optimizers.Adam 1e-4)
               <span class="org-constant">:metrics</span> [<span class="org-string">"accuracy"</span>])

(model.fit train_dataset
           <span class="org-constant">:epochs</span> 2
           <span class="org-constant">:validation_data</span> test_dataset
           <span class="org-constant">:validation_steps</span> 30)
</pre>
</div>


<p>
Save the model trained into a file for reuse purpose.
</p>
<div class="org-src-container">

<pre class="src src-hy">(.save model <span class="org-string">"rnn_tc_model.h5"</span>)
(<span class="org-builtin">setv</span> <span class="org-variable-name">model</span> (tf.keras.models.load_model <span class="org-string">"rnn_tc_model.h5"</span>))
</pre>
</div>


<p>
Set the both loss and accuracy from validation dataset.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-builtin">setv</span> [test_loss test_acc] (model.evaluate test_dataset))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">print</span> (.format <span class="org-string">"Test Loss: {}"</span> test_loss))
(<span class="org-keyword">print</span> (.format <span class="org-string">"Test Accuracy: {}"</span> test_acc))
</pre>
</div>


<pre class="example">
Test Loss: 0.3757523000240326
Test Accuracy: 0.8048400282859802
</pre>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">test-model</span> []
  (<span class="org-keyword">assert</span> (np.isclose test_loss 0.37575 <span class="org-constant">:rtol</span> 0.01))
  (<span class="org-keyword">assert</span> (np.isclose test_acc 0.804840 <span class="org-constant">:rtol</span> 0.01)))
(test-case (test-model))
</pre>
</div>
</div>
</li></ol>
</div>
<div id="outline-container-sec-3-11-3" class="outline-4">
<h4 id="sec-3-11-3"><span class="section-number-4">3.11.3</span> Results</h4>
<div class="outline-text-4" id="text-3-11-3">
<p>
Generate output prediction for the input samples.
Input samples are here new reviews that aren't included from the dataset.
If the prediction is &gt;= 0.5, it's positive, otherwise it's negative.
</p>
<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">defn</span> <span class="org-function-name">sample_predict</span> [sample_pred_text]
  (<span class="org-keyword">-&gt;</span> sample_pred_text
      (encoder.encode)
      (tf.cast tf.float32)
      (tf.expand_dims 0)
      (model.predict)))
</pre>
</div>


<div class="org-src-container">

<pre class="src src-hy">(<span class="org-keyword">print</span> (sample_predict (<span class="org-builtin">+</span> <span class="org-string">"It is a masterpiece. It's brilliant, enigmatic, "</span>
                          <span class="org-string">"and masterfully filmed. I love it."</span>)))
(<span class="org-keyword">print</span>
  (sample_predict
    (<span class="org-builtin">+</span> <span class="org-string">"I enjoyed this movie more on the second viewing than the first."</span>
       <span class="org-string">"It's not just a curiosity piece; it's a very intriguing movie."</span>)))
(<span class="org-keyword">print</span> (sample_predict
         <span class="org-string">"If I could have given this movie a 0 rating, I would have."</span>))
(<span class="org-keyword">print</span> (sample_predict
         <span class="org-string">"This movie is one of the greatest neo-noir films ever made."</span>))
(<span class="org-keyword">print</span>
  (sample_predict
    (<span class="org-builtin">+</span> <span class="org-string">"I thought while it was at times interesting film "</span>
       <span class="org-string">"it was defiantly not the masterpiece I have been lead to believe."</span>)))
</pre>
</div>


<pre class="example">
[[1.1285713]]
[[0.8550527]]
[[-0.3402745]]
[[0.294632]]
[[-0.34261024]]
</pre>
</div>
</div>
<div id="outline-container-sec-3-11-4" class="outline-4">
<h4 id="sec-3-11-4"><span class="section-number-4">3.11.4</span> References</h4>
<div class="outline-text-4" id="text-3-11-4">
<ul class="org-ul">
<li>deeplearning.ai. 2020. <a href="https://www.deeplearning.ai/deep-learning-specialization/">Deep Learning Specialization</a>.
</li>
<li>Tensorflow Team. 2020. <a href="https://www.tensorflow.org/tutorials/text/text_classification_rnn">Text classification with an RNN</a>.
</li>
<li>Maas Andrew. 2011. <a href="http://ai.stanford.edu/%257Eamaas/data/sentiment/">Large Movie Review Dataset</a>.
</li>
<li>Olah Christopher. 2015. <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a>.
</li>
</ul>
</div>
</div>
</div>
</div>
</div><div class="col-md-3"><nav id="table-of-contents">
<div id="text-table-of-contents" class="bs-docs-sidebar">
<ul class="nav">
<li><a href="#sec-1">1. Preface</a>
<ul class="nav">
<li><a href="#sec-1-1">1.1. License</a></li>
<li><a href="#sec-1-2">1.2. Disclaimer</a></li>
<li><a href="#sec-1-3">1.3. Philosophy</a></li>
<li><a href="#sec-1-4">1.4. What is Deep Learning</a></li>
<li><a href="#sec-1-5">1.5. Further Reading</a></li>
</ul>
</li>
<li><a href="#sec-2">2. Build From Sources</a>
<ul class="nav">
<li><a href="#sec-2-1">2.1. Extract Source Code</a></li>
<li><a href="#sec-2-2">2.2. Compile And Run</a></li>
<li><a href="#sec-2-3">2.3. HTML Documentation</a></li>
<li><a href="#sec-2-4">2.4. LaTeX Documentation</a></li>
<li><a href="#sec-2-5">2.5. Export Documentation</a></li>
</ul>
</li>
<li><a href="#sec-3">3. Deep Learning Projects</a>
<ul class="nav">
<li><a href="#sec-3-1">3.1. Documentation Structure</a></li>
<li><a href="#sec-3-2">3.2. Logistic Regression Neural Network Classification From Scratch</a></li>
<li><a href="#sec-3-3">3.3. Deep Neural Network Classification From Scratch</a></li>
<li><a href="#sec-3-4">3.4. Convolutional Neural Network Classification Using Keras</a></li>
<li><a href="#sec-3-5">3.5. Data Augmentation Using Keras</a></li>
<li><a href="#sec-3-6">3.6. Optimization Algorithms From Scratch</a></li>
<li><a href="#sec-3-7">3.7. LeNet-5: Convolutional Neural Network Model From Scratch</a></li>
<li><a href="#sec-3-8">3.8. MNIST Handwritten Digit Classification Using Keras</a></li>
<li><a href="#sec-3-9">3.9. Neural Style Transfer Using TensorFlow</a></li>
<li><a href="#sec-3-10">3.10. Character-Level Language From Scratch</a></li>
<li><a href="#sec-3-11">3.11. Text Classification Using TensorFlow</a></li>
</ul>
</li>
</ul>
</div>
</nav>
</div></div></div>
<footer id="postamble" class="">
<div><p class="author">Author: lascauje</p>
<p class="date">Created: 2020-08-09 Sun 08:07</p>
<p class="creator"><a href="http://www.gnu.org/software/emacs/">Emacs</a> 26.3 (<a href="http://orgmode.org">Org-mode</a> 9.1.9)</p>
</div>
</footer>
</body>
</html>
